<!DOCTYPE html>





<html class="theme-next gemini use-motion" lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="baidu-site-verification" content="Mp17ppyPmu" />
<meta name="generator" content="Hexo 3.9.0">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.2.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.2.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.2.0">
  <link rel="mask-icon" href="/images/logo.svg?v=7.2.0" color="#222">

<link rel="stylesheet" href="/css/main.css?v=7.2.0">

<link rel="stylesheet" href="//fonts.proxy.ustclug.org/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">
  <link rel="stylesheet" href="/lib/needsharebutton/needsharebutton.css">
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2">


<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.2.0',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":true},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    fancybox: false,
    mediumzoom: true,
    lazyload: true,
    pangu: false,
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    search: {
      root: '/',
      path: 'search.xml'
    },
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    }
  };
</script>


  <script>
  (function(i,s,o,g,r,a,m){i["DaoVoiceObject"]=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;a.charset="utf-8";m.parentNode.insertBefore(a,m)})(window,document,"script",('https:' == document.location.protocol ? 'https:' : 'http:') + "//widget.daovoice.io/widget/0f81ff2f.js","daovoice")
  daovoice('init', {
      app_id: "4d92f510"
    });
  daovoice('update');
  </script>

  <meta name="description" content="神经网络工具箱nnautograd实现了自动微分系统，然而对于深度学习来说过于底层。nn模块是构建与autograd之上的神经网络模块。除了nn之外，我们还会介绍神经网络中常用的工具，比如优化器optim,初始化init等。 nn.Moduletorch.nn的核心数据结构是Module，它是一个抽象的概念，既可以表示神经网络中的某个层（layer），也可以表示一个包含很多层的神经网络。在实际使用">
<meta name="keywords" content="pytorch,笔记">
<meta property="og:type" content="article">
<meta property="og:title" content="【notes】pytorch学习笔记3-神经网络工具箱nn">
<meta property="og:url" content="https://zinw623.github.io/2019/08/22/nn.Module/index.html">
<meta property="og:site_name" content="Zihan.W的博客">
<meta property="og:description" content="神经网络工具箱nnautograd实现了自动微分系统，然而对于深度学习来说过于底层。nn模块是构建与autograd之上的神经网络模块。除了nn之外，我们还会介绍神经网络中常用的工具，比如优化器optim,初始化init等。 nn.Moduletorch.nn的核心数据结构是Module，它是一个抽象的概念，既可以表示神经网络中的某个层（layer），也可以表示一个包含很多层的神经网络。在实际使用">
<meta property="og:locale" content="zh-CN">
<meta property="og:updated_time" content="2019-08-23T11:38:08.172Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="【notes】pytorch学习笔记3-神经网络工具箱nn">
<meta name="twitter:description" content="神经网络工具箱nnautograd实现了自动微分系统，然而对于深度学习来说过于底层。nn模块是构建与autograd之上的神经网络模块。除了nn之外，我们还会介绍神经网络中常用的工具，比如优化器optim,初始化init等。 nn.Moduletorch.nn的核心数据结构是Module，它是一个抽象的概念，既可以表示神经网络中的某个层（layer），也可以表示一个包含很多层的神经网络。在实际使用">
  <link rel="alternate" href="/atom.xml" title="Zihan.W的博客" type="application/atom+xml">
  <link rel="canonical" href="https://zinw623.github.io/2019/08/22/nn.Module/">


<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>【notes】pytorch学习笔记3-神经网络工具箱nn | Zihan.W的博客</title>
  


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?Mp17ppyPmu";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>
  <meta name="baidu-site-verification" content="Mp17ppyPmu" />






  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Zihan.W的博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
    <ul id="menu" class="menu">
        
        
        
          
          <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br/>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">
      
    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br/>关于</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">
      
    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br/>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br/>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br/>归档</a>

  </li>
        <li class="menu-item menu-item-search">
          <a href="javascript:;" class="popup-trigger">
          
            <i class="menu-item-icon fa fa-search fa-fw"></i> <br/>搜索</a>
        </li>
      
    </ul>
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>


    </div>
</nav>

</div>
    </header>

    

  <a href="https://github.com/zinw623" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    
    <div class="reading-progress-bar"></div>

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://zinw623.github.io/2019/08/22/nn.Module/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zihan.W">
      <meta itemprop="description" content="Download something from my mind when thinking!">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zihan.W的博客">
    </span>
      <header class="post-header">

        
          <h1 class="post-title" itemprop="name headline">【notes】pytorch学习笔记3-神经网络工具箱nn

              
            
          </h1>
        

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-08-22 17:30:45" itemprop="dateCreated datePublished" datetime="2019-08-22T17:30:45+08:00">2019-08-22</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-08-23 19:38:08" itemprop="dateModified" datetime="2019-08-23T19:38:08+08:00">2019-08-23</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/笔记/" itemprop="url" rel="index"><span itemprop="name">笔记</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/笔记/pytorch/" itemprop="url" rel="index"><span itemprop="name">pytorch</span></a></span>

                
                
              
            </span>
          

          <br/>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
              
                <span class="post-meta-item-text">本文字数：</span>
              
              <span title="本文字数">24k</span>
            </span>
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
              
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              
              <span title="阅读时长">22 分钟</span>
            </span>
          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h3 id="神经网络工具箱nn"><a href="#神经网络工具箱nn" class="headerlink" title="神经网络工具箱nn"></a>神经网络工具箱nn</h3><p>autograd实现了自动微分系统，然而对于深度学习来说过于底层。nn模块是构建与autograd之上的神经网络模块。除了nn之外，我们还会介绍神经网络中常用的工具，比如优化器optim,初始化init等。</p>
<h4 id="nn-Module"><a href="#nn-Module" class="headerlink" title="nn.Module"></a>nn.Module</h4><p>torch.nn的核心数据结构是Module，它是一个抽象的概念，既可以表示神经网络中的某个层（layer），也可以表示一个包含很多层的神经网络。在实际使用中，最常见的做法是继承nn.Module，撰写自己的网络、层。</p>
<p>全连接层，又名仿射层，输出$\boldsymbol{x}$和输入$\boldsymbol{x}$满足$\boldsymbol{y=Wx+b}$，$\boldsymbol{W}$ 和 $\boldsymbol{b}$ 是可学习函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable <span class="keyword">as</span> V</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Linear</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_features, out_features)</span>:</span></span><br><span class="line">        </span><br><span class="line">        super(Linear, self).__init__() <span class="comment">#等价于nn.Module.__init__(self)</span></span><br><span class="line">        self.w = nn.Parameter(t.randn(in_features, out_features))</span><br><span class="line">        self.b = nn.Parameter(t.randn(out_features))</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = x.mm(self.w)</span><br><span class="line">        <span class="keyword">return</span> x + self.b.expand_as(x)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">layer = Linear(<span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line">input = V(t.randn(<span class="number">2</span>, <span class="number">4</span>))</span><br><span class="line">output = layer(input)</span><br><span class="line">output</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[ 2.0269,  5.1465,  1.5603],
        [-0.6868, -0.8096, -0.6427]], grad_fn=&lt;AddBackward0&gt;)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> name, parameter <span class="keyword">in</span> layer.named_parameters():</span><br><span class="line">    </span><br><span class="line">    print(name, parameter)</span><br></pre></td></tr></table></figure>

<pre><code>w Parameter containing:
tensor([[-0.0121, -0.2593, -0.5310],
        [ 0.2982, -0.2846, -0.0437],
        [ 0.6220,  1.7351,  0.8025],
        [ 1.0544,  2.3325,  0.6561]], requires_grad=True)
b Parameter containing:
tensor([0.2586, 2.3734, 0.5372], requires_grad=True)</code></pre><p>但需要注意一下几点：</p>
<ul>
<li>自定义层Linear必须继承nn.Module，并且在其构造函数中需调用nn.Module的构造函数，即super(Linear, self).<strong>init</strong>()或nn.Module.__init__(self)。</li>
<li>在构造函数__init__中必须自己定义可学习的参数，并封装成Parameter，如在本例中我们把w和b封装成Parameter。Parameter是一种特殊的Variable，但其默认需要求导（requires_grad=True）。</li>
<li>forward函数实现前向传播过程，其输入可以是一个或多个variable，对x的任何操作也必须是variable支持的操作。</li>
<li>无需写反向传播函数，因其前向传播都是对variable进行操作，nn.Module能够利用autograd自动实现反向传播，这一点比Function简单许多。</li>
<li>使用时，直观上可将layer看成数学概念中的函数，调用layer(input)即可得到input对应的结果。它等价于layers.__call__(input)，在__call__函数中，主要调用的是layer.forward(X)，另外还对钩子做了一些处理。所以在实际使用中应尽量使用layer(x)而不是使用layer.forward(x)。</li>
</ul>
<p>Module能够自动检测到自己的parameter，并将其作为学习参数。除了parameter，Module还包含子Module，主Module能够递归查找子Module中的parameter。</p>
<p>下面看一个稍微复杂的网络：多层感知机。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Perceptron</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_features, hidden_features, out_features)</span>:</span></span><br><span class="line">        nn.Module.__init__(self)</span><br><span class="line">        self.layer1 = Linear(in_features, hidden_features)</span><br><span class="line">        self.layer2 = Linear(hidden_features, out_features)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.layer1(x)</span><br><span class="line">        x = t.sigmoid(x)</span><br><span class="line">        <span class="keyword">return</span> self.layer2(x)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">perceptron = Perceptron(<span class="number">3</span>, <span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> perceptron.named_parameters():</span><br><span class="line">    print(name, param.size())</span><br></pre></td></tr></table></figure>

<pre><code>layer1.w torch.Size([3, 4])
layer1.b torch.Size([4])
layer2.w torch.Size([4, 1])
layer2.b torch.Size([1])</code></pre><p>注意一下两个知识点。</p>
<ul>
<li>构造函数__init__中，可利用前面自定义的Linear层（module）作为当前module对象的一个字module，它的可学习参数，也会成为当前module的可学习参数。</li>
<li>在前向传播函数中，我们有意识地将输出变量都命名为x，是为了能让python回收一些中间层的输出，从而节省内存。但并不是所有的中间结果都会被回收，有些variable虽然名字被覆盖，但其在反向传播时仍需要用到，此时python的内存回收模块将通过检查引用计数，不会回收这一部分内存。</li>
</ul>
<p>module中parameter的全局命名规范如下。</p>
<ul>
<li>parameter直接命名。例如self.param_name = nn.Parameter(t.randn(3,4))，命名为param_name。</li>
<li>子module中的parameter，会在其名字之前加上当前module的名字，就是sub_module.param_name。</li>
</ul>
<p>为了方便用户使用，pytorch实现了神经网络中绝大多数的layer，这些layer都继承了nn.Module，封装了可学习参数parameter，并实现了forward函数，且专门针对GPU运算进行了CuDNN优化。具体内容可参考<a href="http://pytorch.org/docs/nn.html" target="_blank" rel="noopener">官方文档</a>或在IPython/Jupyter中使用nn.layer。</p>
<p>阅读文档注意：</p>
<ul>
<li>构造函数的参数，如nn.Linear(in_features, out_features, bias)，需关注着三个参数的作用</li>
<li>属性、可学习参数和子module。如nn.Linear中有weight和bias两个可学习参数，不包含子module</li>
<li>输入输出的形状，如nn.Linear的输入形状是(N, input_features)，输出为(N, output_features)，N是batch_size。<br>若想输入一个数据需要调用unsqueeze(0)函数将数据伪装成batch_size = 1的batch</li>
</ul>
<h4 id="常用的神经网络层"><a href="#常用的神经网络层" class="headerlink" title="常用的神经网络层"></a>常用的神经网络层</h4><h5 id="图像相关层"><a href="#图像相关层" class="headerlink" title="图像相关层"></a>图像相关层</h5><p>图像相关层主要包括卷积层（Conv）、池化层（Pool）等，这些层在实际使用中可分为一维、二维和三维，池化层又分为平均池化（AvgPool）、最大值池化（MaxPool）、自适应池化（AdaptiveAvgPool）等。卷积层除了常用的前向卷积外，还有逆卷积（TransposeConv）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable <span class="keyword">as</span> V</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> torchvision.transforms <span class="keyword">import</span> ToTensor, ToPILImage</span><br><span class="line">to_tensor = ToTensor()</span><br><span class="line">to_pil = ToPILImage()</span><br><span class="line">curry = Image.open(<span class="string">'curry'</span>)</span><br><span class="line">curry</span><br></pre></td></tr></table></figure>

<img width="50%" data-src="\uploads\nn.module\output_15_0.png">




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输入一个batch，batch_size = 1</span></span><br><span class="line">input = to_tensor(curry).unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 锐化卷积核</span></span><br><span class="line">kernel = t.ones(<span class="number">3</span>, <span class="number">3</span>) / <span class="number">-9</span></span><br><span class="line">kernel[<span class="number">1</span>][<span class="number">1</span>] = <span class="number">1</span></span><br><span class="line">conv = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, (<span class="number">3</span>, <span class="number">3</span>), <span class="number">1</span>, bias = <span class="literal">False</span>)</span><br><span class="line">conv.weight.data = kernel.view(<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">out = conv(V(input))</span><br><span class="line">to_pil(out.data.squeeze(<span class="number">0</span>))</span><br></pre></td></tr></table></figure>

<img width="50%" data-src="\uploads\nn.module\output_16_0.png">



<p>Shape:</p>
<ul>
<li>Input: $(N, C_{in}, H_{in}, W_{in})$</li>
<li>Output: $(N, C_{out}, H_{out}, W_{out})$ where<br>$H_{out} = \left\lfloor\frac{H_{in}  + 2 \times \text{padding}[0] - \text{dilation}[0]\times (\text{kernel_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor$</li>
</ul>
<p>$W_{out} = \left\lfloor\frac{W_{in}  + 2 \times \text{padding}[1] - \text{dilation}[1]\times (\text{kernel_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor$</p>
<p>图像的卷积操作还有各种变体，有关各种变体的介绍可以参照此处的介绍<a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md" target="_blank" rel="noopener">https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md</a></p>
<p>池化层可以看作是一种特殊的卷积层，用来下采样。但池化层没有可学习参数，其weight是固定的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pool = nn.AvgPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">list(pool.parameters())</span><br></pre></td></tr></table></figure>

<pre><code>[]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">out = pool(V(input))</span><br><span class="line">to_pil(out.data.squeeze(<span class="number">0</span>))</span><br></pre></td></tr></table></figure>

<img width="50%" data-src="\uploads\nn.module\output_21_0.png">



<p>除了卷积层和池化层，深度学习中还将常用到一下几个层</p>
<ul>
<li>Linear：全连接层</li>
<li>BatchNorm：批规范化层，分为1D、2D和3D。除了标准的BatchNorm之外，还有在风格迁移中常用到的InstanceNorm层。</li>
<li>Dropout：dropout层，用来防止过拟合，同样分为1D、2D和3D。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输入batch_size=2，维度3</span></span><br><span class="line">input = V(t.randn(<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">linear = nn.Linear(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">h = linear(input);h</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[-0.4360,  0.3433, -0.1978, -0.3128],
        [-0.9655,  0.6278,  0.2510,  0.1256]], grad_fn=&lt;AddmmBackward&gt;)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 4 channel,初始化标准差为4，均值为0</span></span><br><span class="line">bn = nn.BatchNorm1d(<span class="number">4</span>)</span><br><span class="line">bn.weight.data = t.ones(<span class="number">4</span>) * <span class="number">4</span></span><br><span class="line">bn.bias.data = t.zeros(<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">bn_out = bn(h)</span><br><span class="line"><span class="comment"># 注意输出的均值和方差</span></span><br><span class="line"><span class="comment"># 方差是标准差的平方，计算无偏方差分母会减1</span></span><br><span class="line"><span class="comment"># 使用unbiased=False，分母不减1</span></span><br><span class="line">bn_out.mean(<span class="number">0</span>), bn_out.var(<span class="number">0</span>, unbiased = <span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<pre><code>(tensor([-1.1921e-07,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        grad_fn=&lt;MeanBackward1&gt;),
 tensor([15.9977, 15.9921, 15.9968, 15.9967], grad_fn=&lt;VarBackward1&gt;))</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 每个元素以0.5的概率舍弃</span></span><br><span class="line">dropout = nn.Dropout(<span class="number">0.5</span>)</span><br><span class="line">o = dropout(bn_out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 有一半的概率会变成0</span></span><br><span class="line">o</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[7.9994, -0.0000, -0.0000, -0.0000],
        [-0.0000, 7.9980, 7.9992, 7.9992]], grad_fn=&lt;MulBackward0&gt;)</code></pre><p>以上很多例子中都对module的属性直接操作，其大多数是可学习参数，一般会随着学习的进行而不断改变。实际使用中除非需要使用特殊的初始化，否则尽量不要直接改变参数。</p>
<h5 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h5><p>pytorch实现了常见的激活函数。其他具体的接口信息可参见<a href="http://pytorch.org/docs/nn.html#non-linear-activations" target="_blank" rel="noopener">官方文档</a>，这些激活函数可以作为独立的layer使用。这里介绍最常用的激活函数ReLU，其数学表达式为：<br>$$ReLU(x)=max(0,x)$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">relu = nn.ReLU(inplace = <span class="literal">True</span>)</span><br><span class="line">input = V(t.randn(<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">print(input)</span><br><span class="line">output = relu(input)</span><br><span class="line">print(output) <span class="comment"># 小于0的都被截断为0</span></span><br><span class="line">id(input) == id(output)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[ 0.5049,  0.6093, -0.1565],
        [-0.9114, -0.9594,  1.0539]])
tensor([[0.5049, 0.6093, 0.0000],
        [0.0000, 0.0000, 1.0539]])





True</code></pre><p>ReLU函数有个inplace参数，如果设为True，如果设为True，它会把输出直接覆盖到输入中，这样可以节省内存/显存。之所以可以覆盖是因为在计算ReLU的反向传播时，只需根据输出就能够推算出反向传播的梯度。但是只有少数的autograd操作支持inplace操作（如variable.sigmoid_()），除非你明确知道自己在做什么，否则一般不要使用inplace操作。</p>
<p>在以上例子里，都是将每一层的输出直接作为下一层的输入，这种网络成为前馈传播网络。对于此种网络，如果每次都写复杂的forward函数会有些麻烦，在此就有两种简化方式，ModuleList和Sequential。其中Sequential是一个特殊的Module，它包含几个子module，前向传播时会将输入一层接一层第传递下去。ModuleList也是一个特殊的Module，可以包含几个子Module，可以像用list一样使用它，但不能直接把输入传给ModuleList。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Sequential的三种写法</span></span><br><span class="line"></span><br><span class="line">net1 = nn.Sequential()</span><br><span class="line">net1.add_module(<span class="string">'conv'</span>, nn.Conv2d(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>))     <span class="comment"># 输入为(N, C_&#123;in&#125;, H_&#123;in&#125;, W_&#123;in&#125;)，参数为</span></span><br><span class="line">net1.add_module(<span class="string">'batchnorm'</span>, nn.BatchNorm2d(<span class="number">3</span>)) <span class="comment"># 3为(N, C, H, W)中的C</span></span><br><span class="line">net1.add_module(<span class="string">'activation_layer'</span>, nn.ReLU())</span><br><span class="line"></span><br><span class="line">net2 = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>),</span><br><span class="line">    nn.BatchNorm2d(<span class="number">3</span>),</span><br><span class="line">    nn.ReLU()</span><br><span class="line">    )</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line">net3 = nn.Sequential(OrderedDict([</span><br><span class="line">    (<span class="string">'conv1'</span>, nn.Conv2d(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>)),</span><br><span class="line">    (<span class="string">'bn1'</span>, nn.BatchNorm2d(<span class="number">3</span>)),</span><br><span class="line">    (<span class="string">'relu1'</span>, nn.ReLU()),</span><br><span class="line">]))</span><br><span class="line"></span><br><span class="line">print(<span class="string">'net1'</span>, net1)</span><br><span class="line">print(<span class="string">'net2'</span>, net2)</span><br><span class="line">print(<span class="string">'net3'</span>, net3)</span><br></pre></td></tr></table></figure>

<pre><code>net1 Sequential(
  (conv): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))
  (batchnorm): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (activation_layer): ReLU()
)
net2 Sequential(
  (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU()
)
net3 Sequential(
  (conv1): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))
  (bn1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu1): ReLU()
)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可根据名字或序号取出子module</span></span><br><span class="line">net1.conv, net2[<span class="number">0</span>], net3.conv1</span><br></pre></td></tr></table></figure>

<pre><code>(Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1)),
 Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1)),
 Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1)))</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">input = V(t.rand(<span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">net1(input), net2(input), net3(input), net3.relu1(net2[<span class="number">1</span>](net1.conv(input)))</span><br></pre></td></tr></table></figure>

<pre><code>(tensor([[[[0.0000, 1.4727],
           [0.1600, 0.0000]],

          [[0.0000, 0.0000],
           [0.7015, 1.1069]],

          [[1.7189, 0.0000],
           [0.0000, 0.0000]]]], grad_fn=&lt;ReluBackward0&gt;),
 tensor([[[[0.0000, 1.6957],
           [0.0000, 0.0000]],

          [[1.2454, 0.6350],
           [0.0000, 0.0000]],

          [[1.0204, 0.4811],
           [0.1430, 0.0000]]]], grad_fn=&lt;ReluBackward0&gt;),
 tensor([[[[0.0000, 1.5585],
           [0.1751, 0.0000]],

          [[0.0000, 1.4177],
           [0.1846, 0.0000]],

          [[0.0000, 0.0000],
           [1.3537, 0.2417]]]], grad_fn=&lt;ReluBackward0&gt;),
 tensor([[[[0.0000, 1.4727],
           [0.1600, 0.0000]],

          [[0.0000, 0.0000],
           [0.7015, 1.1069]],

          [[1.7189, 0.0000],
           [0.0000, 0.0000]]]], grad_fn=&lt;ReluBackward0&gt;))</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">modulelist = nn.ModuleList([nn.Linear(<span class="number">3</span>, <span class="number">4</span>), nn.ReLU(), nn.Linear(<span class="number">4</span>, <span class="number">2</span>)])</span><br><span class="line">input = V(t.rand(<span class="number">1</span>, <span class="number">3</span>))</span><br><span class="line"><span class="keyword">for</span> model <span class="keyword">in</span> modulelist:</span><br><span class="line">    input = model(input)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面会报错，因为modellist没有实现forward方法</span></span><br><span class="line"><span class="comment"># output = modellist(input)</span></span><br></pre></td></tr></table></figure>

<p>为何不直接使用python中自带的list，而非要多次一举呢？<br>这是因为ModuleList是Module的子类，当在Module中使用它时，就能自动识别为子module。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyModule</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(MyModule, self).__init__()</span><br><span class="line">        self.list = [nn.Linear(<span class="number">3</span>,<span class="number">4</span>), nn.ReLU()] <span class="comment"># 直接用list</span></span><br><span class="line">        self.module_list = nn.ModuleList([nn.Conv2d(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>), nn.ReLU()]) <span class="comment"># 用nn.ModuleList</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    </span><br><span class="line">model = MyModule()</span><br><span class="line">model</span><br></pre></td></tr></table></figure>

<pre><code>MyModule(
  (module_list): ModuleList(
    (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))
    (1): ReLU()
  )
)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">    </span><br><span class="line">    print(name, param.size())</span><br></pre></td></tr></table></figure>

<pre><code>module_list.0.weight torch.Size([3, 3, 3, 3])
module_list.0.bias torch.Size([3])</code></pre><p>可见，list中的子module并不能被主module识别，而ModuleList中的子module能够被主module识别。</p>
<p>除ModuleList之外还有ParameterList，它是一个可以包含多个parameter的类list对象。在实际应用中，使用方式和ModuleList类似。在构造函数__init__中用到list、tuple、dict等对象，一定要思考是否应该用ModuleList或ParameterList代替。</p>
<h5 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h5><p>在深度学习中会用到各种各样的损失函数，这些损失函数可看作是一种特殊的layer，pytorch也将这些损失函数实现为nn.Module的子类。然而在实际使用中通常将这些损失函数专门提取出来，作为独立的一部分。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#batch_size = 3, 计算对应每个类别的分数</span></span><br><span class="line">score = V(t.randn(<span class="number">3</span>, <span class="number">10</span>))  <span class="comment"># (N, C) N是batch_size，C是class的个数</span></span><br><span class="line"><span class="comment"># 三个样本分别属于1， 0， 1类，label必须是LongTensor</span></span><br><span class="line">label = V(t.Tensor([<span class="number">1</span>, <span class="number">0</span>, <span class="number">9</span>])).long()</span><br><span class="line"></span><br><span class="line"><span class="comment"># loss与普通的layer无差异</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">loss = criterion(score, label)</span><br><span class="line">loss</span><br></pre></td></tr></table></figure>

<pre><code>tensor(2.8392)</code></pre><h4 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h4><p>pytorch将深度学习中常用的优化方法全部封装到torch.optim中，其设计十分灵活，能够很方便地扩展自定义的优化方法。<br>所有的优化方法都是继承基类optim.Optimizer，并实现了自己的优化步骤。下面就以最基本的优化方法————随机梯度下降法（SGD）举例说明。这里需要重点掌握：</p>
<ul>
<li>优化方法的基本使用方法</li>
<li>如何对模型的不同部分设置不同的学习率</li>
<li>如何调整学习率</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先定义一个LeNet网络</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.features = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        self.classifier = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">120</span>, <span class="number">84</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">84</span>, <span class="number">10</span>),</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.features(x)</span><br><span class="line">        x = x.view(<span class="number">-1</span>, <span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>)</span><br><span class="line">        x = self.classifier(x)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">net = Net()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line">optimizer = optim.SGD(params = net.parameters(), lr = <span class="number">1</span>)</span><br><span class="line">optimizer.zero_grad() <span class="comment"># 梯度清零，等价于net.zero_grad()</span></span><br><span class="line"></span><br><span class="line">input = V(t.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>))</span><br><span class="line">output = net(input)</span><br><span class="line">output.backward(output) <span class="comment"># fake backward</span></span><br><span class="line"></span><br><span class="line">optimizer.step() <span class="comment"># 执行优化</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 为不同子网络参数设置不同的学习率，在finetune中经常用到</span></span><br><span class="line"><span class="comment"># 如果对某个参数不指定学习率，就是用默认学习率</span></span><br><span class="line">optimizer = optim.SGD([&#123;<span class="string">'params'</span>:net.features.parameters()&#125;,&#123;<span class="string">'params'</span>:net.classifier.parameters(), <span class="string">'lr'</span>:<span class="number">1e-2</span>&#125;], lr = <span class="number">1e-5</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  只为两个全连接层设置较大的学习率，其余层的学习率较小</span></span><br><span class="line">special_layers = nn.ModuleList([net.classifier[<span class="number">0</span>],net.classifier[<span class="number">2</span>]])</span><br><span class="line">special_layers_params = list(map(id, special_layers.parameters()))</span><br><span class="line">base_params = filter(<span class="keyword">lambda</span> p: id(p) <span class="keyword">not</span> <span class="keyword">in</span> special_layers_params, net.parameters())</span><br><span class="line"></span><br><span class="line">optimizer = t.optim.SGD([</span><br><span class="line">    &#123;<span class="string">'params'</span>:base_params&#125;,</span><br><span class="line">    &#123;<span class="string">'params'</span>:special_layers.parameters(), <span class="string">'lr'</span>: <span class="number">0.01</span>&#125;</span><br><span class="line">], lr = <span class="number">0.001</span>)</span><br></pre></td></tr></table></figure>

<p>调整学习率主要有两种做法。一种是修改optimmizer.param_groups中对应的学习率。另一种是新建优化器（更简单也是更推荐的做法），由于optimizer十分轻量级，构建开销很小，故可以构建新的optimizer。但是新建优化器会重新初始化动量等状态信息，这对使用动量的优化器来说（如带momentum的sgd），可能会造成损失函数在收敛过程中出震荡。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 调整学习率，新建一个optimizer</span></span><br><span class="line">old_lr = <span class="number">0.1</span></span><br><span class="line">optimizer = optim.SGD([</span><br><span class="line">    &#123;<span class="string">'params'</span>: net.features.parameters()&#125;,</span><br><span class="line">    &#123;<span class="string">'params'</span>: net.classifier.parameters(), <span class="string">'lr'</span>:old_lr *<span class="number">0.1</span>&#125;</span><br><span class="line">],lr = <span class="number">1e-5</span>)</span><br></pre></td></tr></table></figure>

<h4 id="nn-functional"><a href="#nn-functional" class="headerlink" title="nn.functional"></a>nn.functional</h4><p>nn中还有一个常用的模块：nn.functional。nn中的大多数layer在functional中都有一个与之相对应的函数。<br>nn.functional中的函数和nn.Module的主要区别在于，用nn.Module实现的layers是一个特殊的类，都是由class Layer(nn.Module)定义，会自动提取可学习的参数；而nn.functional中的函数更像是纯函数，由def function(input)定义。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">input = V(t.randn(<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">model = nn.Linear(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">output1 = model(input)</span><br><span class="line">output2 = nn.functional.linear(input, model.weight, model.bias)</span><br><span class="line">output1 == output2</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[True, True, True, True],
        [True, True, True, True]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">b = nn.functional.relu(input)</span><br><span class="line">b2 = nn.ReLU()(input)</span><br><span class="line">b == b2</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[True, True, True],
        [True, True, True]])</code></pre><p>应该什么时候使用nn.Module，什么时候使用nn.functional？<br>如果模型有可学习的参数，最好用nn.Module，否则既可以使用nn.functional也可以使用nn.Module，二者在性能上没有太大差异，具体的使用方式取决于个人喜好。<br>但建议还是使用nn.Dropout而不是nn.functional.dropout，因为dropout在训练和测试两个阶段的行为有所差别，使用nn.Module对象能够通过model.eval操作以一区分。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv1(x)), <span class="number">2</span>)</span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv2(x)), <span class="number">2</span>)</span><br><span class="line">        x = x.view(<span class="number">-1</span>, <span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">net = Net()</span><br><span class="line">x = t.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line">net(x)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[-0.0539,  0.0145,  0.0214,  0.0474, -0.0516,  0.0890,  0.0539,  0.0805,
          0.0785, -0.1043]], grad_fn=&lt;AddmmBackward&gt;)</code></pre><p>不具备可学习参数的层（激活层、池化层等），将它们用函数代替，这样可以不用放置在构造函数__init__中。有可学习参数的模块，也可以用functional代替，只不过实现起来较繁琐，需要手动定义参数parameter。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyLinear</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(MyLinear, self).__init__()</span><br><span class="line">        self.weight = nn.Parameter(t.randn(<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">        self.bias = nn.Parameter(t.zeros(<span class="number">3</span>))</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,input)</span>:</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> F.linear(input, self.weight, self.bias)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = t.randn(<span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">linear = MyLinear()</span><br><span class="line">linear(x)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[-0.0678,  2.5530,  0.8512]], grad_fn=&lt;AddmmBackward&gt;)</code></pre><h4 id="初始化策略"><a href="#初始化策略" class="headerlink" title="初始化策略"></a>初始化策略</h4><p>在深度学习中参数的初始化十分重要，良好的初始化能使模型收敛更快，并达到更高水平，而糟糕的初始化可能使模型迅速崩溃。pytorch中nn.Module的模块参数都采取了较合理的初始化策略，因此一般不用我们考虑。当然我们可以用自定义的初始化代替系统的默认初始化。自定义初始化尤为重要，因为t.Tensor()返回的是内存中的随机数，很可能会有极大值，这在实际训练网络中会造成溢出或者梯度消失。pytorch中的nn.init模块专门为初始化设计，实现了常用的初始化策略。如果某种初始化策略nn.init不提供，用户也可以自己直接初始化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 利用nn.init初始化</span></span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> init</span><br><span class="line">linear = nn.Linear(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">t.manual_seed(<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 等价于linear.weight.data.normal_(0, std)</span></span><br><span class="line">init.xavier_normal_(linear.weight)</span><br></pre></td></tr></table></figure>

<pre><code>Parameter containing:
tensor([[ 0.3535,  0.1427,  0.0330],
        [ 0.3321, -0.2416, -0.0888],
        [-0.8140,  0.2040, -0.5493],
        [-0.3010, -0.4769, -0.0311]], requires_grad=True)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line">t.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">std = math.sqrt(<span class="number">2</span>)/math.sqrt(<span class="number">7.</span>)</span><br><span class="line">linear.weight.data.normal_(<span class="number">0</span>, std)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[ 0.3535,  0.1427,  0.0330],
        [ 0.3321, -0.2416, -0.0888],
        [-0.8140,  0.2040, -0.5493],
        [-0.3010, -0.4769, -0.0311]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对模型的所有参数进行初始化</span></span><br><span class="line"><span class="keyword">for</span> name, params <span class="keyword">in</span> net.named_parameters():</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> name.find(<span class="string">'linear'</span>) != <span class="number">-1</span>:</span><br><span class="line">        print(params[<span class="number">0</span>]) <span class="comment"># weight</span></span><br><span class="line">        print(params[<span class="number">1</span>]) <span class="comment"># bias</span></span><br><span class="line">    <span class="keyword">elif</span> name.find(<span class="string">'conv'</span>) != <span class="number">-1</span>:</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">elif</span> name.find(<span class="string">'norm'</span>) != <span class="number">-1</span>:</span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>

<h4 id="nn-Module深入分析"><a href="#nn-Module深入分析" class="headerlink" title="nn.Module深入分析"></a>nn.Module深入分析</h4><p>如果想深入地理解nn.Module，研究其原理是很有必要的。首先来看看nn.Module基类的构造函数的源代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">    self._parameters = OrderedDict()</span><br><span class="line">    self._modules = OrderedDict()</span><br><span class="line">    self._buffers = OrderedDict()</span><br><span class="line">    self._backward_hooks = OrderedDict()</span><br><span class="line">    self._forward_hooks = OrderedDict()</span><br><span class="line">    self.training = <span class="literal">True</span></span><br></pre></td></tr></table></figure>

<p>其中每个属性的解释如下：</p>
<ul>
<li>_parameters：字典。保存用户直接设置的parameter，self.param1 = nn.Parameter(t.randn(3, 3))会被检测到，在字典中加入一个key为param，value为对应parameter的item，而self.submodule = nn.Linear(3, 4)中的parameter则不会存于此。</li>
<li>_modules：子module。通过self.submodule = nn.Linear(3, 4)指定的子module会保存于此。</li>
<li>_buffers：缓存。如batchnorm使用momentum机制，每次前向传播需用到上一次前向传播的结果。</li>
<li>_backward_hooks与_forward_hooks：钩子技术，用来提取中间变量，类似variable的hook</li>
<li>training：BatchNorm与Dropout层在训练阶段和测试阶段中采取的策略不同，通过判断training值决定前向传播策略。</li>
</ul>
<p>上述几个属性中，_parameters、_modules和_buffers这三个字典中的键值，都可以通过self.key方式获得，效果等价于self._parameters[‘key’]</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        <span class="comment"># 等价于self.register_parameter('param1', nn.Parameter(t.randn(3, 3)))</span></span><br><span class="line">        self.param1 = nn.Parameter(t.rand(<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">        self.submodel1 = nn.Linear(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">foward</span><span class="params">(self, input)</span>:</span></span><br><span class="line">        x = self.param1 * input</span><br><span class="line">        x = self.submodel1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">net = Net()</span><br><span class="line">net</span><br></pre></td></tr></table></figure>

<pre><code>Net(
  (submodel1): Linear(in_features=3, out_features=4, bias=True)
)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net._modules</span><br></pre></td></tr></table></figure>

<pre><code>OrderedDict([(&apos;submodel1&apos;, Linear(in_features=3, out_features=4, bias=True))])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net._parameters</span><br></pre></td></tr></table></figure>

<pre><code>OrderedDict([(&apos;param1&apos;, Parameter containing:
              tensor([[0.3398, 0.5239, 0.7981],
                      [0.7718, 0.0112, 0.8100],
                      [0.6397, 0.9743, 0.8300]], requires_grad=True))])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net.param1 == net._parameters[<span class="string">'param1'</span>]</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[True, True, True],
        [True, True, True],
        [True, True, True]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> net.named_parameters():</span><br><span class="line">    print(name, param.size())</span><br></pre></td></tr></table></figure>

<pre><code>param1 torch.Size([3, 3])
submodel1.weight torch.Size([4, 3])
submodel1.bias torch.Size([4])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> name, submodel <span class="keyword">in</span> net.named_modules():</span><br><span class="line">    print(name, submodel)</span><br></pre></td></tr></table></figure>

<pre><code> Net(
  (submodel1): Linear(in_features=3, out_features=4, bias=True)
)
submodel1 Linear(in_features=3, out_features=4, bias=True)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">bn = nn.BatchNorm1d(<span class="number">2</span>)</span><br><span class="line">input = V(t.rand(<span class="number">3</span>,<span class="number">2</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">output = bn(input)</span><br><span class="line">bn._buffers</span><br></pre></td></tr></table></figure>

<pre><code>OrderedDict([(&apos;running_mean&apos;, tensor([0.0362, 0.0596])),
             (&apos;running_var&apos;, tensor([0.9009, 0.9262])),
             (&apos;num_batches_tracked&apos;, tensor(1))])</code></pre><p>nn.Module在实际使用中可能层层嵌套，一个module包含若干个子module，每一个子module又包含了更多的子module。为了方便用户访问各个子module，nn.Module实现了很多方法，如函数children可以查看直接子module，函数modules可以查看所有的子module（包括当前module）。与之相对应的还有函数named_children和named_modules，其能够在返回module列表的同时返回它们的名字。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">input = V(t.arange(<span class="number">0</span>, <span class="number">12</span>).view(<span class="number">3</span>, <span class="number">4</span>).float())</span><br><span class="line">model = nn.Dropout()</span><br><span class="line">model(input)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[ 0.,  0.,  0.,  0.],
        [ 0.,  0., 12.,  0.],
        [ 0., 18.,  0.,  0.]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model.training = <span class="literal">False</span></span><br><span class="line">model(input)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[ 0.,  1.,  2.,  3.],
        [ 4.,  5.,  6.,  7.],
        [ 8.,  9., 10., 11.]])</code></pre><p>对batchnorm、dropout、instancenorm等在训练和测试阶段行为差距较大的层，如果在测试时不将其training值设为False，则可能会有很大影响，这在实际使用中千万注意。虽然可通过直接设置training属性将子module设为train和eval模式，但是这种方式比较繁琐。推荐的做法是调用model.train()函数，它会将当前module及其子module中的所有training属性都设为True。model.eval()函数会把training属性都设为False。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(net.training, net.submodel1.training)</span><br><span class="line">net.eval()</span><br><span class="line">net.training, net.submodel1.training</span><br></pre></td></tr></table></figure>

<pre><code>True True





(False, False)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">list(net.named_modules())</span><br></pre></td></tr></table></figure>

<pre><code>[(&apos;&apos;, Net(
    (submodel1): Linear(in_features=3, out_features=4, bias=True)
  )), (&apos;submodel1&apos;, Linear(in_features=3, out_features=4, bias=True))]</code></pre><p>register_forward_hook和register_backward_hook函数的功能类似于variable的register_hook，可在module前向传播或反向传播时注册钩子。每次前向传播执行结束后会执行钩子函数（hook）。前向传播的钩子函数具有如下形式：hook(module, input, output) -&gt; None，而反向传播则具有如下形式：hook(module, grad_input, grad_ouput) -&gt; Tensor or None。钩子函数不应修改输入和输出，并且在使用后应及时删除，以避免每次都运行钩子增加运行负载。钩子函数主要用在获取某些中间结果的情景，如中间某一层的输出或某一层的梯度。这些结果本应写在forward函数中，但如果在forward函数中加上这些处理，可能会使处理逻辑比较复杂，这时使用钩子技术就更合适。下面考虑一种场景：有一个预训练的模型，需要提取模型的某一层（不是最后一层）的输出作为特征进行分类，希望不修改其原有的模型定义文件，这时就可以利用钩子函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">model = VGG()</span><br><span class="line">features = t.Tensor()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hook</span><span class="params">(module, input, output)</span>:</span></span><br><span class="line">    features.copy_(output.data)</span><br><span class="line">    </span><br><span class="line">handle = model.layer8.register_forward_hook(hook)</span><br><span class="line">_ = model(input)</span><br><span class="line"><span class="comment"># 用完hook后删除</span></span><br><span class="line">handle.remove()</span><br></pre></td></tr></table></figure>

<p>nn.Module对象在构造函数中的行为看起来有些诡异，想要理解就需要看两个魔法方法__getattr__和__setattr__。在python中有两个常用的builtin方法：getattr和setattr。getattr(obj, ‘attr1’)等价于obj.attr，setattr(obj, ‘name’, value)等价于obj.name = value。</p>
<ul>
<li>result = obj.name会调用builtin函数getattr(obj, ‘name’)，如果该属性找不到，会调用obj.__getattr__(‘name’)</li>
<li>obj.name = value会调用builtin函数setattr(obj, ‘name’, value)，如果obj对象实现了__setattr__方法，setattr会直接调用obj.__setattr__(‘name’, value)。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">person</span><span class="params">()</span>:</span></span><br><span class="line">    dict = &#123;<span class="string">'name'</span>:<span class="string">'xxx'</span>,<span class="string">'sex'</span>:<span class="string">'boy'</span>,<span class="string">'age'</span>:<span class="number">18</span>&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getattr__</span><span class="params">(self,name)</span>:</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> self.dict[name]</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__setattr__</span><span class="params">(self, name , value)</span>:</span></span><br><span class="line">        </span><br><span class="line">        self.dict[name] = value</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> value</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">one = person()</span><br><span class="line">one.name, one.sex, one.age</span><br></pre></td></tr></table></figure>

<pre><code>(&apos;xxx&apos;, &apos;boy&apos;, 18)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">one.name = <span class="string">'吴彦祖'</span>;one.name</span><br></pre></td></tr></table></figure>

<pre><code>&apos;吴彦祖&apos;</code></pre><p>nn.Module实现了自定义的__setattr__函数，当执行module.name=value时，会在__setattr__中判断value是否为Parameter或nn.Module对象，如果是则将这些对象加到_parameters和_modules两个字典中；如果是其他类型的对象，如Variable、list、dict等，则调用默认的操作，将这个值保存在__dict__中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">module = nn.Module()</span><br><span class="line">module.param = nn.Parameter(t.ones(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">module._parameters,module.param</span><br></pre></td></tr></table></figure>

<pre><code>(OrderedDict([(&apos;param&apos;, Parameter containing:
               tensor([[1., 1.],
                       [1., 1.]], requires_grad=True))]), Parameter containing:
 tensor([[1., 1.],
         [1., 1.]], requires_grad=True))</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">submodule1 = nn.Linear(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">submodule2 = nn.Linear(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">module_list = [submodule1, submodule2]</span><br><span class="line"><span class="comment"># 对于list对象，调用builtin函数，保存在__dict__中</span></span><br><span class="line">module.submodules = module_list</span><br><span class="line">print(<span class="string">'_modules:'</span>,module._modules)</span><br><span class="line">print(<span class="string">"__dict__['submodules']:"</span>,module.__dict__.get(<span class="string">'submodules'</span>))</span><br></pre></td></tr></table></figure>

<pre><code>_modules: OrderedDict()
__dict__[&apos;submodules&apos;]: [Linear(in_features=2, out_features=2, bias=True), Linear(in_features=2, out_features=2, bias=True)]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">module.submodules = nn.ModuleList(module_list)</span><br><span class="line">print(<span class="string">'_modules:'</span>,module._modules)</span><br><span class="line">print(<span class="string">"__dict__['submodules']:"</span>,module.__dict__.get(<span class="string">'submodules'</span>))</span><br></pre></td></tr></table></figure>

<pre><code>_modules: OrderedDict([(&apos;submodules&apos;, ModuleList(
  (0): Linear(in_features=2, out_features=2, bias=True)
  (1): Linear(in_features=2, out_features=2, bias=True)
))])
__dict__[&apos;submodules&apos;]: None</code></pre><p>因_modules和_parameters中的item未保存在__dict__中，所以默认的getattr方法无法获取它，因而nn.Module实现了自定义的__getattr__方法。如果默认的getattr无法处理，就调用自定义的__getattr__方法，尝试从_modules、_parameters和_buffers三个字典中获取。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">getattr(module, <span class="string">'training'</span>) <span class="comment"># 等价于module.training</span></span><br><span class="line"><span class="comment">#error</span></span><br><span class="line"><span class="comment">#module.__getattr__('training')</span></span><br></pre></td></tr></table></figure>

<pre><code>True</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">module.attr1 = <span class="number">2</span></span><br><span class="line">getattr(module, <span class="string">'attr1'</span>)</span><br><span class="line"><span class="comment"># 报错</span></span><br><span class="line"><span class="comment"># module.__getattr__('attr1')</span></span><br></pre></td></tr></table></figure>

<pre><code>2</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">getattr(module, <span class="string">'param'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Parameter containing:
tensor([[1., 1.],
        [1., 1.]], requires_grad=True)</code></pre><p>在pytorch中保存模型十分简单，所有的module对象都具有state_dict()函数，返回当前Module所有的状态数据。将这些状态数据保存后，下次使用模型时即可利用model.load_state_dict()函数将状态加载进来。优化器（optimizer）也有类似机制，不过一般并不需要保存优化器的运行状态。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 保存模型</span></span><br><span class="line">t.save(net.state_dict(), <span class="string">'net.pth'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载已保存的模型</span></span><br><span class="line">net2 = Net()</span><br><span class="line">net2.load_state_dict(t.load(<span class="string">'net.pth'</span>))</span><br></pre></td></tr></table></figure>

<pre><code>&lt;All keys matched successfully&gt;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">t.save(net, <span class="string">'net_all.pth'</span>)</span><br><span class="line">net2 = t.load(<span class="string">'net_all.pth'</span>)</span><br><span class="line">net2</span><br></pre></td></tr></table></figure>

<pre><code>/usr/local/lib/python3.6/dist-packages/torch/serialization.py:256: UserWarning: Couldn&apos;t retrieve source code for container of type Net. It won&apos;t be checked for correctness upon loading.
  &quot;type &quot; + obj.__name__ + &quot;. It won&apos;t be checked &quot;





Net(
  (submodel1): Linear(in_features=3, out_features=4, bias=True)
)</code></pre><p>将Module放在GPU上运行也十分简单，只需一下两步。</p>
<ul>
<li>model = model.cuda():将模型的所有参数转存到GPU</li>
<li>input.cuda():将输入数据放置到GPU上。</li>
</ul>
<p>至于如何在多个GPU上并行计算，pytorch也提供了两个函数，可实现简单高效的并行GPU计算。</p>
<ul>
<li>nn.parallel.data_parallel(module, inputs, device_ids = None, output_device = None, dim = 0, module_kwargs = None)</li>
<li>class torch.nn.DataParallel(module, device_ids = None, output_device = None, dim = 0)</li>
</ul>
<p>可见二者的参数十分相似，通过device_ids参数可以指定在哪些GPU上进行优化，output_device指定输出到哪个GPU上。唯一的不同在于前者直接利用多GPU并行计算得出结果，后者则返回一个新的module，能够自动在多GPU上进行并行加速。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># method1</span></span><br><span class="line">new_net = nn.DataParallel(net, device_ids = [<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">output = new_net(input)</span><br><span class="line"></span><br><span class="line"><span class="comment"># method 2</span></span><br><span class="line">output = nn.parallel.data_parallel(net, input, device_ids = [<span class="number">0</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<p>DataParallel并行的方式，是将输入一个batch的数据均分成多份，分别送到对应的GPU进行计算，然后将各个GPU得到的梯度相加。与Module相关的所有数据也会以浅复制的方式复制多份。</p>
<h4 id="nn和aautograd的关系"><a href="#nn和aautograd的关系" class="headerlink" title="nn和aautograd的关系"></a>nn和aautograd的关系</h4><p>nn.Module利用的是autograd技术，其主要工作是实现前向传播。在forward函数中，nn.Module对输入的Variable进行的各种操作，本质上都用了autograd技术。这里需要对比autograd.Function和nn.Module之间的区别。</p>
<ul>
<li>autograd.Function利用Tensor对autograd技术的扩展，为autograd实现了新的运算op，不仅要实现前向传播还要手动实现反向传播。</li>
<li>nn.Module利用了autograd技术，对nn的功能进行扩展，实现了深度学习中更多的层。只需实现前向传播功能，autograd即会自动实现反向传播。</li>
<li>nn.functional是一些autograd操作的集合，是经过封装的函数。</li>
</ul>
<p>作为两种扩充pytorch接口的方法，我们在实际作用中应该如何选择？如果某一个操作在autograd中尚未支持，那么需要利用Function手动实现对应的前后传播和反向传播。如果某些时候利用autograd接口比较复杂，则可以利用Function将多个操作聚合，实现优化，比直接利用autograd低级别的操作要快。如果只是想在深度学习中怎加某一层，使用nn.Module进行封装则更简单高效。</p>

    </div>
      <p>———————————————感谢阅读———————————————</p>
<blockquote><p>欢迎收藏访问我的<a href="https://zinw623.github.io/">博客</a>  <a href="https://www.zhihu.com/people/tai-li-wu-yan-zu/activities" target="_blank" rel="noopener">知乎</a>   <a href="https://juejin.im/user/5d40058ef265da03b76afb66" target="_blank" rel="noopener">掘金</a>  <a href="https://www.jianshu.com/u/a38373c2e45b" target="_blank" rel="noopener">简书</a> <a href="https://www.zhihu.com/people/tai-li-wu-yan-zu/activities" target="_blank" rel="noopener">知乎</a></p></blockquote>
  <div class="popular-posts-header">相关文章</div>
  <ul class="popular-posts">
    <li class="popular-posts-item">
      
      
      <div class="popular-posts-title"><a href="\2019\08\19\pytorch-1\" rel="bookmark">【notes】pytorch学习笔记1-Tensor部分</a></div>
      
    </li>
  
    <li class="popular-posts-item">
      
      
      <div class="popular-posts-title"><a href="\2019\08\23\pytorch-tools\" rel="bookmark">【notes】pytorch学习笔记4-pytorch常用工具</a></div>
      
    </li>
  
    <li class="popular-posts-item">
      
      
      <div class="popular-posts-title"><a href="\2019\08\20\autograd\" rel="bookmark">【notes】pytorch学习笔记2-autograd部分</a></div>
      
    </li>
  
    <li class="popular-posts-item">
      
      
      <div class="popular-posts-title"><a href="\2019\08\26\pytorch-doc-notes\" rel="bookmark">【翻译】pytorch中文文档（1.2.0）- Notes部分</a></div>
      
    </li>
  
    <li class="popular-posts-item">
      
      
      <div class="popular-posts-title"><a href="\2019\08\26\pytorch-doc-torch\" rel="bookmark">【翻译】pytorch中文文档（1.2.0）- Package Reference/torch</a></div>
      
    </li>
  
  </ul>


    
    
    
      <div id="wechat_subscriber" style="display: block; padding: 10px 0; margin: 20px auto; width: 100%; text-align: center;">
  <img id="wechat_subscriber_qcode" src="/uploads/wechat-qcode.jpg" alt="Zihan.W wechat" style="width: 200px; max-width: 100%;">
  <div>欢迎扫描二维码订阅我的公众号！</div>
</div>

    
      <div>
        <div id="reward-container">
  <div></div>
  <button id="reward-button" disable="enable" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
        
      
      <div style="display: inline-block">
        <img src="/images/alipay.jpg" alt="Zihan.W 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>

      </div>

    <footer class="post-footer">
          
        
        <div class="post-tags">
            <a href="/tags/pytorch/" rel="tag"><i class="fa fa-tag"></i> pytorch</a>
          
            <a href="/tags/笔记/" rel="tag"><i class="fa fa-tag"></i> 笔记</a>
          
        </div>
        <div class="post-widgets">
        

        

        
          <div class="social-share">
            
            
              <div id="needsharebutton-postbottom">
                <span class="btn">
                  <i class="fa fa-share-alt" aria-hidden="true"></i>
                </span>
              </div>
            
          </div>
        
        </div>
      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
              <a href="/2019/08/21/dlrm/" rel="next" title="dlrm">
                <i class="fa fa-chevron-left"></i> dlrm
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
              <a href="/2019/08/23/pytorch-tools/" rel="prev" title="【notes】pytorch学习笔记4-pytorch常用工具">
                【notes】pytorch学习笔记4-pytorch常用工具 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
    </footer>
  </div>
  
  
  
  </article>

  </div>


          </div>
          
    
    
  <div class="comments" id="comments">
    <div id="lv-container" data-id="city" data-uid="MTAyMC80NTY1OS8yMjE3MA=="></div>
  </div>
  
  


        </div>
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">

          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image"
      src="/images/avatar.jpg"
      alt="Zihan.W">
  <p class="site-author-name" itemprop="name">Zihan.W</p>
  <div class="site-description motion-element" itemprop="description">Download something from my mind when thinking!</div>
</div>
  <nav class="site-state motion-element">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">24</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">分类</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-tags">
        
          
            <a href="/tags/">
          
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">18</span>
        <span class="site-state-item-name">标签</span>
        </a>
      </div>
    
  </nav>
  <div class="feed-link motion-element">
    <a href="/atom.xml" rel="alternate">
      <i class="fa fa-rss"></i>RSS
    </a>
  </div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://github.com/zinw623" title="GitHub &rarr; https://github.com/zinw623" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://www.jianshu.com/u/a38373c2e45b" title="简书 &rarr; https://www.jianshu.com/u/a38373c2e45b" rel="noopener" target="_blank"><i class="fa fa-fw fa-heartbeat"></i>简书</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://juejin.im/user/5d40058ef265da03b76afb66" title="掘金 &rarr; https://juejin.im/user/5d40058ef265da03b76afb66" rel="noopener" target="_blank"><i class="fa fa-fw fa-spinner"></i>掘金</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://www.zhihu.com/people/tai-li-wu-yan-zu/activities" title="知乎 &rarr; https://www.zhihu.com/people/tai-li-wu-yan-zu/activities" rel="noopener" target="_blank"><i class="fa fa-fw fa-eye"></i>知乎</a>
      </span>
    
  </div>


  <div class="links-of-blogroll motion-element links-of-blogroll-inline">
    <div class="links-of-blogroll-title">
      <i class="fa  fa-fw fa-link"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.liaoxuefeng.com/wiki/1016959663602400" title="https://www.liaoxuefeng.com/wiki/1016959663602400" rel="noopener" target="_blank">廖雪峰python</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="https://docs.python.org/zh-cn/3/" title="https://docs.python.org/zh-cn/3/" rel="noopener" target="_blank">python中文文档</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="https://pytorch-cn.readthedocs.io/zh/latest/" title="https://pytorch-cn.readthedocs.io/zh/latest/" rel="noopener" target="_blank">pytorch中文文档</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="https://mist.theme-next.org/docs/" title="https://mist.theme-next.org/docs/" rel="noopener" target="_blank">next doc</a>
        </li>
      
    </ul>
  </div>

        </div>
      </div>
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#神经网络工具箱nn"><span class="nav-number">1.</span> <span class="nav-text">神经网络工具箱nn</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#nn-Module"><span class="nav-number">1.1.</span> <span class="nav-text">nn.Module</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#常用的神经网络层"><span class="nav-number">1.2.</span> <span class="nav-text">常用的神经网络层</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#图像相关层"><span class="nav-number">1.2.1.</span> <span class="nav-text">图像相关层</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#激活函数"><span class="nav-number">1.2.2.</span> <span class="nav-text">激活函数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#损失函数"><span class="nav-number">1.2.3.</span> <span class="nav-text">损失函数</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#优化器"><span class="nav-number">1.3.</span> <span class="nav-text">优化器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#nn-functional"><span class="nav-number">1.4.</span> <span class="nav-text">nn.functional</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#初始化策略"><span class="nav-number">1.5.</span> <span class="nav-text">初始化策略</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#nn-Module深入分析"><span class="nav-number">1.6.</span> <span class="nav-text">nn.Module深入分析</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#nn和aautograd的关系"><span class="nav-number">1.7.</span> <span class="nav-text">nn和aautograd的关系</span></a></li></ol></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zihan.W</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    <span title="站点总字数">135k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">2:03</span>
</div>


<span id="sitetime"></span>
<script language=javascript>
	function siteTime(){
		window.setTimeout("siteTime()", 1000);
		var seconds = 1000;
		var minutes = seconds * 60;
		var hours = minutes * 60;
		var days = hours * 24;
		var years = days * 365;
		var today = new Date();
		var todayYear = today.getFullYear();
		var todayMonth = today.getMonth()+1;
		var todayDate = today.getDate();
		var todayHour = today.getHours();
		var todayMinute = today.getMinutes();
		var todaySecond = today.getSeconds();
		/* Date.UTC() -- 返回date对象距世界标准时间(UTC)1970年1月1日午夜之间的毫秒数(时间戳)
		year - 作为date对象的年份，为4位年份值
		month - 0-11之间的整数，做为date对象的月份
		day - 1-31之间的整数，做为date对象的天数
		hours - 0(午夜24点)-23之间的整数，做为date对象的小时数
		minutes - 0-59之间的整数，做为date对象的分钟数
		seconds - 0-59之间的整数，做为date对象的秒数
		microseconds - 0-999之间的整数，做为date对象的毫秒数 */
		var t1 = Date.UTC(2019,07,30,15,00,00); 
		var t2 = Date.UTC(todayYear,todayMonth,todayDate,todayHour,todayMinute,todaySecond);
		var diff = t2-t1;
		var diffYears = Math.floor(diff/years);
		var diffDays = Math.floor((diff/days)-diffYears*365);
		var diffHours = Math.floor((diff-(diffYears*365+diffDays)*days)/hours);
		var diffMinutes = Math.floor((diff-(diffYears*365+diffDays)*days-diffHours*hours)/minutes);
		var diffSeconds = Math.floor((diff-(diffYears*365+diffDays)*days-diffHours*hours-diffMinutes*minutes)/seconds);
		document.getElementById("sitetime").innerHTML=" 已运行"+/*diffYears+" 年 "+*/diffDays+" 天 "+diffHours+" 小时 "+diffMinutes+" 分钟 "+diffSeconds+" 秒";
	}/*因为建站时间还没有一年，就将之注释掉了。需要的可以取消*/
	siteTime();
</script>

        




  <script>
    (function() {
      var hm = document.createElement("script");
      hm.src = "//tajs.qq.com/stats?sId=66456019";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




        
      </div>
    </footer>
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
      </div>

    
  
  <div id="needsharebutton-float">
    <span class="btn">
      <i class="fa fa-share-alt" aria-hidden="true"></i>
    </span>
  </div>
<script src="/lib/needsharebutton/needsharebutton.js"></script>
<script>
    pbOptions = {};
      pbOptions.iconStyle = "box";
    
      pbOptions.boxForm = "horizontal";
    
      pbOptions.position = "bottomCenter";
    
      pbOptions.networks = "Weibo,Wechat,Douban,QQZone,Twitter,Facebook";
    
    new needShareButton('#needsharebutton-postbottom', pbOptions);
    flOptions = {};
      flOptions.iconStyle = "box";
    
      flOptions.boxForm = "horizontal";
    
      flOptions.position = "middleRight";
    
      flOptions.networks = "Weibo,Wechat,Douban,QQZone,Twitter,Facebook";
    
    new needShareButton('#needsharebutton-float', flOptions);
</script>


  </div>

  
    
    
  
  <script color='23,23,23' opacity='0.7' zIndex='-2' count='120' src="/lib/canvas-nest/canvas-nest-nomobile.min.js"></script>
  <script src="/lib/jquery/index.js?v=3.4.1"></script>
  <script src="/lib/mediumzoom/medium-zoom.min.js"></script>
  <script src="/lib/lazyload/lozad.min.js?v=1.10.0"></script>
  <script src="/lib/reading_progress/reading_progress.js"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  <script src="/js/utils.js?v=7.2.0"></script>
  <script src="/js/motion.js?v=7.2.0"></script>

  
  <script src="/js/affix.js?v=7.2.0"></script>
  <script src="/js/schemes/pisces.js?v=7.2.0"></script>



  
  <script src="/js/scrollspy.js?v=7.2.0"></script>
<script src="/js/post-details.js?v=7.2.0"></script>



  <script src="/js/next-boot.js?v=7.2.0"></script>

  
  <script src="/js/js.cookie.js?v=7.2.0"></script>
  <script src="/js/scroll-cookie.js?v=7.2.0"></script>

  

  


  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>





  
    
      <script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script src="//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    
  




  <script src="/lib/bookmark/bookmark.min.js?v=1.0"></script>
  <script>
    bookmark.scrollToMark('auto', "#更多");
  
  </script>















  <script src="/js/local-search.js?v=7.2.0"></script>










<script>
if ($('body').find('pre.mermaid').length) {
  $.ajax({
    type: 'GET',
    url: '//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js',
    dataType: 'script',
    cache: true,
    success: function() {
      mermaid.initialize({
        theme: 'neutral',
        logLevel: 3,
        flowchart: { curve: 'linear' },
        gantt: { axisFormat: '%m/%d/%Y' },
        sequence: { actorMargin: 50 }
      });
    }
  });
}
</script>



    

<script>
  window.livereOptions = {
    refer: '2019/08/22/nn.Module/'
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
</script>


</body>
</html>
