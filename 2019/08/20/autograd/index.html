<!DOCTYPE html>





<html class="theme-next mist use-motion" lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="baidu-site-verification" content="Mp17ppyPmu" />
<meta name="generator" content="Hexo 3.9.0">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.2.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.2.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.2.0">
  <link rel="mask-icon" href="/images/logo.svg?v=7.2.0" color="#222">

<link rel="stylesheet" href="/css/main.css?v=7.2.0">

<link rel="stylesheet" href="//fonts.proxy.ustclug.org/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">
  <link rel="stylesheet" href="/lib/needsharebutton/needsharebutton.css">
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2">


<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '7.2.0',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":true},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    fancybox: false,
    mediumzoom: true,
    lazyload: true,
    pangu: false,
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    search: {
      root: '/',
      path: 'search.xml'
    },
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    }
  };
</script>


  <script>
  (function(i,s,o,g,r,a,m){i["DaoVoiceObject"]=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;a.charset="utf-8";m.parentNode.insertBefore(a,m)})(window,document,"script",('https:' == document.location.protocol ? 'https:' : 'http:') + "//widget.daovoice.io/widget/0f81ff2f.js","daovoice")
  daovoice('init', {
      app_id: "4d92f510"
    });
  daovoice('update');
  </script>

  <meta name="description" content="autogradtorch.autograd是为方便用户使用，专门开发的一套自动求导引擎，它能够根据输入和前向传播过程自动构建计算图，并执行反向传播。 计算图是现代深度学习框架的核心，它为自动求导算法——反向传播提供了理论支持。 Variablepytorch在autograd模块中实现计算图相关功能，autograd中的核心数据结构是Variable。Variable封装了tensor，并记录对">
<meta name="keywords" content="笔记,pytorch">
<meta property="og:type" content="article">
<meta property="og:title" content="【notes】pytorch学习笔记2-autograd部分">
<meta property="og:url" content="https://zinw623.github.io/2019/08/20/autograd/index.html">
<meta property="og:site_name" content="Zihan.W的博客">
<meta property="og:description" content="autogradtorch.autograd是为方便用户使用，专门开发的一套自动求导引擎，它能够根据输入和前向传播过程自动构建计算图，并执行反向传播。 计算图是现代深度学习框架的核心，它为自动求导算法——反向传播提供了理论支持。 Variablepytorch在autograd模块中实现计算图相关功能，autograd中的核心数据结构是Variable。Variable封装了tensor，并记录对">
<meta property="og:locale" content="zh-CN">
<meta property="og:updated_time" content="2019-08-20T09:24:42.211Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="【notes】pytorch学习笔记2-autograd部分">
<meta name="twitter:description" content="autogradtorch.autograd是为方便用户使用，专门开发的一套自动求导引擎，它能够根据输入和前向传播过程自动构建计算图，并执行反向传播。 计算图是现代深度学习框架的核心，它为自动求导算法——反向传播提供了理论支持。 Variablepytorch在autograd模块中实现计算图相关功能，autograd中的核心数据结构是Variable。Variable封装了tensor，并记录对">
  <link rel="alternate" href="/atom.xml" title="Zihan.W的博客" type="application/atom+xml">
  <link rel="canonical" href="https://zinw623.github.io/2019/08/20/autograd/">


<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>【notes】pytorch学习笔记2-autograd部分 | Zihan.W的博客</title>
  


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?Mp17ppyPmu";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>
  <meta name="baidu-site-verification" content="Mp17ppyPmu" />






  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Zihan.W的博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
    <ul id="menu" class="menu">
        
        
        
          
          <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br/>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">
      
    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br/>关于</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">
      
    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br/>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br/>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br/>归档</a>

  </li>
        <li class="menu-item menu-item-search">
          <a href="javascript:;" class="popup-trigger">
          
            <i class="menu-item-icon fa fa-search fa-fw"></i> <br/>搜索</a>
        </li>
      
    </ul>
    

    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>


    </div>
</nav>

</div>
    </header>

    

  <a href="https://github.com/zinw623" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    
    <div class="reading-progress-bar"></div>

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://zinw623.github.io/2019/08/20/autograd/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zihan.W">
      <meta itemprop="description" content="Download something from my mind when thinking!">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zihan.W的博客">
    </span>
      <header class="post-header">

        
          <h1 class="post-title" itemprop="name headline">【notes】pytorch学习笔记2-autograd部分

              
            
          </h1>
        

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-08-20 17:30:45 / 修改时间：17:24:42" itemprop="dateCreated datePublished" datetime="2019-08-20T17:30:45+08:00">2019-08-20</time>
            </span>
          
            

            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/笔记/" itemprop="url" rel="index"><span itemprop="name">笔记</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/笔记/pytorch/" itemprop="url" rel="index"><span itemprop="name">pytorch</span></a></span>

                
                
              
            </span>
          

          <br/>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
              
                <span class="post-meta-item-text">本文字数：</span>
              
              <span title="本文字数">12k</span>
            </span>
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
              
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              
              <span title="阅读时长">11 分钟</span>
            </span>
          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h3 id="autograd"><a href="#autograd" class="headerlink" title="autograd"></a>autograd</h3><p>torch.autograd是为方便用户使用，专门开发的一套自动求导引擎，它能够根据输入和前向传播过程自动构建计算图，并执行反向传播。</p>
<p>计算图是现代深度学习框架的核心，它为自动求导算法——反向传播提供了理论支持。</p>
<h4 id="Variable"><a href="#Variable" class="headerlink" title="Variable"></a>Variable</h4><p>pytorch在autograd模块中实现计算图相关功能，autograd中的核心数据结构是Variable。Variable封装了tensor，并记录对tensor的操作记录用来构建计算图。Variale的数据结构如图：</p>
<pre class="mermaid" style="text-align: center;">
            
            graph LR;
A[autograd.Variable] --> B(data);
A[autograd.Variable] --> C(grad);
A[autograd.Variable] --> D(grad_fn);
          </pre>
<p>Variable的构造函数需要传入tensor，同时有两个可选参数。</p>
<ul>
<li>require_grad(bool)：是否需要对该variable进行求导。</li>
<li>volatile(bool)：意为“挥发”，设置为True，构建在该variable之上的图都不会求导，转为推理阶段设计。</li>
</ul>
<p>Variable支持大部分tensor支持的函数，但其不支持部分inplace函数，因为这些函数会修改tensor自身，而在反向传播中，variable需要缓存原来的tensor来计算梯度。如果想要计算各个Variable的梯度，只需调用根节点variable的backward方法，autograd会自动沿着计算图反向传播，计算每一个叶子节点的梯度。</p>
<ul>
<li>grad_variables：形状与variable一致，对于y.backward()，grad_variables相当于链式法则$\frac{\partial{z}}{\partial{x}}=\frac{\partial{z}}{\partial{y}}\times\frac{\partial{y}}{\partial{x}}$中的$\frac{\partial{z}}{\partial{y}}$。grad_variables也可以是tensor或序列。</li>
</ul>
<ul>
<li>retain_graph：反向传播需要缓存一些中间结果，反向传播之后，这些缓存就被清空，可通过指定这个参数不清空缓存，用来多次反向传播。</li>
</ul>
<ul>
<li>create_graph：对反向传播过程再次构建计算图，可通过backward of backward实现求高阶导数。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable <span class="keyword">as</span> V</span><br><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从tensor中创建variable，指定需要求导</span></span><br><span class="line"></span><br><span class="line">a = V(t.ones(<span class="number">3</span>, <span class="number">4</span>), requires_grad = <span class="literal">True</span>);a</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.]], requires_grad=True)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">b = V(t.zeros(<span class="number">3</span>, <span class="number">4</span>));b</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[0., 0., 0., 0.],
        [0., 0., 0., 0.],
        [0., 0., 0., 0.]])
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 函数的使用和tensor一致</span></span><br><span class="line"><span class="comment"># 也可写成c = a + b</span></span><br><span class="line">c = a.add(b)</span><br><span class="line">c</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.]], grad_fn=&lt;AddBackward0&gt;)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">d = c.sum()</span><br><span class="line">d.backward() <span class="comment"># 反向传播</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 注意二者的区别</span></span><br><span class="line"><span class="comment"># 前者在取data后变为tensor，从tensor计算sum得到float</span></span><br><span class="line"><span class="comment"># 后者计算sum后仍然是Variable</span></span><br><span class="line">c.data.sum(), c.sum()</span><br></pre></td></tr></table></figure>
<pre><code>(tensor(12.), tensor(12., grad_fn=&lt;SumBackward0&gt;))
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a.grad</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.]])
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 此处虽然没有指定c需要求导，但c依赖于a，而a需要求导</span></span><br><span class="line"><span class="comment"># 因此c的requires_grad属性会自动设为True</span></span><br><span class="line">a.requires_grad, b.requires_grad, c.requires_grad</span><br></pre></td></tr></table></figure>
<pre><code>(True, False, True)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># c.grad是None，c不是叶子节点，他的梯度是用来计算a的梯度</span></span><br><span class="line"><span class="comment"># 虽然c.requires_grad = True，但其梯度计算完之后即被释放</span></span><br><span class="line">c.grad <span class="keyword">is</span> <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<pre><code>True
</code></pre><p>接下来看看autograd计算导数和我们手动推导的导数的区别。</p>
<script type="math/tex; mode=display">y=x^2e^x</script><p>它的导函数是：</p>
<script type="math/tex; mode=display">\frac{\partial{y}}{\partial{x}}=2xe^x+x^2e^x</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">    y = x**<span class="number">2</span> * t.exp(x)</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradf</span><span class="params">(x)</span>:</span></span><br><span class="line">    dx = <span class="number">2</span>*x*t.exp(x) + x**<span class="number">2</span>*t.exp(x)</span><br><span class="line">    <span class="keyword">return</span> dx</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = V(t.randn(<span class="number">3</span>, <span class="number">4</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">y = f(x)</span><br><span class="line">y</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[1.3949e-01, 2.7201e-01, 4.9848e-01, 2.2968e+00],
        [3.2033e-01, 3.3618e-01, 2.3554e-02, 1.0507e+01],
        [3.9416e+01, 3.5322e+00, 9.6847e-02, 1.2743e+01]],
       grad_fn=&lt;MulBackward0&gt;)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y.backward(t.ones(y.size())) <span class="comment"># grad_variables形状与y一致</span></span><br><span class="line">x.grad</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[ 1.0154, -0.4398, -0.1755,  7.1583],
        [-0.4095,  1.7961,  0.3532, 24.3531],
        [76.1412, 10.0143, -0.4190, 28.6505]])
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># autograd的计算结果与利用公式手动计算的结果一致</span></span><br><span class="line">gradf(x)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[ 1.0154, -0.4398, -0.1755,  7.1583],
        [-0.4095,  1.7961,  0.3532, 24.3531],
        [76.1412, 10.0143, -0.4190, 28.6505]], grad_fn=&lt;AddBackward0&gt;)
</code></pre><h4 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h4><p>pytorch中autograd的底层采用了计算图，计算图是一种特殊的有向无环图（DAG），用于记录算子与变量之间的关系。一般用矩形表示算子，椭圆形表示变量。如表达式$\boldsymbol{z}=\boldsymbol{wx}+\boldsymbol{b}$可分解为$\boldsymbol{y}=\boldsymbol{wx}$和$\boldsymbol{z}=\boldsymbol{y}+\boldsymbol{b}$，其计算图如图所示，图中的MUL和ADD都是算子，$\boldsymbol{w}$、$\boldsymbol{x}$、$\boldsymbol{b}$为变量。</p>
<pre class="mermaid" style="text-align: center;">
            
            graph LR;
A((W)) --> C[MUL];
B((X)) --> C[MUL];
C[MUL] --> D((y));
E((b)) --> F[Add];
D((y)) --> F[Add];
F[Add] --> G((z));
          </pre>
<p>如上有向无环图，$\boldsymbol{X}$和$\boldsymbol{b}$是叶子节点，这些节点通常有用户自己创建，不依赖于其他变量。$\boldsymbol{z}$称为根节点，是计算图的最终目标。利用链式法则很容易求各个叶子节点的梯度。</p>
<script type="math/tex; mode=display">\frac{\partial{z}}{\partial{b}}=1,\frac{\partial{z}}{\partial{y}}=1\\\frac{\partial{y}}{\partial{w}}=x,\frac{\partial{y}}{\partial{x}}=w\\\frac{\partial{z}}{\partial{x}}=\frac{\partial{z}}{\partial{y}}\frac{\partial{y}}{\partial{x}}=1*w\\\frac{\partial{z}}{\partial{w}}=\frac{\partial{z}}{\partial{y}}\frac{\partial{y}}{\partial{w}}=1*x</script><p>有了计算图上述链式求导可自动利用计算图的反向传播自动完成，其过程如图所示：</p>
<pre class="mermaid" style="text-align: center;">
            
            graph LR;
A((dz)) --> B[addBackward];
B[addBackward] --> C((dy));
B[addBackward] --> D((db));
C((dy)) --> E[mulBackward];
E[mulBackward] --> F((dX));
E[mulBackward] --> G((dW));
          </pre>
<p>图中记录了操作function，每个变量在图中的位置可通过其grad_fn属性在图中的位置推测得到。在反向传播的过程中，autograd沿着这个图从当前变量（根节点z）溯源，可以利用链式求导法则计算所有叶子节点的梯度。</p>
<p>每一个前向传播操作的函数都有与之对应的反向传播函数用来计算输入的各个variable的梯度，这些函数的函数名通常以Backward结尾。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = V(t.ones(<span class="number">1</span>))</span><br><span class="line">b = V(t.rand(<span class="number">1</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">w = V(t.rand(<span class="number">1</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">y = w * x <span class="comment"># 等价于y = w.mul(x)</span></span><br><span class="line">z = y + b <span class="comment"># 等价于z = y.add(b)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.requires_grad, b.requires_grad, w.requires_grad</span><br></pre></td></tr></table></figure>
<pre><code>(False, True, True)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 虽然未指定y.requires_grad为True，但由于y依赖于需要求导的w</span></span><br><span class="line"><span class="comment"># 故而y.requires_grad为True</span></span><br><span class="line">y.requires_grad</span><br></pre></td></tr></table></figure>
<pre><code>True
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.is_leaf, w.is_leaf, b.is_leaf</span><br></pre></td></tr></table></figure>
<pre><code>(True, True, True)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y.is_leaf, z.is_leaf</span><br></pre></td></tr></table></figure>
<pre><code>(False, False)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># grad_fn可以查看这个variable的反向传播函数</span></span><br><span class="line"><span class="comment"># z是add函数的输出，所以它的反向传播函数是AddBackward</span></span><br><span class="line">z.grad_fn</span><br></pre></td></tr></table></figure>
<pre><code>&lt;AddBackward0 at 0x7f073e390e80&gt;
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#next_function保存grad_fn的输入，grad_fn的输入是一个tuple</span></span><br><span class="line"><span class="comment"># 第一个是y，它是乘法（mul）的输出，所以对应的反向传播函数y.grad_fn是MulBackward</span></span><br><span class="line"><span class="comment"># 第二个是b，它是叶子节点，由用户创建，grad_fn为None，但是有</span></span><br><span class="line">z.grad_fn.next_functions</span><br></pre></td></tr></table></figure>
<pre><code>((&lt;MulBackward0 at 0x7f073e390828&gt;, 0),
 (&lt;AccumulateGrad at 0x7f073e390f28&gt;, 0))
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># variable的grad_fn对应着图中的function</span></span><br><span class="line">z.grad_fn.next_functions[<span class="number">0</span>][<span class="number">0</span>] == y.grad_fn</span><br></pre></td></tr></table></figure>
<pre><code>True
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第一个是w，叶子节点，需要求导，梯度是累加的</span></span><br><span class="line"><span class="comment"># 第二个是x，叶子节点，不需要求导，所以为None</span></span><br><span class="line">y.grad_fn.next_functions</span><br></pre></td></tr></table></figure>
<pre><code>((&lt;AccumulateGrad at 0x7f073e390470&gt;, 0), (None, 0))
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 叶子节点的grad_fn是None</span></span><br><span class="line">w.grad_fn, x.grad_fn</span><br></pre></td></tr></table></figure>
<pre><code>(None, None)
</code></pre><p>计算$\boldsymbol{w}$的梯度时需要用到$\boldsymbol{x}$的数值（$\frac{\partial{y}}{\partial{w}}=x$），这些数值在前向过程中会保存成buffer，在计算完梯度之后会自动清空。为了能够多次反向传播需要指定retain_graph来保留这些buffer。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y.grad_fn.saved_variables</span><br></pre></td></tr></table></figure>
<pre><code>---------------------------------------------------------------------------

AttributeError                            Traceback (most recent call last)

&lt;ipython-input-30-284a59926bb7&gt; in &lt;module&gt;
----&gt; 1 y.grad_fn.saved_variables


AttributeError: &#39;MulBackward0&#39; object has no attribute &#39;saved_variables&#39;
</code></pre><p>原因确实是版本问题，PyTorch0.3 中把许多python的操作转移到了C++中，saved_variables 现在是一个c++的对象，无法通过python访问。<a href="https://github.com/chenyuntc/pytorch-book/issues/7" target="_blank" rel="noopener">https://github.com/chenyuntc/pytorch-book/issues/7</a></p>
<p>可以查看这里进行学习<a href="https://github.com/chenyuntc/pytorch-book/blob/0.3/chapter3-Tensor" target="_blank" rel="noopener">https://github.com/chenyuntc/pytorch-book/blob/0.3/chapter3-Tensor</a>和autograd/Autograd.ipynb,省掉上面的操作</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用retain_graph保存buffer</span></span><br><span class="line">z.backward(retain_graph = <span class="literal">True</span>)</span><br><span class="line">w.grad</span><br></pre></td></tr></table></figure>
<pre><code>tensor([1.])
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 多次反向传播，梯度累加，这也就是w中AccumulateGrad标识的含义</span></span><br><span class="line">z.backward()</span><br><span class="line">w.grad</span><br></pre></td></tr></table></figure>
<pre><code>tensor([2.])
</code></pre><p>pytorch使用的是动态图，它的计算图在每次前向传播时都是从头开始构建的，所以它能够使用python的控制语句（如for、if等），根据需求创建计算图。这一点在自然语言处理领域中很有用，它意为你不需要事先构建所有可能用到的图的路径，图在运行时才构建。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">abs</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> x.data[<span class="number">0</span>] &gt; <span class="number">0</span>: <span class="keyword">return</span> x</span><br><span class="line">    <span class="keyword">else</span>: <span class="keyword">return</span> -x</span><br><span class="line">x = V(t.ones(<span class="number">1</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">y = abs(x)</span><br><span class="line">y.backward()</span><br><span class="line">x.grad</span><br></pre></td></tr></table></figure>
<pre><code>tensor([1.])
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = V(<span class="number">-1</span> * t.ones(<span class="number">1</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">y = abs(x)</span><br><span class="line">y.backward()</span><br><span class="line">print(x.grad)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([-1.])
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">    result = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> ii <span class="keyword">in</span> x:</span><br><span class="line">        <span class="keyword">if</span> ii.data &gt; <span class="number">0</span>: result = ii * result</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line">x = V(t.arange(<span class="number">-2</span>, <span class="number">4</span>).float(), requires_grad = <span class="literal">True</span>)</span><br><span class="line">y = f(x) <span class="comment"># y = x[3] * x[4] * x[5]</span></span><br><span class="line">y.backward()</span><br><span class="line">x.grad</span><br></pre></td></tr></table></figure>
<pre><code>tensor([0., 0., 0., 6., 3., 2.])
</code></pre><p>变量的requires_grad属性默认为False，如果某一个节点requires_grad被设置为True，那么所有依赖它的节点requires_grad都是True。</p>
<p>with torch.no_grad()内的variable均为不会求导，其优先级高于requires_grad。函数可以用装饰器@torch.no_grad()。可实现一定程度的速度提升，并节省约一半显存，因为其不需要分配空间保存梯度。</p>
<p>详细内容可见：<a href="https://pytorch.org/docs/master/autograd.html#locally-disable-grad" target="_blank" rel="noopener">https://pytorch.org/docs/master/autograd.html#locally-disable-grad</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = t.tensor([<span class="number">1.</span>], requires_grad = <span class="literal">True</span>)</span><br><span class="line"><span class="keyword">with</span> t.no_grad():</span><br><span class="line">    y = x * <span class="number">2</span></span><br><span class="line">y.requires_grad</span><br></pre></td></tr></table></figure>
<pre><code>False
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@t.no_grad()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">doubler</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x * <span class="number">2</span></span><br><span class="line">z = doubler(x)</span><br><span class="line">z.requires_grad</span><br></pre></td></tr></table></figure>
<pre><code>False
</code></pre><p>在反向传播过程中非叶子节点的导数计算完之后即被清空。若想查看这些变量的梯度，有以下两种方法：</p>
<ul>
<li>使用autograd.grad函数</li>
<li>使用hook</li>
</ul>
<p>推荐使用hook方法，但在实际使用中应尽量避免修改grad的值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = V(t.ones(<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">w = V(t.rand(<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">y = x * w</span><br><span class="line"><span class="comment"># y依赖于w，而w.requires_grad = True</span></span><br><span class="line">z = y.sum()</span><br><span class="line">x.requires_grad, w.requires_grad, y.requires_grad</span><br></pre></td></tr></table></figure>
<pre><code>(True, True, True)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 非叶子节点grad计算完之后自动清空，y.grad是None</span></span><br><span class="line">z.backward()</span><br><span class="line">x.grad, w.grad, y.grad</span><br></pre></td></tr></table></figure>
<pre><code>(tensor([0.1283, 0.8326, 0.6539]), tensor([1., 1., 1.]), None)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第一种方法：使用grad获取中间变量的梯度</span></span><br><span class="line">x = V(t.ones(<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">w = V(t.rand(<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">y = x * w</span><br><span class="line"><span class="comment"># y依赖于w，而w.requires_grad = True</span></span><br><span class="line">z = y.sum()</span><br><span class="line"><span class="comment"># z对y的梯度，隐式调用backward()</span></span><br><span class="line">t.autograd.grad(z, y)</span><br></pre></td></tr></table></figure>
<pre><code>(tensor([1., 1., 1.]),)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第二种方法：使用hook</span></span><br><span class="line"><span class="comment"># hook是一个函数，输入是梯度，不应该有返回值</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">variable_hook</span><span class="params">(grad)</span>:</span></span><br><span class="line">    print(<span class="string">'y的梯度：\r\n'</span>,grad)</span><br><span class="line">    </span><br><span class="line">x = V(t.ones(<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">w = V(t.rand(<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">y = x * w</span><br><span class="line"><span class="comment"># 注册hook</span></span><br><span class="line">hook_handle = y.register_hook(variable_hook)</span><br><span class="line">z = y.sum()</span><br><span class="line">z.backward()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 除非你每次都要用hook，否则用完之后记得移除hook</span></span><br><span class="line">hook_handle.remove()</span><br></pre></td></tr></table></figure>
<pre><code>y的梯度：
 tensor([1., 1., 1.])
</code></pre><p>最后再来看看variable中grad属性和backward函数grad_variables参数的含义。</p>
<ul>
<li>variables $\boldsymbol{x}$ 的梯度是目标函数$f(x)$对$\boldsymbol{x}$的梯度，形状与$\boldsymbol{x}$一致。</li>
<li>y.backward(grad_variables)中grad_variables相当于链式法则中的$\frac{\partial{z}}{\partial{x}}=\frac{\partial{z}}{\partial{y}}\frac{\partial{y}}{\partial{x}}$中的$\frac{\partial{z}}{\partial{y}}$。z是目标函数，一般是个标量，故而$\frac{\partial{z}}{\partial{y}}$的形状与$\boldsymbol{y}$的形状一致。z.backward()等价于y.backward(grad_y)。而z.backward()省略了grad_variables参数，是因为z是个标量，而$\frac{\partial{z}}{\partial{z}}=1$</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = V(t.arange(<span class="number">0</span>, <span class="number">3</span>).float(), requires_grad = <span class="literal">True</span>)</span><br><span class="line">y = x**<span class="number">2</span> + x*<span class="number">2</span></span><br><span class="line">z = y.sum()</span><br><span class="line">z.backward()</span><br><span class="line">x.grad</span><br></pre></td></tr></table></figure>
<pre><code>tensor([2., 4., 6.])
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = V(t.arange(<span class="number">0</span>, <span class="number">3</span>).float(), requires_grad = <span class="literal">True</span>)</span><br><span class="line">y = x**<span class="number">2</span> + x*<span class="number">2</span></span><br><span class="line">z = y.sum()</span><br><span class="line">y_grad_variables = V(t.Tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])) </span><br><span class="line">y.backward(y_grad_variables)</span><br><span class="line">x.grad</span><br></pre></td></tr></table></figure>
<pre><code>tensor([2., 4., 6.])
</code></pre><p>值得注意的是，只有对variable的操作才能使用autograd，如果variable的data直接进行操作，将无法使用反向传播。除了参数初始化，一般我们不会直接修改variable.data的值。</p>
<p>在pytorch中计算图的特点总结如下：</p>
<ul>
<li>autograd根据用户对variable的操作构建计算图。对variable的操作抽象为Function。</li>
<li>由用户创建的节点称为叶子节点，叶子节点的grad_fn为None。叶子节点中需要求导的variable，具有AccumulateGrad标识，因其梯度是累加的。</li>
<li>variable默认是不需要求导的，即requires_grad属性默认为False。如果某一个节点requires_grad被设置为True，那么所有依赖它的节点requires_grad都为True。</li>
<li>with torch.no_grad和@torch.no_grad()的作用下的节点都不会求导，优先级比requires_grad高。</li>
<li>多次反向传播时，梯度是累加的。反向传播的中间缓存会被清空，为进行多次反向传播需指定retain_graph=True来保存这些缓存。</li>
<li>非叶子节点的梯度计算完后即被清空，可以使用autograd.grad或hook技术获取非叶子节点梯度的值。</li>
<li>variable的grad与data形状一致，应避免直接修改variable.data，因为对data的直接操作无法利用autograd进行反向传播。</li>
<li>反向传播函数backward的参数grad_variables可以看成链式求导的中间结果，如果是标量，可以省略，默认为1。</li>
<li>pytorch采用动态图设计，可以很方便地查看中间层的输出，动态地设计计算图结构。</li>
</ul>
<h4 id="扩展autograd"><a href="#扩展autograd" class="headerlink" title="扩展autograd"></a>扩展autograd</h4><p>目前绝大多数函数都可以使用autograd实现反向求导，但如果需要自己写一个复杂的函数，不支持自动反向求导怎么办？答案是写一个Function，实现它的前向传播和反向传播代码，Function对应于计算图中的矩形，它接受参数，计算并返回结果。下面给出了一个例子。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Function</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiplyAdd</span><span class="params">(Function)</span>:</span></span><br><span class="line">    </span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(ctx, w, x, b)</span>:</span></span><br><span class="line">        print(<span class="string">'type in forward'</span>, type(x))</span><br><span class="line">        ctx.save_for_backward(w, x)<span class="comment">#存储用来反向传播的参数</span></span><br><span class="line">        output = w*x +b</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(ctx, grad_output)</span>:</span></span><br><span class="line">        w, x = ctx.saved_tensors <span class="comment"># 老版本是saved_variables</span></span><br><span class="line">        print(<span class="string">'type in backward'</span>,type(x))</span><br><span class="line">        grad_w = grad_output * x</span><br><span class="line">        grad_x = grad_output * w</span><br><span class="line">        grad_b = grad_output * <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> grad_w, grad_x, grad_b</span><br></pre></td></tr></table></figure>
<p>分析如下：</p>
<ul>
<li>自定义的Function需要继承autograd.Function，没有构造函数<strong>init</strong>，forward和backward函数都是静态方法</li>
<li>forward函数的输入和输出都是Tensor，backward函数的输入和输出都是Variable</li>
<li>backward函数的输出和forward函数的输入一一对应，backward函数的输入和forward函数的输出一一对应</li>
<li>backward函数的grad_output参数即t.autograd.backward中的grad_variables</li>
<li>如果某一个输入不需要求导，直接返回None，如forward中的输入参数x_requires_grad显然无法对它求导，直接返回None即可</li>
<li>反向传播可能需要利用前向传播的某些中间结果，需要进行保存，否则前向传播结束后这些对象即被释放</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">x = V(t.ones(<span class="number">1</span>))</span><br><span class="line">w = V(t.rand(<span class="number">1</span>),requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = V(t.rand(<span class="number">1</span>),requires_grad=<span class="literal">True</span>)</span><br><span class="line">print(<span class="string">'开始前向传播'</span>)</span><br><span class="line">z = MultiplyAdd.apply(w, x, b)</span><br><span class="line">print(<span class="string">'开始反向传播'</span>)</span><br><span class="line">z.backward()</span><br><span class="line"></span><br><span class="line"><span class="comment"># x不需要求导，中间过程还是会计算它的导数，但随后被清空</span></span><br><span class="line">x.grad, w.grad, b.grad</span><br></pre></td></tr></table></figure>
<pre><code>开始前向传播
type in forward &lt;class &#39;torch.Tensor&#39;&gt;
开始反向传播
type in backward &lt;class &#39;torch.Tensor&#39;&gt;





(None, tensor([1.]), tensor([1.]))
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">x = V(t.ones(<span class="number">1</span>))</span><br><span class="line">w = V(t.rand(<span class="number">1</span>),requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = V(t.rand(<span class="number">1</span>),requires_grad=<span class="literal">True</span>)</span><br><span class="line">print(<span class="string">'开始前向传播'</span>)</span><br><span class="line">z = MultiplyAdd.apply(w, x, b)</span><br><span class="line">print(<span class="string">'开始反向传播'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用MultiplyAdd.backward</span></span><br><span class="line"><span class="comment"># 会自动输出grad_w, grad_x, grad_b</span></span><br><span class="line">z.grad_fn.apply(V(t.ones(<span class="number">1</span>)))</span><br></pre></td></tr></table></figure>
<pre><code>开始前向传播
type in forward &lt;class &#39;torch.Tensor&#39;&gt;
开始反向传播
type in backward &lt;class &#39;torch.Tensor&#39;&gt;





(tensor([1.]), tensor([0.5986], grad_fn=&lt;MulBackward0&gt;), tensor([1.]))
</code></pre><p>在backward函数里之所以也要对variable进行操作是为了能计算梯度的梯度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = V(t.Tensor([<span class="number">5</span>]), requires_grad = <span class="literal">True</span>)</span><br><span class="line">y = x ** <span class="number">2</span></span><br><span class="line">grad_x = t.autograd.grad(y, x, create_graph = <span class="literal">True</span>)</span><br><span class="line">grad_x</span><br></pre></td></tr></table></figure>
<pre><code>(tensor([10.], grad_fn=&lt;MulBackward0&gt;),)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grad_grad_x = t.autograd.grad(grad_x[<span class="number">0</span>],x);grad_grad_x</span><br></pre></td></tr></table></figure>
<pre><code>(tensor([2.]),)
</code></pre>
    </div>
      <p>———————————————感谢阅读———————————————</p>
<blockquote><p>欢迎收藏访问我的<a href="https://zinw623.github.io/">博客</a>  <a href="https://www.zhihu.com/people/tai-li-wu-yan-zu/activities" target="_blank" rel="noopener">知乎</a>   <a href="https://juejin.im/user/5d40058ef265da03b76afb66" target="_blank" rel="noopener">掘金</a>  <a href="https://www.jianshu.com/u/a38373c2e45b" target="_blank" rel="noopener">简书</a> <a href="https://www.zhihu.com/people/tai-li-wu-yan-zu/activities" target="_blank" rel="noopener">知乎</a></p></blockquote>
  <div class="popular-posts-header">相关文章</div>
  <ul class="popular-posts">
    <li class="popular-posts-item">
      
      
      <div class="popular-posts-title"><a href="\2019\08\22\nn.Module\" rel="bookmark">【notes】pytorch学习笔记3-神经网络工具箱nn</a></div>
      
    </li>
  
    <li class="popular-posts-item">
      
      
      <div class="popular-posts-title"><a href="\2019\08\19\pytorch-1\" rel="bookmark">【notes】pytorch学习笔记1-Tensor部分</a></div>
      
    </li>
  
    <li class="popular-posts-item">
      
      
      <div class="popular-posts-title"><a href="\2019\08\21\dlrm\" rel="bookmark">dlrm</a></div>
      
        <div class="popular-posts-excerpt"><p><blockquote>
<p>一个人在不接触对方的情况下，通过一种特殊的方式，和对方进行一系列的问答。如果在想当长时间内，他无法根据这些问题判断对方是人还是计算机，那么就可以认为这个计算机是智能的。<br></p></div>
      
    </li>
  
    <li class="popular-posts-item">
      
      
      <div class="popular-posts-title"><a href="\2019\08\09\docker-6\" rel="bookmark">【notes】docker学习笔记6-docker容器数据管理</a></div>
      
    </li>
  
    <li class="popular-posts-item">
      
      
      <div class="popular-posts-title"><a href="\2019\08\08\docker-5\" rel="bookmark">【notes】docker学习笔记5-dockerfile</a></div>
      
    </li>
  
  </ul>


    
    
    
      <div id="wechat_subscriber" style="display: block; padding: 10px 0; margin: 20px auto; width: 100%; text-align: center;">
  <img id="wechat_subscriber_qcode" src="/uploads/wechat-qcode.jpg" alt="Zihan.W wechat" style="width: 200px; max-width: 100%;">
  <div>欢迎扫描二维码订阅我的公众号！</div>
</div>

    
      <div>
        <div id="reward-container">
  <div></div>
  <button id="reward-button" disable="enable" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
        
      
      <div style="display: inline-block">
        <img src="/images/alipay.jpg" alt="Zihan.W 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>

      </div>

    <footer class="post-footer">
          
        
        <div class="post-tags">
            <a href="/tags/笔记/" rel="tag"><i class="fa fa-tag"></i> 笔记</a>
          
            <a href="/tags/pytorch/" rel="tag"><i class="fa fa-tag"></i> pytorch</a>
          
        </div>
        <div class="post-widgets">
        

        

        
          <div class="social-share">
            
            
              <div id="needsharebutton-postbottom">
                <span class="btn">
                  <i class="fa fa-share-alt" aria-hidden="true"></i>
                </span>
              </div>
            
          </div>
        
        </div>
      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
              <a href="/2019/08/19/pytorch-1/" rel="next" title="【notes】pytorch学习笔记1-Tensor部分">
                <i class="fa fa-chevron-left"></i> 【notes】pytorch学习笔记1-Tensor部分
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
              <a href="/2019/08/21/dlrm/" rel="prev" title="dlrm">
                dlrm <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
    </footer>
  </div>
  
  
  
  </article>

  </div>


          </div>
          
    
    
  <div class="comments" id="comments">
    <div id="lv-container" data-id="city" data-uid="MTAyMC80NTY1OS8yMjE3MA=="></div>
  </div>
  
  


        </div>
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">

          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image"
      src="/images/avatar.jpg"
      alt="Zihan.W">
  <p class="site-author-name" itemprop="name">Zihan.W</p>
  <div class="site-description motion-element" itemprop="description">Download something from my mind when thinking!</div>
</div>
  <nav class="site-state motion-element">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">16</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">分类</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-tags">
        
          
            <a href="/tags/">
          
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">14</span>
        <span class="site-state-item-name">标签</span>
        </a>
      </div>
    
  </nav>
  <div class="feed-link motion-element">
    <a href="/atom.xml" rel="alternate">
      <i class="fa fa-rss"></i>RSS
    </a>
  </div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://github.com/zinw623" title="GitHub &rarr; https://github.com/zinw623" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://www.jianshu.com/u/a38373c2e45b" title="简书 &rarr; https://www.jianshu.com/u/a38373c2e45b" rel="noopener" target="_blank"><i class="fa fa-fw fa-heartbeat"></i>简书</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://juejin.im/user/5d40058ef265da03b76afb66" title="掘金 &rarr; https://juejin.im/user/5d40058ef265da03b76afb66" rel="noopener" target="_blank"><i class="fa fa-fw fa-spinner"></i>掘金</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://www.zhihu.com/people/tai-li-wu-yan-zu/activities" title="知乎 &rarr; https://www.zhihu.com/people/tai-li-wu-yan-zu/activities" rel="noopener" target="_blank"><i class="fa fa-fw fa-eye"></i>知乎</a>
      </span>
    
  </div>


  <div class="links-of-blogroll motion-element links-of-blogroll-inline">
    <div class="links-of-blogroll-title">
      <i class="fa  fa-fw fa-link"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.liaoxuefeng.com/wiki/1016959663602400" title="https://www.liaoxuefeng.com/wiki/1016959663602400" rel="noopener" target="_blank">廖雪峰python</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="https://docs.python.org/zh-cn/3/" title="https://docs.python.org/zh-cn/3/" rel="noopener" target="_blank">python中文文档</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="https://pytorch-cn.readthedocs.io/zh/latest/" title="https://pytorch-cn.readthedocs.io/zh/latest/" rel="noopener" target="_blank">pytorch中文文档</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="https://mist.theme-next.org/docs/" title="https://mist.theme-next.org/docs/" rel="noopener" target="_blank">next doc</a>
        </li>
      
    </ul>
  </div>

        </div>
      </div>
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#autograd"><span class="nav-number">1.</span> <span class="nav-text">autograd</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Variable"><span class="nav-number">1.1.</span> <span class="nav-text">Variable</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#计算图"><span class="nav-number">1.2.</span> <span class="nav-text">计算图</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#扩展autograd"><span class="nav-number">1.3.</span> <span class="nav-text">扩展autograd</span></a></li></ol></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zihan.W</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    <span title="站点总字数">65k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">59 分钟</span>
</div>


<span id="sitetime"></span>
<script language=javascript>
	function siteTime(){
		window.setTimeout("siteTime()", 1000);
		var seconds = 1000;
		var minutes = seconds * 60;
		var hours = minutes * 60;
		var days = hours * 24;
		var years = days * 365;
		var today = new Date();
		var todayYear = today.getFullYear();
		var todayMonth = today.getMonth()+1;
		var todayDate = today.getDate();
		var todayHour = today.getHours();
		var todayMinute = today.getMinutes();
		var todaySecond = today.getSeconds();
		/* Date.UTC() -- 返回date对象距世界标准时间(UTC)1970年1月1日午夜之间的毫秒数(时间戳)
		year - 作为date对象的年份，为4位年份值
		month - 0-11之间的整数，做为date对象的月份
		day - 1-31之间的整数，做为date对象的天数
		hours - 0(午夜24点)-23之间的整数，做为date对象的小时数
		minutes - 0-59之间的整数，做为date对象的分钟数
		seconds - 0-59之间的整数，做为date对象的秒数
		microseconds - 0-999之间的整数，做为date对象的毫秒数 */
		var t1 = Date.UTC(2019,07,30,15,00,00); 
		var t2 = Date.UTC(todayYear,todayMonth,todayDate,todayHour,todayMinute,todaySecond);
		var diff = t2-t1;
		var diffYears = Math.floor(diff/years);
		var diffDays = Math.floor((diff/days)-diffYears*365);
		var diffHours = Math.floor((diff-(diffYears*365+diffDays)*days)/hours);
		var diffMinutes = Math.floor((diff-(diffYears*365+diffDays)*days-diffHours*hours)/minutes);
		var diffSeconds = Math.floor((diff-(diffYears*365+diffDays)*days-diffHours*hours-diffMinutes*minutes)/seconds);
		document.getElementById("sitetime").innerHTML=" 已运行"+/*diffYears+" 年 "+*/diffDays+" 天 "+diffHours+" 小时 "+diffMinutes+" 分钟 "+diffSeconds+" 秒";
	}/*因为建站时间还没有一年，就将之注释掉了。需要的可以取消*/
	siteTime();
</script>

        




  <script>
    (function() {
      var hm = document.createElement("script");
      hm.src = "//tajs.qq.com/stats?sId=66456019";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




        
      </div>
    </footer>
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
      </div>

    
  
  <div id="needsharebutton-float">
    <span class="btn">
      <i class="fa fa-share-alt" aria-hidden="true"></i>
    </span>
  </div>
<script src="/lib/needsharebutton/needsharebutton.js"></script>
<script>
    pbOptions = {};
      pbOptions.iconStyle = "box";
    
      pbOptions.boxForm = "horizontal";
    
      pbOptions.position = "bottomCenter";
    
      pbOptions.networks = "Weibo,Wechat,Douban,QQZone,Twitter,Facebook";
    
    new needShareButton('#needsharebutton-postbottom', pbOptions);
    flOptions = {};
      flOptions.iconStyle = "box";
    
      flOptions.boxForm = "horizontal";
    
      flOptions.position = "middleRight";
    
      flOptions.networks = "Weibo,Wechat,Douban,QQZone,Twitter,Facebook";
    
    new needShareButton('#needsharebutton-float', flOptions);
</script>


  </div>

  
    
    
  
  <script color='23,23,23' opacity='0.7' zIndex='-2' count='120' src="/lib/canvas-nest/canvas-nest-nomobile.min.js"></script>
  <script src="/lib/jquery/index.js?v=3.4.1"></script>
  <script src="/lib/mediumzoom/medium-zoom.min.js"></script>
  <script src="/lib/lazyload/lozad.min.js?v=1.10.0"></script>
  <script src="/lib/reading_progress/reading_progress.js"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  <script src="/js/utils.js?v=7.2.0"></script>
  <script src="/js/motion.js?v=7.2.0"></script>

  
  <script src="/js/schemes/muse.js?v=7.2.0"></script>



  
  <script src="/js/scrollspy.js?v=7.2.0"></script>
<script src="/js/post-details.js?v=7.2.0"></script>



  <script src="/js/next-boot.js?v=7.2.0"></script>

  
  <script src="/js/js.cookie.js?v=7.2.0"></script>
  <script src="/js/scroll-cookie.js?v=7.2.0"></script>

  

  


  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>





  
    
      <script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script src="//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    
  




  <script src="/lib/bookmark/bookmark.min.js?v=1.0"></script>
  <script>
    bookmark.scrollToMark('auto', "#更多");
  
  </script>















  <script src="/js/local-search.js?v=7.2.0"></script>










<script>
if ($('body').find('pre.mermaid').length) {
  $.ajax({
    type: 'GET',
    url: '//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js',
    dataType: 'script',
    cache: true,
    success: function() {
      mermaid.initialize({
        theme: 'neutral',
        logLevel: 3,
        flowchart: { curve: 'linear' },
        gantt: { axisFormat: '%m/%d/%Y' },
        sequence: { actorMargin: 50 }
      });
    }
  });
}
</script>



    

<script>
  window.livereOptions = {
    refer: '2019/08/20/autograd/'
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
</script>


</body>
</html>
