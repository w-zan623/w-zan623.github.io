<!DOCTYPE html>





<html class="theme-next gemini use-motion" lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="baidu-site-verification" content="Mp17ppyPmu" />
<meta name="generator" content="Hexo 3.9.0">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.2.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.2.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.2.0">
  <link rel="mask-icon" href="/images/logo.svg?v=7.2.0" color="#222">

<link rel="stylesheet" href="/css/main.css?v=7.2.0">

<link rel="stylesheet" href="//fonts.proxy.ustclug.org/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">
  <link rel="stylesheet" href="/lib/needsharebutton/needsharebutton.css">
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2">


<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.2.0',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":true},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    fancybox: false,
    mediumzoom: true,
    lazyload: true,
    pangu: false,
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    search: {
      root: '/',
      path: 'search.xml'
    },
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    }
  };
</script>


  <script>
  (function(i,s,o,g,r,a,m){i["DaoVoiceObject"]=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;a.charset="utf-8";m.parentNode.insertBefore(a,m)})(window,document,"script",('https:' == document.location.protocol ? 'https:' : 'http:') + "//widget.daovoice.io/widget/0f81ff2f.js","daovoice")
  daovoice('init', {
      app_id: "4d92f510"
    });
  daovoice('update');
  </script>

  <meta name="description" content="译者博客：https://zinw623.github.io/ 什么是PYTORCH?[原英文网站] 它是基于python的科学计算包，读者定位为两种：  替代使用Numpy来使用GPU的功能 最灵活快速的深度学习研究平台  准备开始TensorsTensors是类似于加上能在GPU上进行加速计算功能的Numpy的ndarrays。 12from __future__ import print_f">
<meta name="keywords" content="pytorch,翻译">
<meta property="og:type" content="article">
<meta property="og:title" content="翻译：pytorch官网60分钟教程（1.2.0）">
<meta property="og:url" content="https://zinw623.github.io/2019/09/03/pytorch-60-minutes/index.html">
<meta property="og:site_name" content="今天写了点东西">
<meta property="og:description" content="译者博客：https://zinw623.github.io/ 什么是PYTORCH?[原英文网站] 它是基于python的科学计算包，读者定位为两种：  替代使用Numpy来使用GPU的功能 最灵活快速的深度学习研究平台  准备开始TensorsTensors是类似于加上能在GPU上进行加速计算功能的Numpy的ndarrays。 12from __future__ import print_f">
<meta property="og:locale" content="zh-CN">
<meta property="og:updated_time" content="2019-09-03T14:24:01.701Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="翻译：pytorch官网60分钟教程（1.2.0）">
<meta name="twitter:description" content="译者博客：https://zinw623.github.io/ 什么是PYTORCH?[原英文网站] 它是基于python的科学计算包，读者定位为两种：  替代使用Numpy来使用GPU的功能 最灵活快速的深度学习研究平台  准备开始TensorsTensors是类似于加上能在GPU上进行加速计算功能的Numpy的ndarrays。 12from __future__ import print_f">
  <link rel="alternate" href="/atom.xml" title="今天写了点东西" type="application/atom+xml">
  <link rel="canonical" href="https://zinw623.github.io/2019/09/03/pytorch-60-minutes/">


<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>翻译：pytorch官网60分钟教程（1.2.0） | 今天写了点东西</title>
  


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?Mp17ppyPmu";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>
  <meta name="baidu-site-verification" content="Mp17ppyPmu" />






  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">今天写了点东西</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
    <ul id="menu" class="menu">
        
        
        
          
          <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br/>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">
      
    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br/>关于</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">
      
    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br/>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br/>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br/>归档</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-book">
      
    

    <a href="/book/" rel="section"><i class="menu-item-icon fa fa-fw fa-book"></i> <br/>书</a>

  </li>
        <li class="menu-item menu-item-search">
          <a href="javascript:;" class="popup-trigger">
          
            <i class="menu-item-icon fa fa-search fa-fw"></i> <br/>搜索</a>
        </li>
      
    </ul>
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>


    </div>
</nav>

</div>
    </header>

    

  <a href="https://github.com/zinw623" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    
    <div class="reading-progress-bar"></div>

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://zinw623.github.io/2019/09/03/pytorch-60-minutes/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="涵贰十">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="今天写了点东西">
    </span>
      <header class="post-header">

        
          <h1 class="post-title" itemprop="name headline">翻译：pytorch官网60分钟教程（1.2.0）

              
            
          </h1>
        

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-09-03 22:17:34 / 修改时间：22:24:01" itemprop="dateCreated datePublished" datetime="2019-09-03T22:17:34+08:00">2019-09-03</time>
            </span>
          
            

            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/翻译/" itemprop="url" rel="index"><span itemprop="name">翻译</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/翻译/pytorch文档（1-2-0）/" itemprop="url" rel="index"><span itemprop="name">pytorch文档（1.2.0）</span></a></span>

                
                
              
            </span>
          

          <br/>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
              
                <span class="post-meta-item-text">本文字数：</span>
              
              <span title="本文字数">27k</span>
            </span>
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
              
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              
              <span title="阅读时长">24 分钟</span>
            </span>
          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>译者博客：<a href="https://zinw623.github.io/">https://zinw623.github.io/</a></p>
<h2 id="什么是PYTORCH"><a href="#什么是PYTORCH" class="headerlink" title="什么是PYTORCH?"></a>什么是PYTORCH?</h2><p><a href="https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py" target="_blank" rel="noopener">[原英文网站]</a></p>
<p>它是基于python的科学计算包，读者定位为两种：</p>
<ul>
<li>替代使用Numpy来使用GPU的功能</li>
<li>最灵活快速的深度学习研究平台</li>
</ul>
<h3 id="准备开始"><a href="#准备开始" class="headerlink" title="准备开始"></a>准备开始</h3><h4 id="Tensors"><a href="#Tensors" class="headerlink" title="Tensors"></a>Tensors</h4><p>Tensors是类似于加上能在GPU上进行加速计算功能的Numpy的ndarrays。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure>

<blockquote>
<p>注意：<br>    未初始化的矩阵被声明，但是在使用前不会包含确切的已知的值。当一个未初始化的矩阵被创建，分配内存中当时的任何值都将作为初始值出现。</p>
</blockquote>
<p>构建一个5x3的矩阵，未初始化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.empty(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>

<p>输出:</p>
<pre><code>tensor([[0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.]])</code></pre><p>构建一个随机的初始化过的矩阵：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<pre><code>tensor([[0.6259, 0.0797, 0.8297],
        [0.6732, 0.7944, 0.2363],
        [0.6775, 0.2497, 0.3846],
        [0.8515, 0.5171, 0.6957],
        [0.7759, 0.6000, 0.1323]])</code></pre><p>构建一个dtype为long且用0填充的矩阵：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.zeros(<span class="number">5</span>, <span class="number">3</span>, dtype=torch.long)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<pre><code>tensor([[0, 0, 0],
        [0, 0, 0],
        [0, 0, 0],
        [0, 0, 0],
        [0, 0, 0]])</code></pre><p>构建一个直接从data里构建tensor：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">5.5</span>, <span class="number">3</span>])</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<pre><code>tensor([5.5000, 3.0000])</code></pre><p>或者从已有的tensor创建tensor。这些方法将重用输入tensor的内容，例如dtype，除非使用者提供新的值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = x.new_ones(<span class="number">5</span>, <span class="number">3</span>, dtype = torch.double) <span class="comment"># new_*方法接受了大小（sizes）</span></span><br><span class="line">print(x)</span><br><span class="line"></span><br><span class="line">x = torch.randn_like(x, dtype = torch.float) <span class="comment"># 重写了dtype</span></span><br><span class="line">print(x)                                     <span class="comment"># 结构具有相同的size</span></span><br></pre></td></tr></table></figure>

<p>输出：</p>
<pre><code>tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], dtype=torch.float64)
tensor([[ 0.5955, -0.2528, -0.2648],
        [ 0.7689,  0.2396, -0.0121],
        [ 1.3478,  0.0460,  0.0255],
        [ 0.1266, -1.1526, -0.5546],
        [-0.2001, -0.0542, -0.6439]])</code></pre><p>得到它的size（大小）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(x.size())</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<pre><code>torch.Size([5, 3])</code></pre><blockquote>
<p>注意：<br>    <code>torch.Size</code>事实上是元组，所以它支持所有的元组操作。</p>
</blockquote>
<h4 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h4><p>有多种操作的语法。在下面的例子里，我们将看一下加法操作。</p>
<p>加法：语法1</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(x + y)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<pre><code>tensor([[ 1.1550,  0.5950, -0.0519],
        [ 1.3954,  0.9232,  0.8904],
        [ 1.7020,  0.8187,  0.0265],
        [ 0.3831, -0.6057, -0.2829],
        [ 0.5647,  0.5976,  0.1128]])</code></pre><p>加法：语法2</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(torch.add(x, y))</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<pre><code>tensor([[ 1.1550,  0.5950, -0.0519],
        [ 1.3954,  0.9232,  0.8904],
        [ 1.7020,  0.8187,  0.0265],
        [ 0.3831, -0.6057, -0.2829],
        [ 0.5647,  0.5976,  0.1128]])</code></pre><p>加法：提供一个输出向量作为参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">result = torch.emtpy(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">torch.add(x, y, out = result)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<pre><code>tensor([[ 1.1550,  0.5950, -0.0519],
        [ 1.3954,  0.9232,  0.8904],
        [ 1.7020,  0.8187,  0.0265],
        [ 0.3831, -0.6057, -0.2829],
        [ 0.5647,  0.5976,  0.1128]])</code></pre><p>加法：in-place</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 把x加到y上</span></span><br><span class="line">y.add_(x)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<pre><code>tensor([[ 1.1550,  0.5950, -0.0519],
        [ 1.3954,  0.9232,  0.8904],
        [ 1.7020,  0.8187,  0.0265],
        [ 0.3831, -0.6057, -0.2829],
        [ 0.5647,  0.5976,  0.1128]])</code></pre><blockquote>
<p>注意：<br>    任何改变张量的in-place操作后固定带一个<code>_</code>。例如：<code>x.copy_(y)</code>,<code>x.t_()</code>，将改变<code>x</code>的值。</p>
</blockquote>
<p>你可以使用类似于标准的numpy索引的所有附加功能。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(x[:, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<pre><code>tensor([-0.2528,  0.2396,  0.0460, -1.1526, -0.0542])</code></pre><p>改变大小：如果你想resize/reshape张量，你可以使用<code>torch.view</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">y = x.view(<span class="number">16</span>)</span><br><span class="line">z = x.view(<span class="number">-1</span>, <span class="number">8</span>)  <span class="comment"># size -1 从其他维度推断</span></span><br><span class="line">print(x.size(), y.size(), z.size())</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<pre><code>torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])</code></pre><p>如果你有一个元素的张量，可以使用<code>.item()</code>获得python number的值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">1</span>)</span><br><span class="line">print(x)</span><br><span class="line">print(x.item())</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<pre><code>tensor([-0.8748])
-0.8748161792755127</code></pre><p>然后阅读：<br>100+张量操作，<a href="https://pytorch.org/docs/torch" target="_blank" rel="noopener">here</a></p>
<h3 id="numpy桥"><a href="#numpy桥" class="headerlink" title="numpy桥"></a>numpy桥</h3><p>将Torch Tensor转换成numpy array，反之也很简单。</p>
<p>Torch Tensor和numpy array潜在地共享内存（如果torch tensor在CPU上），并且改变一个将会使另一个改变。</p>
<h4 id="将NumPy-Array转换成Torch-Tensor"><a href="#将NumPy-Array转换成Torch-Tensor" class="headerlink" title="将NumPy Array转换成Torch Tensor"></a>将NumPy Array转换成Torch Tensor</h4><p>看如何通过改变np array自动地改变Torch Tensor。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.ones(<span class="number">5</span>)</span><br><span class="line">b = torch.from_numpy(a)</span><br><span class="line">np.add(a, <span class="number">1</span>, out = a)</span><br><span class="line">print(a)</span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<pre><code>[2. 2. 2. 2. 2.]
tensor([2., 2., 2., 2., 2.], dtype=torch.float64)</code></pre><h3 id="CUDA张量"><a href="#CUDA张量" class="headerlink" title="CUDA张量"></a>CUDA张量</h3><p>tensors可以通过<code>.to</code>方法被移动到任何设备。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 只有CUDA可用时运行这个cell</span></span><br><span class="line"><span class="comment"># 我们使用"torch.device"控制张量出入GPU</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    device = torch.device(<span class="string">"cuda"</span>)           <span class="comment"># 一个CUDA device对象</span></span><br><span class="line">    y = torch.ones_like(x, device = device) <span class="comment"># 直接在GPU上创建tensor</span></span><br><span class="line">    x = x.to(device)                        <span class="comment"># 或者使用字符串".to("cuda")"</span></span><br><span class="line">    z = x + y</span><br><span class="line">    print(z)</span><br><span class="line">    print(z.to(<span class="string">"cpu"</span>, torch.double))        <span class="comment"># ''.to''也能一起改变dtype</span></span><br></pre></td></tr></table></figure>

<p>输出：</p>
<pre><code>tensor([0.1252], device=&apos;cuda:0&apos;)
tensor([0.1252], dtype=torch.float64)</code></pre><h2 id="AUTOGRAD-自动求导"><a href="#AUTOGRAD-自动求导" class="headerlink" title="AUTOGRAD:自动求导"></a>AUTOGRAD:自动求导</h2><p><a href="https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py" target="_blank" rel="noopener">[原英文网站]</a></p>
<p>Pytorch所有神经网络的核心是<code>autograd</code>包。让我们简单地看一下这个，然后我们将要去训练我们的第一个神经网络。</p>
<p><code>autograd</code>包为所有tensors操作提供自动求导。它是一个定义即运行的框架，这以为着你的代码如何运行你的反向传播就如何被定义，每一次迭代都可以不同。</p>
<p>让我们用更简单的术语和一些例子来看看。</p>
<h3 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h3><p><code>torch.Tensor</code>是包的核心类。如果你设置了它的属性<code>.requires_grad</code>为<code>True</code>，它开始时会追踪所有作用在它之上的操作。当你完成你的计算时你可以通过调用<code>.backward()</code>并且自动地计算所有梯度。这个张量的梯度将会被累积到<code>.grad</code>这个属性里。</p>
<p>为了组织张量追踪历史，你可以调用<code>.detach()</code>来从计算历史中将它分离，并且防止了在未来计算中被追踪。</p>
<p>为了防止追踪历史（并且使用内存），你也可以将代码块包装到<code>with torch.no_grad():</code>。这在当评估模型时非常有帮助，因为模型可能有<code>requires_grad=True</code>的可训练参数，但是我们并不需要梯度。</p>
<p>为了autograd的执行还有另一个非常重要的类 - <code>Function</code>。</p>
<p><code>Tensor</code>和<code>Function</code>是相互关联的并且建立一个无环图，这图编码了计算的完整历史。每一个张量都有一个<code>.grad_fn</code>属性，参照了创建这个<code>Tensor</code>的<code>Function</code>（除了被用户创建的张量 - 它们的<code>grad_fn is None</code>）。</p>
<p>如果你想要计算衍生物，你可以调用一个<code>Tensor</code>的<code>.backward()</code>方法。如果<code>Tensor</code>是一个标量（即只有一个元素），你不需要为<code>.backward()</code>指定任何参数，然而如果它有更多的元素，你需要指定一个<code>gradient</code>参数，它是一个匹配形状的张量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure>

<p>创建一个张量并且设置<code>requires_grad=True</code>并追踪它的计算。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">2</span>, <span class="number">2</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<pre><code>tensor([[1., 1.],
        [1., 1.]], requires_grad=True)</code></pre><p>进行一个张量计算：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y = x + <span class="number">2</span></span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<pre><code>tensor([[3., 3.],
        [3., 3.]], grad_fn=&lt;AddBackward0&gt;)</code></pre><p><code>y</code>作为一个操作的结果被创建，所以它有一个<code>grad_fn</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(y.grad_fn)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<pre><code>&lt;AddBackward0 object at 0x7f3772e36588&gt;</code></pre><p>对<code>y</code>做更多的操作</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">z = y * y * <span class="number">3</span></span><br><span class="line">out = z.mean()</span><br><span class="line"></span><br><span class="line">print(z, out)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<pre><code>tensor([[27., 27.],
        [27., 27.]], grad_fn=&lt;MulBackward0&gt;) tensor(27., grad_fn=&lt;MeanBackward0&gt;)</code></pre><p><code>.requires_grad_( ... )</code>in-place改变已存在张量的<code>requires_grad=True</code>标示。如果没有给定输入默认的标示是<code>False</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">a = ((a * <span class="number">3</span>) / (a - <span class="number">1</span>))</span><br><span class="line">print(a.requires_grad)</span><br><span class="line">a.requires_grad_(<span class="literal">True</span>)</span><br><span class="line">print(a.requires_grad)</span><br><span class="line">b = (a * a).sum()</span><br><span class="line">print(b.grad_fn)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<pre><code>False
True
&lt;SumBackward0 object at 0x7f3772e36dd8&gt;</code></pre><h3 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h3><p>现在开始反向传播。因为<code>out</code>包含一个单独的标量，<code>out.backward()</code>等价于<code>out.backward(torch.tensor(1.))</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">out.backward()</span><br></pre></td></tr></table></figure>

<p>打印梯度d(out)/dx</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(x.grad)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<pre><code>tensor([[4.5000, 4.5000],
        [4.5000, 4.5000]])</code></pre><p>你应当有一个<code>4.5</code>的矩阵。让我们调用<code>out</code>的张量“o”，我们有$o=\frac{1}{4}\sum_{i}z_i,z_i=3(x_i+2)^2$和$z_i\mid_{x_i=1}=27$。然而，$\frac{\partial{o}}{\partial{x_i}}\mid_{x_i=1}=\frac{9}{2}=4.5$。</p>
<p>数学上的，如果你有一个向量值的函数$\vec{y}=f(\vec{x})$，然后$\vec{y}$关于$\vec{x}$的梯度是雅克比矩阵：</p>
<p>$$<br>J =<br>\left[<br>\begin{matrix}<br>\frac{\partial{y_1}}{\partial{x_1}}  &amp; \cdots &amp;\frac{\partial{y_1}}{\partial{x_n}}  \<br> \vdots &amp; \ddots &amp; \vdots \<br> \frac{\partial{y_m}}{\partial{x_1}}  &amp; \cdots &amp; \frac{\partial{y_m}}{\partial{x_n}}   \<br>\end{matrix}<br>\right]<br>$$</p>
<p>总的来说，<code>torch.autograd</code>是为了计算向量-雅克比积(vector-Jacobian product)。那是给定向量$v=(v_1  v_2  \cdots v_m)^T$，计算积$v^T \cdot J$。如果$v$恰好是标量函数$l=g(\vec{y})$的梯度，就是$v=(\frac{\partial{l}}{\partial{y_1}}\cdots\frac{\partial{l}}{\partial{y_m}})^T$，然后通过链式法则，向量-雅克比积将会成为$l$关于$\vec{x}$的梯度：</p>
<p>$$<br>J^T\cdot v =<br>\left[<br>\begin{matrix}<br>\frac{\partial{y_1}}{\partial{x_1}}  &amp; \cdots &amp;\frac{\partial{y_1}}{\partial{x_n}}  \<br> \vdots &amp; \ddots &amp; \vdots \<br> \frac{\partial{y_m}}{\partial{x_1}}  &amp; \cdots &amp; \frac{\partial{y_m}}{\partial{x_n}}   \<br>\end{matrix}<br>\right]<br>\left[<br>\begin{matrix}<br>\frac{\partial{l}}{\partial{y_1}} \<br>\vdots \<br>\frac{\partial{l}}{\partial{y_m}}<br>\end{matrix}<br>\right]=<br>\left[<br>\begin{matrix}<br>\frac{\partial{l}}{\partial{x_1}} \<br>\vdots \<br>\frac{\partial{l}}{\partial{x_n}}<br>\end{matrix}<br>\right]<br>$$</p>
<p>（注意：$v^T\cdot J$给出了一个行向量，这个可以通过$J^T\cdot v$得到列向量。）</p>
<p>向量-雅克比积的这个特征使得给一个非标量输出的模型输入一个的外部梯度非常便捷。</p>
<p>现在让我们看看一个向量-雅克比积的例子。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">3</span>, requires_grad = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">y = x * <span class="number">2</span></span><br><span class="line"><span class="keyword">while</span> y.data.norm() &lt; <span class="number">1000</span>:</span><br><span class="line">    y = y * <span class="number">2</span></span><br><span class="line"></span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<pre><code>tensor([-1350.9803,   805.9799,  -188.3773], grad_fn=&lt;MulBackward0&gt;)</code></pre><p>现在<code>y</code>不在是标量，<code>torch.autograd</code>不能直接计算完整的雅克比矩阵，但是如果我们只是想要向量-雅克比积，只需简单地输入一个向量给<code>backward</code>作为参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">v = torch.tensor([<span class="number">0.1</span>, <span class="number">1.0</span>, <span class="number">0.0001</span>], dtype = torch.float)</span><br><span class="line">y.backward(v)</span><br><span class="line"></span><br><span class="line">print(x.grad)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<pre><code>tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])</code></pre><p>你也能通过把代码块封装到<code>with torch.no_grad():</code>中来停止autograd追踪带有<code>.requires_grad=True</code>的张量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">print(x.requires_grad)</span><br><span class="line">print((x ** <span class="number">2</span>).requires_grad)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    print((x ** <span class="number">2</span>).requires_grad)</span><br></pre></td></tr></table></figure>

<p>输出:</p>
<pre><code>True
True
False</code></pre><h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><p><a href="https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html" target="_blank" rel="noopener">[原英文网站]</a></p>
<p>神经网络可以通过<code>torch.nn</code>包来构建。</p>
<p>现在你看一眼<code>autograd</code>，<code>nn</code>依赖于<code>autograd</code>来定义模型并且对他们求导。一个<code>nn.Module</code>包括层（layers)和一个<code>forward(input)</code>方法，这个方法会返回<code>outupt</code>。</p>
<p>例如，看这个分类数字图像的网络：</p>
<p><img alt data-src="/uploads/torch-60-tu/mnist.png"></p>
<center>卷积网络</center>

<p>这是一个简单的前馈神经网络。它接受一个输入，将它输入穿过许多层然后一个接着另一个，然后最后给出一个输出。</p>
<p>一个典型的神经网络的训练流程如下：</p>
<ul>
<li>定义一个带有许多可学习的参数（或权重）的神经网络。</li>
<li>在输入的训练集上迭代。</li>
<li>通过网络处理输入。</li>
<li>计算损失（loss 输出与正确有多远）。</li>
<li>反向传播求网络的梯度。</li>
<li>更新网络权重，典使用一个简单典型的更新规则：<code>weight = weight - learning_rate * gradient</code></li>
</ul>
<h3 id="定义网络"><a href="#定义网络" class="headerlink" title="定义网络"></a>定义网络</h3><p>让我们来定义网络：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        <span class="comment"># 1是图片通道，6是输出通道，3x3正方形卷积</span></span><br><span class="line">        <span class="comment"># 核（kernel）</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">3</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">3</span>)</span><br><span class="line">        <span class="comment"># 一个仿射操作：y = wx + b</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">6</span> * <span class="number">6</span>, <span class="number">128</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># 在(2, 2)窗口上的最大池化</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv1(x)), (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">        <span class="comment"># 如果是正方形size可以指定一个数</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv2(x)), <span class="number">2</span>)</span><br><span class="line">        x = x.view(<span class="number">-1</span>, self.num_flat_feature(x))</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> num_features</span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line">print(net)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<pre><code>Net(
(conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))
(conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))
(fc1): Linear(in_features=576, out_features=120, bias=True)
(fc2): Linear(in_features=120, out_features=84, bias=True)
(fc3): Linear(in_features=84, out_features=10, bias=True)
)</code></pre><p>你只需要定义一个<code>forward</code>函数，<code>backward</code>函数（计算梯度的函数）会自动使用<code>autograd</code>定义。你在<code>forward</code>函数里使用任何张量操作。</p>
<p>模型的可学习参数通过<code>net.parameters()</code>获得</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">params = list(net.parameters())</span><br><span class="line">print(len(params))</span><br><span class="line">print(params[<span class="number">0</span>].size())     <span class="comment"># conv1的权重</span></span><br></pre></td></tr></table></figure>

<p>输出：</p>
<pre><code>10
torch.Size([6, 1, 3, 3])</code></pre><p>让我们尝试一个随机的32x32的输入。注意：这个网络（LeNet）期望输入的大小是32x32。在MNIST数据集上使用这个网络，请把数据集的图片的大小转换成32x32。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">input = torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line">out = net(input)</span><br><span class="line">print(out)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<pre><code>tensor([[ 0.0057, -0.0952,  0.1055, -0.0916, -0.1350,  0.0857, -0.0890,  0.0326,
        -0.0554,  0.1451]], grad_fn=&lt;AddmmBackward&gt;)</code></pre><p>清零所有参数的梯度的缓冲和用随机的梯度进行反向传播。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net.zero_grad()</span><br><span class="line">out.backward(torch.randn(<span class="number">1</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure>

<blockquote>
<p>注意：<br>    <code>torch.nn</code>只支持mini-batches(小的批处理)。整个<code>torch.nn</code>包只支持输入是样本的mini-batch，而不是单一的样本。<br>    例如，<code>nn.Conv2d</code>将接受一个<code>样本数x通道数x高x宽</code>的四维张量。<br>    如果你有一个单一的样本，只需使用<code>input.unsqueeze(0)</code>来增加一个伪batch维度。</p>
</blockquote>
<p>在进行下一步之前，让我们重新回顾你目前看过的所有的类。</p>
<p>回顾：</p>
<ul>
<li><code>torch.Tensor</code> - 一个支持像是<code>backward()</code>的自动求导的多维数组。也保留关于这个张量的梯度。</li>
<li><code>nn.Module</code> - 神经网络模块。封装参数的便捷方式。帮助他们移动到GPU，导出，加载等。</li>
<li><code>nn.Parameter</code> - 一种Tensor，当它被分配给一个<code>Module</code>时，它会被自动注册为一个参数。</li>
<li><code>autograd.Function</code> - 执行前向和一个自动求导操作的反向定义。每个<code>Tensor</code>操作创建至少一个单一的<code>Function</code>节点，与创建<code>Tensor</code>的和编码它的历史的函数。</li>
</ul>
<p>至此，我们讨论了：</p>
<ul>
<li>定义一个神经网络</li>
<li>执行了输入和调用反向传播</li>
</ul>
<p>还剩下：</p>
<ul>
<li>计算损失</li>
<li>更新网络的权重</li>
</ul>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>损失函数接受一对输入（output, target)，和计算输出与目标之间距离的值。</p>
<p>在nn包下有很多不同的<a href="https://pytorch.org/docs/nn.html#loss-functions" target="_blank" rel="noopener">损失函数</a>。一个简单的损失是：<code>nn.MSELoss</code>,这个是计算输入和目标的均方误差。</p>
<p>例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">output = net(input)</span><br><span class="line">target = torch.randn(<span class="number">10</span>)</span><br><span class="line">target = target.view(<span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line"></span><br><span class="line">loss = criterion(output, target)</span><br><span class="line">print(loss)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<pre><code>tensor(0.9991, grad_fn=&lt;MseLossBackward&gt;)</code></pre><p>现在，如果你使用它的<code>.grad_fn</code>属性沿着<code>loss</code>的方向向后移动，你将看到像这样的计算图：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">input -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d</span><br><span class="line">      -&gt; view -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear</span><br><span class="line">      -&gt; MSELoss</span><br><span class="line">      -&gt; loss</span><br></pre></td></tr></table></figure>

<p>所以，当我们调用<code>loss.backward()</code>，整个图对loss求导，并且图中所有有<code>requires_grad=True</code>的张量有一个累积梯度的<code>.grad</code>张量。</p>
<p>为了说明这一点，我们后退几步：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(loss.grad_fn)  <span class="comment"># MSELoss</span></span><br><span class="line">print(loss.grad_fn.next_functions[<span class="number">0</span>][<span class="number">0</span>]) <span class="comment"># Linear</span></span><br><span class="line">print(loss.grad_fn.next_functions[<span class="number">0</span>][<span class="number">0</span>].next_function[<span class="number">0</span>][<span class="number">0</span>])   <span class="comment"># Relu</span></span><br></pre></td></tr></table></figure>

<p>输出：</p>
<pre><code>&lt;MseLossBackward object at 0x7ff716c28630&gt;
&lt;AddmmBackward object at 0x7ff716c28400&gt;
&lt;AccumulateGrad object at 0x7ff716c28400&gt;</code></pre><h3 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h3><p>为了反向传播误差我们必须做的全部只是调用<code>loss.backward()</code>。不过，您需要清除现有的梯度，否则梯度将累积为现有梯度。</p>
<p>现在，我们将调用<code>loss.backward()</code>，看一下conv1的偏置梯度在反向之前和之后的区别。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">net.zero_grad()         <span class="comment"># 清零所有参数的梯度缓冲</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">'conv1.bias.grad before backward'</span>)</span><br><span class="line">print(net.conv1.bias.grad)</span><br><span class="line"></span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'conv1.bias.grad after backward'</span>)</span><br><span class="line">print(net.conv1.bias.grad)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<pre><code>conv1.bias.grad before backward
tensor([0., 0., 0., 0., 0., 0.])
conv1.bias.grad after backward
tensor([ 0.0081,  0.0029,  0.0248, -0.0054,  0.0051,  0.0008])</code></pre><p>现在，我们看如何使用损失函数。</p>
<p>我们剩下需要学习的是：</p>
<ul>
<li>更新网络的权重</li>
</ul>
<h3 id="更新参数"><a href="#更新参数" class="headerlink" title="更新参数"></a>更新参数</h3><p>在实践中使用的最简单的更新规则是随机梯度下降（SGD)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line"><span class="keyword">for</span> f <span class="keyword">in</span> net.parameters():</span><br><span class="line">    f.data.sub_(f.grad.data * learning_rate)</span><br></pre></td></tr></table></figure>

<p>然而，当你使用神经网络时，你想要使用不同的更新规则像是SGD，Nesterov-SGD，Adam，RMSProp等等。为了其中这些，你可以使用一个小的包：<code>torch.optim</code>来执行所有这些方法。使用它非常简单：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建你的优化器</span></span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr = <span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在训练循环里</span></span><br><span class="line">optimizer.zero_grad()   <span class="comment"># 清零梯度缓冲</span></span><br><span class="line">output = net(input)</span><br><span class="line">loss = criterion(output, target)</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()        <span class="comment"># 更新</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>注意：<br>    观察到必须是手动地使用<code>optimizer.zero_grad()</code>清零梯度缓冲的。这是因为梯度是累积的，这个在反向传播里解释了。</p>
</blockquote>
<h2 id="训练一个分类器"><a href="#训练一个分类器" class="headerlink" title="训练一个分类器"></a>训练一个分类器</h2><p><a href="https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html" target="_blank" rel="noopener">[原英文网站]</a></p>
<p>就是这个。你已经看过如何定义神经网络，计算损失和更新网络的参数。</p>
<p>现在你可能会思考了，</p>
<h3 id="那数据呢？"><a href="#那数据呢？" class="headerlink" title="那数据呢？"></a>那数据呢？</h3><p>总体来说，当你不得不处理图片，文本，音频，视频数据，你可以使用标准的python库来把它们加载成numpy数组，然后你可以将数组转化成<code>torch.*Tensor</code>。</p>
<ul>
<li>对于图像，像是Pillow,OpenCV包是有效的</li>
<li>对于音频，有scipy或librosa包</li>
<li>对于文本，原始的python或基于Cython加载，或NLTK和SpaCy都是有效的。</li>
</ul>
<p>特别的对于视觉方面，我们已经创建了一个叫做<code>torchvision</code>的包，它提供了常见数据集（Imagenet，CIFAR10，MNIST等等）的数据加载器和图片的数据处理器，也就是<code>torchvision.datasets</code>和<code>torch.utils.data.DataLoader</code>。</p>
<p>这提供了很大的便利和避免了编写样本代码。</p>
<p>对于这个教程，我们将使用CIFAR10数据集。它有类别：飞机、汽车、鸟、猫、鹿、狗、青蛙、马、船、卡车。CIFAR-10中的图片都是3x32x32的，也就是32x32像素大小、三颜色通道的图片。</p>
<p><img alt data-src="/uploads/torch-60-tu/cifar10.png"></p>
<center>cifar10</center>

<h3 id="训练一个图片分类器"><a href="#训练一个图片分类器" class="headerlink" title="训练一个图片分类器"></a>训练一个图片分类器</h3><p>我们将依次进行下面的几步：</p>
<ol>
<li>使用<code>torchvision</code>加载和标准化CIFAR10的训练集和测试集</li>
<li>定义卷积神经网络</li>
<li>定义损失函数</li>
<li>在训练集上训练神经网络</li>
<li>在测试集上测试网络</li>
</ol>
<h4 id="1-加载和规范化CIFAR10"><a href="#1-加载和规范化CIFAR10" class="headerlink" title="1.加载和规范化CIFAR10"></a>1.加载和规范化CIFAR10</h4><p>使用<code>torchvision</code>，非常简单地加载CIFAR10</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br></pre></td></tr></table></figure>

<p>torchvision datasets输出是在范围[0, 1]的PILImage的图片。我们可以将他们处理成规范化过的范围在[-1, 1]的张量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.Compose(</span><br><span class="line">    [transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainset = torchvision.datasets.CIFAR10(root=<span class="string">'./data'</span>, train = <span class="literal">True</span>, download = <span class="literal">True</span>, transform=transform)</span><br><span class="line">trainloader = torch.utils.data.DataLoader(trainset, batch_size = <span class="number">4</span>, shuffle=<span class="literal">True</span>, num_workers = <span class="number">2</span>)</span><br><span class="line">testset = torchvision.datasets.CIFAR10(root=<span class="string">'./data'</span>, train = <span class="literal">True</span>, download = <span class="literal">True</span>, transform = transform)</span><br><span class="line">testloader = torch.utils.data.DataLoader(testset, batch_size = <span class="number">4</span>, shuffle=<span class="literal">False</span>, num_workers = <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">classes = (<span class="string">'plane'</span>, <span class="string">'car'</span>, <span class="string">'bird'</span>, <span class="string">'cat'</span>, <span class="string">'deer'</span>, <span class="string">'dog'</span>, <span class="string">'frog'</span>, <span class="string">'horse'</span>, <span class="string">'ship'</span>, <span class="string">'truck'</span>)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<pre><code>Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz
Extracting ./data/cifar-10-python.tar.gz to ./data
Files already downloaded and verified</code></pre><p>让我们展示一些训练集图片。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 展示图片的函数</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">imshow</span><span class="params">(img)</span>:</span></span><br><span class="line">    img = img / <span class="number">2</span> + <span class="number">0.5</span></span><br><span class="line">    npimg = img.numpy()</span><br><span class="line">    plt.imshow(np.transpose(npimg, (<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获得一些随机的训练集图片</span></span><br><span class="line">dataiter = iter(trainloader)</span><br><span class="line">images, labels = dataiter.next()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 展示图片</span></span><br><span class="line">imshow(torchvision.utils.make_grid(images))</span><br><span class="line"><span class="comment"># 打印标签</span></span><br><span class="line">print(<span class="string">' '</span>.join(<span class="string">'%5s'</span> % classes[labels[j]] <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">4</span>)))</span><br></pre></td></tr></table></figure>

<p><img alt data-src="/uploads/torch-60-tu/sphx_glr_cifar10_tutorial_001.png"></p>
<p>输出：</p>
<pre><code>frog  ship   cat plane</code></pre><h4 id="2-定义一个卷积神经网络"><a href="#2-定义一个卷积神经网络" class="headerlink" title="2.定义一个卷积神经网络"></a>2.定义一个卷积神经网络</h4><p>从之前的神经网络章节复制神经网络，并且修改成三通道的图片（它之前定义的是一通道的图片）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.pool(F.relu(self.conv1(x)))</span><br><span class="line">        x = self.pool(F.relu(self.conv2(x)))</span><br><span class="line">        x = x.view(<span class="number">-1</span>, <span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">net = Net()</span><br></pre></td></tr></table></figure>

<h4 id="3-定义损失函数和优化器"><a href="#3-定义损失函数和优化器" class="headerlink" title="3.定义损失函数和优化器"></a>3.定义损失函数和优化器</h4><p>让我们使用分类交叉熵损失和带有动量的SGD。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr = <span class="number">0.001</span>, momentum = <span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>

<h4 id="4-训练网络"><a href="#4-训练网络" class="headerlink" title="4.训练网络"></a>4.训练网络</h4><p>这时事情变得有趣起来。我们简单地循环一下我们的数据迭代器，把输入喂给网络并且做优化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">2</span>):   <span class="comment"># 循环几次数据集</span></span><br><span class="line"></span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(trainloader, <span class="number">0</span>):</span><br><span class="line">        <span class="comment"># 获得输入；数据是[输入，标签]的列表</span></span><br><span class="line">        inputs, labels = data</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 清零参数的梯度</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 前向+反向+优化</span></span><br><span class="line">        outputs = net(inputs)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 打印统计信息</span></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">2000</span> == <span class="number">1999</span>:    <span class="comment"># 每2000个mini-batches打印一次</span></span><br><span class="line">            print(<span class="string">'[%d, %5d] loss: %.3f'</span> % (epoch + <span class="number">1</span>, i + <span class="number">1</span>, running_loss / <span class="number">2000</span>))</span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">'Finished Training'</span>)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<pre><code>[1,  2000] loss: 2.169
[1,  4000] loss: 1.808
[1,  6000] loss: 1.659
[1,  8000] loss: 1.553
[1, 10000] loss: 1.488
[1, 12000] loss: 1.455
[2,  2000] loss: 1.379
[2,  4000] loss: 1.346
[2,  6000] loss: 1.320
[2,  8000] loss: 1.305
[2, 10000] loss: 1.275
[2, 12000] loss: 1.262
Finished Training</code></pre><h4 id="5-在测试集上测试网络"><a href="#5-在测试集上测试网络" class="headerlink" title="5.在测试集上测试网络"></a>5.在测试集上测试网络</h4><p>我们已经在训练集上训练了两次网络。但是我们需要检测网络是否有学习。</p>
<p>我们将通过网络预测输出的类别标签检测这个，并且和事实做对比。如果预测是正确的，我们向正确预测的列表里添加该样本。</p>
<p>OK，第一步。让我们从测试集中取出图片来展示熟悉一下。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">dataiter = iter(testloader)</span><br><span class="line">images, labels = dataiter.next()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印图片</span></span><br><span class="line">imshow(torch.vision.utils.make_grid(images))</span><br><span class="line">print(<span class="string">'GroundTruth: '</span>, <span class="string">' '</span>.join(<span class="string">'%5s'</span> % classes[labels[j]] <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">4</span>)))</span><br></pre></td></tr></table></figure>

<p><img alt data-src="/uploads/torch-60-tu/sphx_glr_cifar10_tutorial_002.png"></p>
<p>输出：</p>
<pre><code>GroundTruth:    cat  ship  ship plane</code></pre><p>ok，现在让我们看看神经网络认为上面的例子是什么：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">output = net(images)</span><br></pre></td></tr></table></figure>

<p>输出是这10类的信念值（energies）。越高信念值的一类，模型越认为图片是这一类。所以，让我们最高信念值的标签：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">_, predicted = torch.max(output, <span class="number">1</span>)</span><br><span class="line">print(<span class="string">'Predicted:'</span>, <span class="string">' '</span>.join(<span class="string">'%5s'</span> % classes[predicted[j]] <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">4</span>)))</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<pre><code>Predicted:    cat plane plane  ship</code></pre><p>结果看起来不错。</p>
<p>让我们看一下网络在整个数据集上表现怎么样。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">correct = <span class="number">0</span></span><br><span class="line">total = <span class="number">0</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> testloader:</span><br><span class="line">        images, labels = data</span><br><span class="line">        outputs = net(images)</span><br><span class="line">        _, predicted = torch.max(outputs.data, <span class="number">1</span>)</span><br><span class="line">        total += labels.size(<span class="number">0</span>)</span><br><span class="line">        correct += (predicted == labels).sum().item()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Accuracy of the network on the 10000 test images: %d %%'</span> % (</span><br><span class="line">    <span class="number">100</span> * correct / total))</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<pre><code>Accuracy of the network on the 10000 test images: 54 %</code></pre><p>看起来比碰运气（10%的准确率，从10类中随机选一类）要好的多。像是网络已经学习了一些东西。</p>
<p>嗯<del>~</del>，哪些类表现的好，哪些类表现的不好呢：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">class_correct = list(<span class="number">0.</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>))</span><br><span class="line">class_total = list(<span class="number">0.</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>))</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> testloader:</span><br><span class="line">        images, labels = data</span><br><span class="line">        outputs = net(images)</span><br><span class="line">        _, predicted = torch.max(outputs, <span class="number">1</span>)</span><br><span class="line">        c = (predicted == labels).squeeze()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">            label = labels[i]</span><br><span class="line">            class_correct[label] += c[i].item()</span><br><span class="line">            class_total[label] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    print(<span class="string">'Accuracy of %5s : %2d %%'</span> % (</span><br><span class="line">        classes[i], <span class="number">100</span> * class_correct[i] / class_total[i]))</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<pre><code>Accuracy of plane : 46 %
Accuracy of   car : 63 %
Accuracy of  bird : 50 %
Accuracy of   cat : 37 %
Accuracy of  deer : 40 %
Accuracy of   dog : 51 %
Accuracy of  frog : 70 %
Accuracy of horse : 48 %
Accuracy of  ship : 76 %
Accuracy of truck : 64 %</code></pre><p>ok，那下一步干什么？</p>
<p>我们如何在GPU上运行神经网络？</p>
<h3 id="在GPU上训练"><a href="#在GPU上训练" class="headerlink" title="在GPU上训练"></a>在GPU上训练</h3><p>就像如何把张量移动到GPU上，把神经网络移动到GPU上。</p>
<p>如果我们有可用的CUDA， 首先让我们定义设备为第一个可得到的cuda设备。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">"cuda:0"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设我们有一个CUDA机器，然后我们打印CUDA设备：</span></span><br><span class="line"></span><br><span class="line">print(device)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<pre><code>cuda:0</code></pre><p>这个章节的剩余内容都假设<code>device</code>是一个CUDA设备。</p>
<p>然后这些方法将在所有module上递归运行，把它们的参数和缓冲转换成CUDA tensors。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net.to(device)</span><br></pre></td></tr></table></figure>

<p>记得你也要将每次迭代的输入和标签都送到GPU：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">inputs, labels = data[<span class="number">0</span>].to(device), data[<span class="number">1</span>].to(device)</span><br></pre></td></tr></table></figure>

<p>为什么我没有注意到与CPU相比的巨大加速?因为你的网络真的很小。</p>
<p><strong>练习</strong>: 尝试增加网络的宽度（第一个<code>nn.Conv2d</code>的参数2和第二个<code>nn.Conv2d</code>的参数1必须相等），看看你得到怎样的加速。</p>
<p><strong>已完成的目标</strong>:</p>
<ul>
<li>高层次地理解了pytorch张量的库和神经网络</li>
<li>训练了一个小的图片分类的神经网络</li>
</ul>
<h3 id="在多GPU上训练"><a href="#在多GPU上训练" class="headerlink" title="在多GPU上训练"></a>在多GPU上训练</h3><p>如果你想使用你全部的GPU看到巨大的加速，请参考下一章节</p>
<h2 id="选读：数据并行"><a href="#选读：数据并行" class="headerlink" title="选读：数据并行"></a>选读：数据并行</h2><p><a href="https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html#imports-and-parameters" target="_blank" rel="noopener">[原英文网站]</a></p>
<p>在这个教程里，我们将使用<code>DataParallel</code>学习如何使用多个GPUS（译者注：一机多卡）</p>
<p>pytorch很容易实现多GPU。你能把模型放到一个GPU上：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">"cuda:0"</span>)</span><br><span class="line">model.to(device)</span><br></pre></td></tr></table></figure>

<p>然后你可以把你的所有数据放到GPU上：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mytensor = my_tensor.to(device)</span><br></pre></td></tr></table></figure>

<p>请注意只是调用<code>my_tensor.to(device)</code>返回一个新的在GPU上的<code>my_tensor</code>的拷贝，而不是覆盖<code>my_tensor</code>。你需要分配一个新的tensor并使用这个在GPU上的tensor。</p>
<p>在多GPU上执行正向和反向传播是很自然的。然而，pytorch将默认使用一个GPU。你可以使用<code>DataParallel</code>使你的模型并行运行来简单地在多GPU上运行你的操作。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = nn.DataParallel(model)</span><br></pre></td></tr></table></figure>

<p>这就是本教程的核心。我们将在下面更详细地探讨它。</p>
<h3 id="引入和参数"><a href="#引入和参数" class="headerlink" title="引入和参数"></a>引入和参数</h3><p>引入pytorch模块和定义参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"></span><br><span class="line"><span class="comment"># Parameters and DataLoaders</span></span><br><span class="line">input_size = <span class="number">5</span></span><br><span class="line">output_size = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">30</span></span><br><span class="line">data_size = <span class="number">100</span></span><br></pre></td></tr></table></figure>

<p>设备：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">"cuda:0"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br></pre></td></tr></table></figure>

<h3 id="虚拟数据集"><a href="#虚拟数据集" class="headerlink" title="虚拟数据集"></a>虚拟数据集</h3><p>建立一个虚拟（随机）的数据集，你只需执行getitem。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RandomDataset</span><span class="params">(Dataset)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, length)</span>:</span></span><br><span class="line">        self.len = length</span><br><span class="line">        self.data = torch.randn(length, size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.data[index]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.len</span><br><span class="line"></span><br><span class="line">rand_loader = DataLoader(dataset=RandomDataset(input_size, data_size),</span><br><span class="line">                         batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<h3 id="简单的模型"><a href="#简单的模型" class="headerlink" title="简单的模型"></a>简单的模型</h3><p>作为小样，我们的模型只需获得输入，执行线性操作，然后得出输出。然而，你能使用<code>DataParallel</code>在任何模型上(CNN, RNN, Capsule Net等等。)</p>
<p>我们在模型中放置了一个print语句来监视输入和输出张量的大小。请注意批号为0的打印内容。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="comment"># Our model</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, output_size)</span>:</span></span><br><span class="line">        super(Model, self).__init__()</span><br><span class="line">        self.fc = nn.Linear(input_size, output_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span></span><br><span class="line">        output = self.fc(input)</span><br><span class="line">        print(<span class="string">"\tIn Model: input size"</span>, input.size(),</span><br><span class="line">              <span class="string">"output size"</span>, output.size())</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>

<h3 id="创建模型和数据并行"><a href="#创建模型和数据并行" class="headerlink" title="创建模型和数据并行"></a>创建模型和数据并行</h3><p>这是本教程的核心内容。首先，我们需要创建一个模型实例和检查是否有多个GPU。如果我们有多个GPU，我们可以使用<code>nn.DataParallel</code>封装你的模型。然后我们把模型通过<code>model.to(device)</code>放到GPU上。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">model = Model(input_size, output_size)</span><br><span class="line"><span class="keyword">if</span> torch.cuda.device_count() &gt; <span class="number">1</span>:</span><br><span class="line">  print(<span class="string">"Let's use"</span>, torch.cuda.device_count(), <span class="string">"GPUs!"</span>)</span><br><span class="line">  <span class="comment"># dim = 0 [30, xxx] -&gt; [10, ...], [10, ...], [10, ...] on 3 GPUs</span></span><br><span class="line">  model = nn.DataParallel(model)</span><br><span class="line"></span><br><span class="line">model.to(device)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<pre><code>Let&apos;s use 2 GPUs!</code></pre><h3 id="运行模型"><a href="#运行模型" class="headerlink" title="运行模型"></a>运行模型</h3><p>现在我们看一下输入和输出张量的大小。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> rand_loader:</span><br><span class="line">    input = data.to(device)</span><br><span class="line">    output = model(input)</span><br><span class="line">    print(<span class="string">"Outside: input size"</span>, input.size(),</span><br><span class="line">          <span class="string">"output_size"</span>, output.size())</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<pre><code>In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])
        In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])
Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])
        In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])
        In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])
Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])
        In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])
        In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])
Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])
        In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])
        In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])
Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])</code></pre><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>如果你没有或一个GPU，当你批处理30个输入，模型会得到30个预期的输出。但是如果你有多GPU，然后你会得到像这样的结果。</p>
<h4 id="2-GPUs"><a href="#2-GPUs" class="headerlink" title="2 GPUs"></a>2 GPUs</h4><p>如果你有2个，你将看到：</p>
<pre><code>Let&apos;s use 2 GPUs!
    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])
    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])
Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])
    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])
    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])
Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])
    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])
    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])
Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])
    In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])
    In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])
Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])</code></pre><h4 id="3GPUs"><a href="#3GPUs" class="headerlink" title="3GPUs"></a>3GPUs</h4><p>如果你有3个GPU，你将看到：</p>
<pre><code>Let&apos;s use 3 GPUs!
    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])
    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])
    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])
Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])
    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])
    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])
    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])
Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])
    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])
    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])
    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])
Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])
    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])
    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])
    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])
Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])</code></pre><h4 id="8GPUs"><a href="#8GPUs" class="headerlink" title="8GPUs"></a>8GPUs</h4><p>如果你有8个，你将看到：</p>
<pre><code>Let&apos;s use 8 GPUs!
    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])
    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])
    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])
    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])
    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])
    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])
    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])
    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])
Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])
    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])
    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])
    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])
    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])
    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])
    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])
    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])
    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])
Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])
    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])
    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])
    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])
    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])
    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])
    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])
    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])
    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])
Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])
    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])
    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])
    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])
    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])
    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])
Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])</code></pre><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>DataParallel会自动地分割你的数据，并将工作订单发送到多个gpu上的多个模型。在每个模型完成它们的工作后，DataParallel收集并合并结果，然后返回给你。</p>
<p>更多的信息可以查看<a href="https://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html." target="_blank" rel="noopener">https://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html.</a></p>

    </div>
      <p>———————————————感谢阅读———————————————</p>
<blockquote><p>欢迎收藏访问我的<a href="https://zinw623.github.io/">博客</a>  <a href="https://www.zhihu.com/people/tai-li-wu-yan-zu/activities" target="_blank" rel="noopener">知乎</a>   <a href="https://juejin.im/user/5d40058ef265da03b76afb66" target="_blank" rel="noopener">掘金</a>  <a href="https://www.jianshu.com/u/a38373c2e45b" target="_blank" rel="noopener">简书</a> <a href="https://www.zhihu.com/people/tai-li-wu-yan-zu/activities" target="_blank" rel="noopener">知乎</a></p></blockquote>
  <div class="popular-posts-header">相关文章</div>
  <ul class="popular-posts">
    <li class="popular-posts-item">
      
      
      <div class="popular-posts-title"><a href="\2019\09\04\DATA-LOADING-AND-PROCESSING\" rel="bookmark">翻译：pytorch数据加载和处理</a></div>
      
        <div class="popular-posts-excerpt"><p><p>在解决任何机器学习问题时，都需要花费大量的精力来准备数据。PyTorch提供了许多工具来简化数据加载，希望能使代码更具可读性。在这篇教程里，我们将看看如何从非平凡的数据集中加载和预处理/扩增数据。</p></p></div>
      
    </li>
  
    <li class="popular-posts-item">
      
      
      <div class="popular-posts-title"><a href="\2019\08\28\pytorch-doc-torch-autograd\" rel="bookmark">【翻译】pytorch中文文档（1.2.0）- Package Reference/torch.autograd</a></div>
      
    </li>
  
    <li class="popular-posts-item">
      
      
      <div class="popular-posts-title"><a href="\2019\08\26\pytorch-doc-notes\" rel="bookmark">【翻译】pytorch中文文档（1.2.0）- Notes部分</a></div>
      
    </li>
  
    <li class="popular-posts-item">
      
      
      <div class="popular-posts-title"><a href="\2019\08\26\pytorch-doc-torch\" rel="bookmark">【翻译】pytorch中文文档（1.2.0）- Package Reference/torch</a></div>
      
    </li>
  
    <li class="popular-posts-item">
      
      
      <div class="popular-posts-title"><a href="\2019\08\23\pytorch-tools\" rel="bookmark">【notes】pytorch学习笔记4-pytorch常用工具</a></div>
      
    </li>
  
  </ul>


    
    
    
      <div id="wechat_subscriber" style="display: block; padding: 10px 0; margin: 20px auto; width: 100%; text-align: center;">
  <img id="wechat_subscriber_qcode" src="/uploads/wechat-qcode.jpg" alt="涵贰十 wechat" style="width: 200px; max-width: 100%;">
  <div>欢迎扫描二维码订阅我的公众号！</div>
</div>

    
      <div>
        <div id="reward-container">
  <div></div>
  <button id="reward-button" disable="enable" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
        
      
      <div style="display: inline-block">
        <img src="/images/alipay.jpg" alt="涵贰十 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>

      </div>

    <footer class="post-footer">
          
        
        <div class="post-tags">
            <a href="/tags/pytorch/" rel="tag"><i class="fa fa-tag"></i> pytorch</a>
          
            <a href="/tags/翻译/" rel="tag"><i class="fa fa-tag"></i> 翻译</a>
          
        </div>
        <div class="post-widgets">
        

        

        
          <div class="social-share">
            
            
              <div id="needsharebutton-postbottom">
                <span class="btn">
                  <i class="fa fa-share-alt" aria-hidden="true"></i>
                </span>
              </div>
            
          </div>
        
        </div>
      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
              <a href="/2019/08/28/pytorch-doc-torch-autograd/" rel="next" title="【翻译】pytorch中文文档（1.2.0）- Package Reference/torch.autograd">
                <i class="fa fa-chevron-left"></i> 【翻译】pytorch中文文档（1.2.0）- Package Reference/torch.autograd
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
              <a href="/2019/09/04/DATA-LOADING-AND-PROCESSING/" rel="prev" title="翻译：pytorch数据加载和处理">
                翻译：pytorch数据加载和处理 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
    </footer>
  </div>
  
  
  
  </article>

  </div>


          </div>
          
    
    
  <div class="comments" id="comments">
    <div id="lv-container" data-id="city" data-uid="MTAyMC80NTY1OS8yMjE3MA=="></div>
  </div>
  
  


        </div>
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">

          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image"
      src="/images/avatar.jpg"
      alt="涵贰十">
  <p class="site-author-name" itemprop="name">涵贰十</p>
  <div class="site-description motion-element" itemprop="description"></div>
</div>
  <nav class="site-state motion-element">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">21</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">分类</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-tags">
        
          
            <a href="/tags/">
          
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">标签</span>
        </a>
      </div>
    
  </nav>
  <div class="feed-link motion-element">
    <a href="/atom.xml" rel="alternate">
      <i class="fa fa-rss"></i>RSS
    </a>
  </div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://github.com/zinw623" title="GitHub &rarr; https://github.com/zinw623" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://www.zhihu.com/people/tai-li-wu-yan-zu/activities" title="知乎 &rarr; https://www.zhihu.com/people/tai-li-wu-yan-zu/activities" rel="noopener" target="_blank"><i class="fa fa-fw fa-eye"></i>知乎</a>
      </span>
    
  </div>


  <div class="links-of-blogroll motion-element links-of-blogroll-inline">
    <div class="links-of-blogroll-title">
      <i class="fa  fa-fw fa-link"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.liaoxuefeng.com/wiki/1016959663602400" title="https://www.liaoxuefeng.com/wiki/1016959663602400" rel="noopener" target="_blank">廖雪峰python</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="https://docs.python.org/zh-cn/3/" title="https://docs.python.org/zh-cn/3/" rel="noopener" target="_blank">python中文文档</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="https://pytorch-cn.readthedocs.io/zh/latest/" title="https://pytorch-cn.readthedocs.io/zh/latest/" rel="noopener" target="_blank">pytorch中文文档</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="https://mist.theme-next.org/docs/" title="https://mist.theme-next.org/docs/" rel="noopener" target="_blank">next doc</a>
        </li>
      
    </ul>
  </div>

        </div>
      </div>
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#什么是PYTORCH"><span class="nav-number">1.</span> <span class="nav-text">什么是PYTORCH?</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#准备开始"><span class="nav-number">1.1.</span> <span class="nav-text">准备开始</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Tensors"><span class="nav-number">1.1.1.</span> <span class="nav-text">Tensors</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#操作"><span class="nav-number">1.1.2.</span> <span class="nav-text">操作</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#numpy桥"><span class="nav-number">1.2.</span> <span class="nav-text">numpy桥</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#将NumPy-Array转换成Torch-Tensor"><span class="nav-number">1.2.1.</span> <span class="nav-text">将NumPy Array转换成Torch Tensor</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CUDA张量"><span class="nav-number">1.3.</span> <span class="nav-text">CUDA张量</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#AUTOGRAD-自动求导"><span class="nav-number">2.</span> <span class="nav-text">AUTOGRAD:自动求导</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Tensor"><span class="nav-number">2.1.</span> <span class="nav-text">Tensor</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#梯度"><span class="nav-number">2.2.</span> <span class="nav-text">梯度</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#神经网络"><span class="nav-number">3.</span> <span class="nav-text">神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#定义网络"><span class="nav-number">3.1.</span> <span class="nav-text">定义网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#损失函数"><span class="nav-number">3.2.</span> <span class="nav-text">损失函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#反向传播"><span class="nav-number">3.3.</span> <span class="nav-text">反向传播</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#更新参数"><span class="nav-number">3.4.</span> <span class="nav-text">更新参数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#训练一个分类器"><span class="nav-number">4.</span> <span class="nav-text">训练一个分类器</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#那数据呢？"><span class="nav-number">4.1.</span> <span class="nav-text">那数据呢？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#训练一个图片分类器"><span class="nav-number">4.2.</span> <span class="nav-text">训练一个图片分类器</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-加载和规范化CIFAR10"><span class="nav-number">4.2.1.</span> <span class="nav-text">1.加载和规范化CIFAR10</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-定义一个卷积神经网络"><span class="nav-number">4.2.2.</span> <span class="nav-text">2.定义一个卷积神经网络</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-定义损失函数和优化器"><span class="nav-number">4.2.3.</span> <span class="nav-text">3.定义损失函数和优化器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-训练网络"><span class="nav-number">4.2.4.</span> <span class="nav-text">4.训练网络</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-在测试集上测试网络"><span class="nav-number">4.2.5.</span> <span class="nav-text">5.在测试集上测试网络</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#在GPU上训练"><span class="nav-number">4.3.</span> <span class="nav-text">在GPU上训练</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#在多GPU上训练"><span class="nav-number">4.4.</span> <span class="nav-text">在多GPU上训练</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#选读：数据并行"><span class="nav-number">5.</span> <span class="nav-text">选读：数据并行</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#引入和参数"><span class="nav-number">5.1.</span> <span class="nav-text">引入和参数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#虚拟数据集"><span class="nav-number">5.2.</span> <span class="nav-text">虚拟数据集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#简单的模型"><span class="nav-number">5.3.</span> <span class="nav-text">简单的模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#创建模型和数据并行"><span class="nav-number">5.4.</span> <span class="nav-text">创建模型和数据并行</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#运行模型"><span class="nav-number">5.5.</span> <span class="nav-text">运行模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#结果"><span class="nav-number">5.6.</span> <span class="nav-text">结果</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-GPUs"><span class="nav-number">5.6.1.</span> <span class="nav-text">2 GPUs</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3GPUs"><span class="nav-number">5.6.2.</span> <span class="nav-text">3GPUs</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8GPUs"><span class="nav-number">5.6.3.</span> <span class="nav-text">8GPUs</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#总结"><span class="nav-number">5.7.</span> <span class="nav-text">总结</span></a></li></ol></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">涵贰十</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    <span title="站点总字数">133k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">2:01</span>
</div>


<span id="sitetime"></span>
<script language=javascript>
	function siteTime(){
		window.setTimeout("siteTime()", 1000);
		var seconds = 1000;
		var minutes = seconds * 60;
		var hours = minutes * 60;
		var days = hours * 24;
		var years = days * 365;
		var today = new Date();
		var todayYear = today.getFullYear();
		var todayMonth = today.getMonth()+1;
		var todayDate = today.getDate();
		var todayHour = today.getHours();
		var todayMinute = today.getMinutes();
		var todaySecond = today.getSeconds();
		/* Date.UTC() -- 返回date对象距世界标准时间(UTC)1970年1月1日午夜之间的毫秒数(时间戳)
		year - 作为date对象的年份，为4位年份值
		month - 0-11之间的整数，做为date对象的月份
		day - 1-31之间的整数，做为date对象的天数
		hours - 0(午夜24点)-23之间的整数，做为date对象的小时数
		minutes - 0-59之间的整数，做为date对象的分钟数
		seconds - 0-59之间的整数，做为date对象的秒数
		microseconds - 0-999之间的整数，做为date对象的毫秒数 */
		var t1 = Date.UTC(2019,07,30,15,00,00); 
		var t2 = Date.UTC(todayYear,todayMonth,todayDate,todayHour,todayMinute,todaySecond);
		var diff = t2-t1;
		var diffYears = Math.floor(diff/years);
		var diffDays = Math.floor((diff/days)-diffYears*365);
		var diffHours = Math.floor((diff-(diffYears*365+diffDays)*days)/hours);
		var diffMinutes = Math.floor((diff-(diffYears*365+diffDays)*days-diffHours*hours)/minutes);
		var diffSeconds = Math.floor((diff-(diffYears*365+diffDays)*days-diffHours*hours-diffMinutes*minutes)/seconds);
		document.getElementById("sitetime").innerHTML=" 已运行"+/*diffYears+" 年 "+*/diffDays+" 天 "+diffHours+" 小时 "+diffMinutes+" 分钟 "+diffSeconds+" 秒";
	}/*因为建站时间还没有一年，就将之注释掉了。需要的可以取消*/
	siteTime();
</script>

        




  <script>
    (function() {
      var hm = document.createElement("script");
      hm.src = "//tajs.qq.com/stats?sId=66456019";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




        
      </div>
    </footer>
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
      </div>

    
  
  <div id="needsharebutton-float">
    <span class="btn">
      <i class="fa fa-share-alt" aria-hidden="true"></i>
    </span>
  </div>
<script src="/lib/needsharebutton/needsharebutton.js"></script>
<script>
    pbOptions = {};
      pbOptions.iconStyle = "box";
    
      pbOptions.boxForm = "horizontal";
    
      pbOptions.position = "bottomCenter";
    
      pbOptions.networks = "Weibo,Wechat,Douban,QQZone,Twitter,Facebook";
    
    new needShareButton('#needsharebutton-postbottom', pbOptions);
    flOptions = {};
      flOptions.iconStyle = "box";
    
      flOptions.boxForm = "horizontal";
    
      flOptions.position = "middleRight";
    
      flOptions.networks = "Weibo,Wechat,Douban,QQZone,Twitter,Facebook";
    
    new needShareButton('#needsharebutton-float', flOptions);
</script>


  </div>

  
    
    
  
  <script color='23,23,23' opacity='0.7' zIndex='-2' count='120' src="/lib/canvas-nest/canvas-nest-nomobile.min.js"></script>
  <script src="/lib/jquery/index.js?v=3.4.1"></script>
  <script src="/lib/mediumzoom/medium-zoom.min.js"></script>
  <script src="/lib/lazyload/lozad.min.js?v=1.10.0"></script>
  <script src="/lib/reading_progress/reading_progress.js"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  <script src="/js/utils.js?v=7.2.0"></script>
  <script src="/js/motion.js?v=7.2.0"></script>

  
  <script src="/js/affix.js?v=7.2.0"></script>
  <script src="/js/schemes/pisces.js?v=7.2.0"></script>



  
  <script src="/js/scrollspy.js?v=7.2.0"></script>
<script src="/js/post-details.js?v=7.2.0"></script>



  <script src="/js/next-boot.js?v=7.2.0"></script>

  
  <script src="/js/js.cookie.js?v=7.2.0"></script>
  <script src="/js/scroll-cookie.js?v=7.2.0"></script>

  

  


  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>





  
    
      <script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script src="//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    
  




  <script src="/lib/bookmark/bookmark.min.js?v=1.0"></script>
  <script>
    bookmark.scrollToMark('auto', "#更多");
  
  </script>















  <script src="/js/local-search.js?v=7.2.0"></script>








<script>
if ($('body').find('div.pdf').length) {
  $.ajax({
    type: 'GET',
    url: '//cdn.jsdelivr.net/npm/pdfobject@2/pdfobject.min.js',
    dataType: 'script',
    cache: true,
    success: function() {
      $('body').find('div.pdf').each(function(i, o) {
        PDFObject.embed($(o).attr('target'), $(o), {
          pdfOpenParams: {
            navpanes: 0,
            toolbar: 0,
            statusbar: 0,
            pagemode: 'thumbs',
            view: 'FitH'
          },
          PDFJS_URL: '/lib/pdf/web/viewer.html',
          height: $(o).attr('height') || '500px'
        });
      });
    },
  });
}
</script>


<script>
if ($('body').find('pre.mermaid').length) {
  $.ajax({
    type: 'GET',
    url: '//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js',
    dataType: 'script',
    cache: true,
    success: function() {
      mermaid.initialize({
        theme: 'neutral',
        logLevel: 3,
        flowchart: { curve: 'linear' },
        gantt: { axisFormat: '%m/%d/%Y' },
        sequence: { actorMargin: 50 }
      });
    }
  });
}
</script>



    

<script>
  window.livereOptions = {
    refer: '2019/09/03/pytorch-60-minutes/'
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
</script>


</body>
</html>
