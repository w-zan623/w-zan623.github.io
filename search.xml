<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[【翻译】pytorch中文文档（1.2.0）- Notes部分]]></title>
    <url>%2F2019%2F08%2F26%2Fpytorch_doc-notes%2F</url>
    <content type="text"><![CDATA[autograd机制 对应的英文版文档https://pytorch.org/docs/stable/notes/autograd.html 这篇笔记将会展示自动求导是如何工作和如何记录操作的概述，没有绝对的必要去理解这些全部内容，但是我们推荐最好熟悉它，因为它会帮助你写出更有效率、更简洁的程序，并且在调试时会帮助到你。 在反向传播（backwards）时排除子图每一个张量都有一个标示：requires_grad，它使得在梯度计算时精细地排除子图并且变得更有效率。 requires_grad如果一个操作仅有一个输入且需要梯度，那么它的输出也需要梯度。相反，只有所有的输入都不需要梯度，输出才会不需要梯度。如果子图中所有的张量都不需要梯度，那么反向传播就不会再其中执行。 12345x = torch.randn(5, 5) # requires_grad = False by defaulty = torch.randn(5, 5) # requires_grad = False by defaultz = torch.randn((5, 5), requires_grad = True)a = x + ya.requires_grad False当你想要冻结你模型的一部分或者你提前知道你将不使用一些参数的梯度。例如，如果你想要微调一个预训练过的卷积神经网络，将要冻结的部分的requires_grad标示切换就足够了，并且知道计算到最后一层才会被保存到中间缓存区，其中仿射变换将使用需要梯度的权重，并且网络的输出也将会需要它们。 1234567891011model = torchvision.models.resnet18(pretrained = True)for param in model.parameters(): param.requires_grad = False# Replace the last fully-connected layer# Parameters of newly constructed modules have # requires_grad = True by defaultmodel.fc = nn.Linear(512, 100)# Optimize only the classifieroptimizer = optim.SGD(model.fc.parameters(), lr = 1e-2, momentum = 0.9) autograd如何编码历史信息Autograd是反向的自动求导系统。概念上，autograd记录了一张图，这张图记录了所有的操作，当你运行这些操作时它们会产生数据。得到的这张图是一个有向无环图，图的叶子节点是输入张量，根节点是输出张量。通过从根节点到叶子节点跟踪这张图，你可以自动地使用链式法则来计算梯度。 在内部，autograd将会将这张图表示为 Function 对象组成的图（真正的表达），函数可以通过 apply() 来求图的值。当计算前向传播时，autograd同时做到执行请求操作和建立用来表示计算梯度的函数的图（每一个 torch.Tensor 的 .grad_fn 属性是进入这张（用来计算梯度的）图的入口）。当前向传播完成后，我们求出这场图在反向传播时的值来计算梯度。 一件需要注意的重要的事是图在每次迭代时会重新建立图，并且这允许使用任意的python控制语句，即使这些语句每次迭代都会改变图的整个形状和大小。你不必在启动训练之前编写出所有可能的路径-what you run is what you differentiate（你运行什么就会对什么求导）。 autograd的In-place操作在autograd中使用in-place操作是困难的事，并且我们在大多数情况下不鼓励使用。Autograd的缓存区积极地释放和重用非常高效,很少场合in-place操作能明显地降低内存的使用。如果不是你的操作在很大的内存压力下，你可能永远不会使用它们。 对于限制in-place操作的适用范围有两个主要的原因： in-place操作能潜在地覆盖梯度计算所需要的值。 每一个in-place操作确实需要实施重写计算图。out-of-place版本仅是分配新的对象并且保持对旧图的引用。而in-place操作需要把所有输入的creator改为代表这些操作的Function。这会比较棘手，特别是有很多Tensors共享相同的内存（storage）（例如通过索引或转置创建），并且如果被修改的输入的储存（storage）被其他的任何的Tensor引用，那么in-place会抛出错误。 In-place正确性检查每一个tensor都保留一个版本记数器（version counter），当张量在任何操作中被使用后，它每次都会递增。当Function为反向传播保存任何张量时，这些保留的张量的版本计数器也会被保存。一旦你是用self.saved_tensors它将会被检查，并且如果它大于被保存的值将会抛出错误。这确保了如果你是用in-place操作并且没有看到任何操作，你就能确定被计算出的梯度是正确的。 广播语义 对应的英文版文档：https://pytorch.org/docs/stable/notes/broadcasting.html 许多pytorch的操作支持NumPy广播语义。 简而言之，如果pytorch操作支持广播，那么它的张量参数会被自动地扩展成相等的大小（无需复制数据） 一般语义如果满足以下规则，那么两个张量是可广播的： 每个张量至少有一个维度。 当迭代维度的大小时，从末尾（trailing）的维度开始，维度大小必须相等，或者它们中的一个的维度大小为1，或者它们中的一个的维度不存在。例如： 12345678910111213141516171819202122x = torch.empty(5, 7, 3)y = torch.empty(5, 7, 3)# 相同形状的永远是可广播的（上面的规则永远成立）x = torch.empty((0,))y = torch.empty(2, 2)# x和y不是可广播的，因为x没有至少一个维度# 可以排列出的维度x = torch.empty(5, 3, 4, 1)y = torch.empty( 3, 1, 1)# x和y是可广播的# 第一末尾的维度：都是1。# 第二靠近尾部的维度：y是1# 第三靠近尾部的维度：x的大小等于y的大小# 第四靠近尾部的维度：y的该维度不存在# 但是：x = torch.empty(5,2,4,1)y = torch.empty( 3,1,1)# x和y是不可广播的# 因为第三靠近尾部的维度x的2不等于y的3 如果两个张量x，y是“可广播的”，那么结果张量的大小是按照下面的方法计算的： 如果x和y的维度的长度不相等，就在维度个数更少的张量的维度前面加1，使两个张量的维度相等。 然后，对于每个维度的大小，最后得出的结果的维度大小是x和y中维度大小最大的那一个的值。 例如： 1234# 可以列出各维度来使阅读更容易x = torch.empty(5, 1, 4, 1)y = torch.empty( 3, 1, 1)(x + y).size() torch.Size([5, 3, 4, 1])1234# 但是没有必要:x = torch.empty(1)y = torch.empty(3, 1, 7)(x + y).size() torch.Size([3, 1, 7])123x = torch.empty(5, 2, 4, 1)y = torch.empty(3, 1, 1)(x + y).size() --------------------------------------------------------------------------- RuntimeError Traceback (most recent call last) &lt;ipython-input-9-d19949393c3d&gt; in &lt;module&gt; 1 x = torch.empty(5, 2, 4, 1) 2 y = torch.empty(3, 1, 1) ----&gt; 3 (x + y).size() RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1In-place语义一个复杂的问题是in-place操作不允许in-place张量由于广播而改变形状。 例如： 123x = torch.empty(5, 3, 4, 1)y = torch.empty(3, 1, 1)(x.add_(y)).size() torch.Size([5, 3, 4, 1])1234567# 但是：x = torch.empty(1, 3, 1)y = torch.empty(3, 1, 7)(x.add_(y)).size()# 一般广播会将x的size改成(3, 3, 7)# 但是x为in-place张量 --------------------------------------------------------------------------- RuntimeError Traceback (most recent call last) &lt;ipython-input-16-300828b970da&gt; in &lt;module&gt; 2 x = torch.empty(1, 3, 1) 3 y = torch.empty(3, 1, 7) ----&gt; 4 (x.add_(y)).size() 5 6 # 一般广播会将x的size改成(3, 3, 7) RuntimeError: output with shape [1, 3, 1] doesn&apos;t match the broadcast shape [3, 3, 7]反向传播兼容性pytorch的较早版本允许在不同形状的张量上执行逐点函数（pointwise functions），只要这些张量的元素数量相等。然后逐点函数将把各张量看作一维的张量然后执行。pytorch现在支持广播而“一维”逐点计算已经被弃用了，并且在张量不可广播但元素数目一样的情况下将生成Python警告。 注意，如果两个张量形状不同，但可广播且元素数目相同，则引入广播会导致反向传播不兼容的变化。 1torch.add(torch.ones(4, 1), torch.randn(4)) tensor([[2.8629, 0.4929, 0.8330, 0.1047], [2.8629, 0.4929, 0.8330, 0.1047], [2.8629, 0.4929, 0.8330, 0.1047], [2.8629, 0.4929, 0.8330, 0.1047]])这个例子之前生成size为[4,1]的张量，但是现在生成了一个size为[4,4]的张量。为了帮助识别代码里出现由于广播而导致的反向传播不兼容的情况，可以将torch.utils.backcompat.broadcast_warning设为True，在这种情况下将生成Python警告 1torch.utils.backcompat.broadcast_warning.enable = True 1torch.add(torch.ones(4, 1), torch.ones(4)) tensor([[2., 2., 2., 2.], [2., 2., 2., 2.], [2., 2., 2., 2.], [2., 2., 2., 2.]])译者注： 文档里说运行会出现下面这个warning，但是实际运行没用出现，咱也不知道为啥，咱也不知道问谁。 __main__:1: UserWarning: self and other do not have the same shape, but are broadcastable, and have the same number of elements.Changing behavior in a backwards incompatible manner to broadcasting rather than viewing as 1-dimensional. 未完待译。。。]]></content>
      <categories>
        <category>翻译</category>
        <category>pytorch文档（1.2.0）</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>翻译</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【翻译】pytorch中文文档（1.2.0）- Package Reference/torch]]></title>
    <url>%2F2019%2F08%2F26%2Fpytorch_doc-torch%2F</url>
    <content type="text"><![CDATA[TORCHtorch package包含多维张量和定义好的数学运算的数据结构。另外，它提供了许多实用程序用于有效的序列化张量和任意类型，以及其他有用的实用程序。 它支持CUDA环境，使你能在NVIDIA GPU上进行你的张量计算，这要求你compute capability&gt;=3.0。 Tensors torch.is_tensor(obj) [源码] 如果是一个PyTorch tensor返回True Parameters obj(Object)-要测试的对象 torch.is_storage(obj) [源码] 如果obj是pytorch storage对象就返回True Parameters obj( Object ) - 要测试的对象 torch.is_floating_point(input) ——&gt; (bool) 如果input的数据类型是浮点型数据（即torch.float64, torch.float32和torch.float16中的一种），就返回True。 _Parameters input(Tensor) —— 要测试的pytorch张量 torch.set_default_dtype(d) [源码] 把默认的浮点数类型的dtype设为`d`。这种类型将被用做`torch.tensor()`类型推断的默认浮点类型。 这个默认的浮点dtype是初始为torch.float32 * _Parameters_ __d__(`torch.dtype`) —— 将被设为默认的浮点型dtype 例如: 12345&gt;&gt;&gt; torch.tensor([1.2, 3]).dtype # 初始默认的浮点型是torch.float32torch.float32&gt;&gt;&gt; torch.set_default_dtype(torch.float64)&gt;&gt;&gt; torch.tensor([1.2, 3]).dtype # 新的浮点型tensortorch.float64 torch.get_default_dtype() ——&gt; torch.dtype 获得当前默认浮点型torch.dtype。 例如： 123456789&gt;&gt;&gt; torch.get_default_dtype() # 初始默认的浮点型为torch.float32torch.float32&gt;&gt;&gt; torch.set_default_dtype(torch.float64)&gt;&gt;&gt; torch.get_default_dtype() # 默认改为torch.float64torch.float64&gt;&gt;&gt; torch.set_default_tensor_type(torch.FloatTensor) # 设置tensor type也会影响这个&gt;&gt;&gt; torch.get_default_dtype() # 改为了对应于torch.FloatTensor对应的dtype：torch.float32torch.float32 torch.set_default_tensor_type( t ) [源码] 设置默认的torch.Tensor类型为浮点型tensor类型t。这种类型也将会被勇作为torch.tensor()类型推断的默认浮点型类型。 默认的浮点型tensor type是初始化为torch.FloatTensor。 Parameters t( type or string ) —— 浮点型tensor type或者它的名字 例如： 12345&gt;&gt;&gt; torch.tensor([1.2, 3]).dtype # 初始的默认浮点型是torch.float32torch.float32&gt;&gt;&gt; torch.set_default_tensor_type(torch.DoubleTensor)&gt;&gt;&gt; torch.tensor([1.2, 3]).dtype # 新的浮点型tensortorch.float64 torch.numel( input ) ——&gt; int 返回input张量里元素的数量 Parameters input( Tensor ) —— 输入的向量 例如： 123456789101112131415161718192021222324252627282930313233343536373839404142434445 &gt;&gt;&gt; a = torch.randn(1, 2, 3, 4, 5) &gt;&gt;&gt; torch.numel(a) 120 &gt;&gt;&gt; a = torch.zeros(4, 4) &gt;&gt;&gt; torch.numel(a) 16 ``` * torch.set_printoptions( _precision=None, threshold=None, edgeitems=None, linewidth=None, profile=None, sci_mode=None_ ) [[源码]](https://pytorch.org/docs/stable/_modules/torch/_tensor_str.html#set_printoptions) 为print设置选项。从NumPy中无耻地拿出的项目 * _Parameters_ * __precision__ —— 输出浮点型精度的位数（默认为4） * __threshold__ —— 输出时的阈值，当数组总和超过阈值，会被截断输出（默认为1000） * __edgeitems__ —— 每个维度统计的数组条目的数量（默认为3） * __linewidth__ —— 为了插入换行符每行设置的字符数（默认为80）。Thresholded矩阵将会忽略这个参数 * __profile__ —— Sane为了好的打印的默认设置，可以用以上任何选项覆盖掉（可以是 _default,short,full_ 中任意一个） * __sci_mode__ —— 是（True）否（False）使用科学符号。如果指定是None（默认），那么值将会被_Formatter定义。* torch.set_flush_denormal( _mode_ ) ——&gt; bool 禁用CPU上的非规格化的浮点数 如果你的系统支持非规格化数字（flushing denormal numbers）并且成功配置非规格化模式（flush denormal mode）将会返回True。`set_flush_denormal()`仅使用在支持SSE3的x86架构。 * _Parameters_ __mode__( _boor_ ) —— 控制是否使用非规格化模式（flush denormal mode） 例如： ```python &gt;&gt;&gt; torch.set_flush_denormal(True) True &gt;&gt;&gt; torch.tensor([1e-323], dtype = torch.float32) tensor([0,], dtype=torch.float64) &gt;&gt;&gt; torch.set_flush_denormal(False) True &gt;&gt;&gt; torch.tensor([1e-323], dtype=torch.float64) tensor(9.88131e-324 * [ 1.0000], dtype=torch.float64) Creation Ops NOTE 随机抽样创建操作被列在Random sampling之下，包括：torch.rand() torch.rand_like() torch.randn_like() torch.randint() torch.randint_like() torch.randperm() 你可能也使用torch.empty()并使用In-place random sampling方法来更广泛的分布范围中取样的值来创建torch.Tensors torch.tensor(data, dtype=None, device=None, requires_grad=False, pin_memory=False) ——&gt; Tensor 通过data构造一个张量 WARNING torch.tensor()总是拷贝data。如果你有一个Tensordata而且想避免拷贝，请使用torch.Tensor.requires_grad_()或torch.Tensor.detach()。如果你有一个NumPyndarray并且向避免拷贝，请使用torch.as_tensor()。 WARNING 当data是一个tensor x，torch.tensor()会读取出“the data”，无论之前传入过什么，并且构建一个leaf variable（）。因此torch.tensor(x)等价于x.clone().detach()，torch.tensor(x, requires_grad = True)等价于x.clone().detach().requires_grad_(True)。对于等价的操作，推荐使用clone()和detach()。 Parameters data( array_like ) —— 为tensor初始化data。可以是list，tuple，Numpy ndarray，scalar，和其他类型。 dtype(torch.dtype, optional) —— 返回tensor期望的数据类型。默认：如果为None，从data中推断数据类型 device(torch.device, optional) —— 返回tensor期望使用的硬件。默认：如果是None，对当前张量类型使用当前硬件（参考torch.set_default_tensor_type())。device可以是提供CPU张量类型的CPU和支持CUDA张量类型的CUDA设备。 requires_grad( bool, optional) —— 如果自动求导应当记录返回张量的操作。默认：False pin_memory( bool, optional) —— 如果设置，返回的向量分配到锁页内存（pinned memory)。这仅对CPU张量有效。默认：False。 例如： 1234567891011121314151617&gt;&gt;&gt; torch.tensor([[0.1, 1.2], [2.2, 3.1], [4.9, 5.2]])tensor([[ 0.1000, 1.2000], [ 2.2000, 3.1000], [ 4.9000, 5.2000]])&gt;&gt;&gt; torch.tensor([0, 1]) # 基于data判断类型tensor([ 0, 1])&gt;&gt;&gt; torch.tensor([[0.11111, 0.222222, 0.3333333]], dtype=torch.float64, device=torch.device('cuda:0')) # 创建一个torch.cuda.DoubleTensortensor([[ 0.1111, 0.2222, 0.3333]], dtype=torch.float64, device='cuda:0')&gt;&gt;&gt; torch.tensor(3.14159) # 创建一个标量tensor(3.1416)&gt;&gt;&gt; torch.tensor([]) # 创建一个空张量（size为(0,))tensor([])Indexing, Slicing, Joining, Mutating OpsGeneratorsRandom samplingIn-place random samplingQuasi-random samplingSerializationParallelismLocally disabling gradient computationMath operationsPointwise OpsReduction OpsComparision OpsSpectral OpsOther OperationsBLAS and LAPACK OperationsUtilities]]></content>
      <categories>
        <category>翻译</category>
        <category>pytorch文档（1.2.0）</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>翻译</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【notes】pytorch学习笔记4-pytorch常用工具]]></title>
    <url>%2F2019%2F08%2F23%2Fpytorch-tools%2F</url>
    <content type="text"><![CDATA[pytorch中常见的工具在训练神经网络的过程中需要用到很多工具、其中最重要的三部分是数据、可视化和GPU加速。本章主要介绍pytorch在这几方面常用的工具，合理使用这些工具能极大地提高编码效率。 数据处理在解决深度学习问题的过程中，往往需要花费大量的精力去处理数据，包括图像、文本、语音或其他二进制数据等。数据的处理对训练神经网络来说十分重要，良好的数据处理不仅会加速模型训练，也会提高模型效果。 数据加载在pytorch中，数据加载可以通过自定义的数据集对象实现。数据集对象被抽象成Dataset类，实现自定义的数据集需要继承Dataset，并实现两个Python魔法方法。 __getitem__：返回一条数据或一个样本。obj[index]等价于obj.__getitem__(index)。 __len__：返回样本的数量。len(obj)等价于obj.__len__()。 这里我们用Kaggle经典挑战赛“Dogs vs.Cat” 12%env LS_COLORS = None!tree --charset ascii data/dogcat/ env: LS_COLORS=None data/dogcat/ |-- cat.0.jpg |-- cat.1.jpg |-- cat.2.jpg |-- cat.3.jpg |-- cat.4.jpg |-- cat.5.jpg |-- cat.6.jpg |-- dog.0.jpg |-- dog.1.jpg |-- dog.10.jpg |-- dog.1000.jpg `-- dog.10000.jpg 0 directories, 12 files123import torch as tfrom torch.utils import dataimport matplotlib.pyplot as plt 123import osfrom PIL import Imageimport numpy as np 12345678910111213141516class DogCat(data.Dataset): def __init__(self, root): imgs = os.listdir(root) # 所有图片的绝对路径 self.imgs = [os.path.join(root, img) for img in imgs] def __getitem__(self, index): img_path = self.imgs[index] # dog-&gt;1 cat-&gt;0 label = 1 if 'dog' in img_path.split('/')[-1] else 0 pil_img = Image.open(img_path) array = np.asarray(pil_img) data = t.from_numpy(array) return data, label def __len__(self): return len(self.imgs) 1234dataset = DogCat('./data/dogcat/')img, label = dataset[0]for img, label in dataset: print(img.size(), img.float().mean(), label) torch.Size([375, 499, 3]) tensor(116.7904) 1 torch.Size([499, 327, 3]) tensor(133.5602) 1 torch.Size([144, 175, 3]) tensor(166.6151) 0 torch.Size([292, 269, 3]) tensor(157.4856) 1 torch.Size([375, 499, 3]) tensor(96.8243) 0 torch.Size([375, 499, 3]) tensor(120.7302) 1 torch.Size([280, 300, 3]) tensor(71.6653) 0 torch.Size([396, 312, 3]) tensor(131.8400) 0 torch.Size([303, 400, 3]) tensor(129.1319) 0 torch.Size([374, 500, 3]) tensor(119.7826) 0 torch.Size([412, 263, 3]) tensor(152.9542) 1 torch.Size([414, 500, 3]) tensor(156.6921) 0通过上面的代码，我们学习了如何自定义自己的数据集，并可以依次获取。但这里返回的数据不适用实际使用，因其具有如下两方面问题： 返回样本的形状之一，每张图片的大小不一样，这对于需要去batch训练的神经网络来说很不友好。 返回样本的数值较大，为归一化至[-1, 1]针对上述问题，pytorch提供了torchvision。它是一个视觉工具包，提供了很多视觉图像处理的工具，其中transforms模块提供了对PIL Image对象和Tensor对象的常用操作。 针对上述问题，pytorch提供torchvision。它是一个视觉工具包，提供了很多视觉图像处理的工具，其中transforms模块提供了对PIL Image对象和Tensor对象的常用操作。对PIL Image的常见操作如下。 Resize：调整图片尺寸 CenterCrop、RandomCrop、RandomSizedCrop：裁剪图片。 Pad：填充。 ToTensor：将PIL Image对象转成Tensor，会自动将[0, 225]归一化至[0, 1]。 对Tensor的常见操作如下。 Normalize：标准化，即减均值，除以标准差。 ToPILImage：将Tensor转为PIL Image对象。 如果要对图片进行多个操作，可通过Compose将这些操作拼接起来，类似于nn.Sequential。注意，这些操作定义后是以对象的形式存在，真正使用时需要调用它的__call__方法，类似于nn.Module。下面利用这些操作来优化上面的dataset。 1234567891011121314151617181920212223242526272829import osfrom PIL import Imageimport numpy as npfrom torchvision import transforms as Ttransform = T.Compose([ T.Resize(224), # 缩放图片（Image），保持长宽比不变，最短边为224像素 T.CenterCrop(224), # 从中间取出224 x224的图片 T.ToTensor(), # 将图片转换成Tensor并归一化至[0, 1] T.Normalize(mean = [.5, .5, .5], std = [.5, .5 , .5]) # 标准化至[-1, 1]])class DogCat(data.Dataset): def __init__(self, root, transforms = None): imgs = os.listdir(root) self.imgs = [os.path.join(root, img) for img in imgs] self.transforms = transforms def __getitem__(self, index): img_path = self.imgs[index] label = 0 if 'dog' in img_path.split('/')[-1] else 1 data = Image.open(img_path) if self.transforms: data = self.transforms(data) return data, label def __len__(self): return len(self.imgs) 12345from torchvision.transforms import ToPILImagedataset = DogCat('./data/dogcat/', transforms = transform)img, label = dataset[0]for img, label in dataset: print(img.size(), label) torch.Size([3, 224, 224]) 0 torch.Size([3, 224, 224]) 0 torch.Size([3, 224, 224]) 1 torch.Size([3, 224, 224]) 0 torch.Size([3, 224, 224]) 1 torch.Size([3, 224, 224]) 0 torch.Size([3, 224, 224]) 1 torch.Size([3, 224, 224]) 1 torch.Size([3, 224, 224]) 1 torch.Size([3, 224, 224]) 1 torch.Size([3, 224, 224]) 0 torch.Size([3, 224, 224]) 1除了上述操作之外，transforms还可以通过Lambda封装自定义的转换策略。例如，想对PIL Image进行随机旋转，则可写成trans = T.Lambda(lambda img: img.rotate(random() * 360))。 torchvision已经预先实现了常用的Dataset，包括前面使用过的CIFAR-10，以及ImageNet、COCO、MNIST、LSUN等数据集，可通过调用torchvision.datasets下相应对象来调用相关数据集。还有一个经常使用到的Dataset——ImageFolder，它的实现和上述DogCat很相似。ImageFolder假设所有的文件按文件夹保存，每个文件夹下储存同一个类别的图片，文件夹名为类名，其构造函数如下： 1ImageFolder(root, transform = None, target_transform = None, loader = default_loader) 它主要有以下四个参数。 root：在root指定的路径下寻找图片。 transform：对PIL Image进行转换操作，transform的输入是使用loader读取图片的返回对象。 target_transform：对label的转换。 loader：指定加载图片的函数，默认操作是读取为PIL Image对象。 label是按照文件夹名顺序排序后存成字典的，即{类名:类序号}，一般来说最好直接将文件夹命名为从0开始的数字，这样会和ImageFolder实际的label一致，如果不是这种命名规范，建议通过self.class_to_idx属性了解label和文件夹名的映射关系。 1!tree --charset ASCII data/dogcat2/ data/dogcat2/ |-- cat | |-- cat.0.jpg | |-- cat.1.jpg | |-- cat.2.jpg | |-- cat.3.jpg | `-- cat.4.jpg `-- dog |-- dog.0.jpg |-- dog.1.jpg |-- dog.10.jpg |-- dog.1000.jpg `-- dog.10000.jpg 2 directories, 10 files12from torchvision.datasets import ImageFolderdataset = ImageFolder('data/dogcat2/') 1dataset.class_to_idx {&apos;cat&apos;: 0, &apos;dog&apos;: 1}1dataset.imgs [(&apos;data/dogcat2/cat/cat.0.jpg&apos;, 0), (&apos;data/dogcat2/cat/cat.1.jpg&apos;, 0), (&apos;data/dogcat2/cat/cat.2.jpg&apos;, 0), (&apos;data/dogcat2/cat/cat.3.jpg&apos;, 0), (&apos;data/dogcat2/cat/cat.4.jpg&apos;, 0), (&apos;data/dogcat2/dog/dog.0.jpg&apos;, 1), (&apos;data/dogcat2/dog/dog.1.jpg&apos;, 1), (&apos;data/dogcat2/dog/dog.10.jpg&apos;, 1), (&apos;data/dogcat2/dog/dog.1000.jpg&apos;, 1), (&apos;data/dogcat2/dog/dog.10000.jpg&apos;, 1)]123# 没有任何transform，所以返回的还是PIL Image对象dataset[0][1] # 第一维是第几张图，第二维为1是label，为0是Image对象dataset[0][0] 12345678# 加上transformnormalize = T.Normalize(mean = [0.4, 0.4, 0.4], std = [0.2, 0.2, 0.2])transform = T.Compose([ T.RandomResizedCrop(224), T.RandomHorizontalFlip(), T.ToTensor(), normalize,]) 1dataset = ImageFolder('data/dogcat2/', transform = transform) 1dataset.class_to_idx 123# 深度学习中图片数据一般保存为CHW，即通道数 x 高 x 宽for img, index in dataset: print(img.size(), index) torch.Size([3, 224, 224]) 0 torch.Size([3, 224, 224]) 0 torch.Size([3, 224, 224]) 0 torch.Size([3, 224, 224]) 0 torch.Size([3, 224, 224]) 0 torch.Size([3, 224, 224]) 1 torch.Size([3, 224, 224]) 1 torch.Size([3, 224, 224]) 1 torch.Size([3, 224, 224]) 1 torch.Size([3, 224, 224]) 1123to_img = T.ToPILImage()# 0.2和0.4是标准差和均值的近似to_img(dataset[0][0]*0.2 + 0.4) Dataset只负责数据的抽象，一次调用__getattr__只返回一个样本。而训练神经网络是对一个batch的数据进行操作，同时还需要对数据进行shuffle和并行加速等。对此，pytorch提供了DataLoader帮助我们实现这些功能。DataLoader的函数定义如下。 1DataLoader(dataset, batch_size = 1, shuffle = False, sampler = None, num_workers = 0, collate_fn = default_collate, pin_memory = False, drop_last = False) dataset：加载的数据集（Dataset对象） batch_size：batch size（批大小） shuffle：是否将数据打乱 sampler：样本抽样，后续会详细介绍。 num_workers：使用多进程加载的进程数，0代表不使用多进程。 collate_fn：如何将多个样本数据拼接成一个batch，一般使用默认的拼接方式即可。 pin_memory：是否将数据保存在pin memory区，pin memory中的数据转到GPU会快一些。 drop_last：dataset中的数据个数可能不是batch_size的整数倍，drop_last为True会将多出来不足一个batch的数据丢失。 1from torch.utils.data import DataLoader 1dataloader = DataLoader(dataset, batch_size = 3, shuffle = True, num_workers = 0, drop_last = False) 123dataiter = iter(dataloader)imgs, labels = next(dataiter)imgs.size() torch.Size([3, 3, 224, 224])dataloader是一个可迭代对象，我们可以向使用迭代器一样使用它，利用： 12for batch_datas, batch_labels in dataloader: train() 或 12dataiter = iter(dataloader)batch_datas, batch_labels = next(dataiter) 在数据处理中，有时会出现某个样本无法读取等问题，例如某张图片损坏。这时__getitem__函数中将出现异常，此时最好的解决方案即是将出错的样本剔除。如果遇到这种情况实在无法处理，则可以返回None对象，然后在Dataloader中实现自定义的collate_fn，将空对象过滤掉。但要注意，在这种情况下dataloader返回的一个batch的样本数目会少于batch_size。对丢弃样本异常图片而言，这种做法会更好一些，因为它能保证每个batch样本的数目仍是batch_size。但在大多数情况下，最好的方式还是对数据进行彻底清除。 DataLoader里并没有太多的魔法方法，它封装了python的标准库multiprocessing使其能够实现多进程加速。在Dataset和DataLoader的使用方面有以下建议。 高负载的操作放在__getitem__中，如加载图片 dataset中应尽量只包含只读对象，避免修改任何可变对象。 第一点是因为多进程会并行地调用__getitem__函数，将负载高的放在__getitem__函数中能够实现并行加速。第二点是因为dataloader使用多进程加载，如果在Dataset中使用了可变对象，可能会有意想不到的冲突。在多线程/多进程中，修改一个可变对象需要加锁，但是dataloader的设计使得其很难加锁（在实际使用中也应尽量避免锁的存在）。如果一定要修改可变对象，建议使用python标准库queue使用python multiprocessing库的另一个问题是，在使用多进程时，如果主程序异常终止（比如用“Ctrl+C”快捷键强行退出），相应的数据加载进程可能无法正常退出。这时需要手动强行终止进程。 1ps x | grep &lt;cmdline&gt; | awk '&#123;print $1&#125;' | xargs kill ps x:获取当前用户的所有进程。 grep : 找到已经停止的pytorch程序的进程，例如你是通过python train.py启动的，那就需要些grep ‘python train.py’。 awk ‘{print $1}’：获取进程的pid xargs kill：终止进程，根据需要可能要写成xargs kill -9强制终止进程。 pytorch还单独提供一个sampler模块，用来对数据进行采样。常用的有随机采样器RandomSampler，当dataloader的shuffle参数为True时，就是调用的这个。这里介绍一个很有用的采样方法：WeightedRandomSampler，它会根据每个样本的权重选取数据，在样本比例不均衡的问题中，可用它进行重采样。 构建WeightedRandomSampler时需提供两个参数：每个样本的权重weights、共选取的样本总数num_samples，以及一个可选参数replacement。权重越大的样本被选中的概率越大，待选取的样本数量一般小于全部的样本数目。replacement用于指定是否可以重复选取某一个样本，默认为True，即允许在一个epoch中重复采样某一个数据。如果设为False，则当某一类样本被全部选取完，但其样本数目仍未达到num_samples时，sampler将不会从该类中选取数据，此时可能导致weights参数失效。 1!tree --charset ASCII data/dogcat/ data/dogcat/ |-- cat.0.jpg |-- cat.1.jpg |-- cat.2.jpg |-- cat.5.jpg |-- cat.6.jpg |-- dog.0.jpg |-- dog.1.jpg |-- dog.10.jpg |-- dog.1000.jpg `-- dog.10000.jpg 0 directories, 10 files123456dataset = DogCat('data/dogcat',transforms = transform)# 狗的图片被取出的概率是猫的概率的两倍# 两类图片被取出的概率与weights的绝对大小无关，只和比值有关weights = [2 if label == 1 else 1 for data, label in dataset]weights [1, 1, 2, 1, 1, 2, 2, 2, 2, 1]12345from torch.utils.data.sampler import WeightedRandomSamplersampler = WeightedRandomSampler(weights, num_samples=12, replacement=True)dataloader = DataLoader(dataset, batch_size = 3, sampler=sampler)for datas, labels in dataloader: print(labels.tolist()) [0, 0, 1] [0, 1, 1] [1, 1, 1] [0, 0, 0]一共只有10个样本，却返回了12个，说明样本被重复返回，这就是replacement参数的作用 12345# replacement改为Falsesampler = WeightedRandomSampler(weights, num_samples=10, replacement=False)dataloader = DataLoader(dataset, batch_size = 5, sampler=sampler)for datas, labels in dataloader: print(labels.tolist()) [0, 1, 0, 1, 1] [1, 1, 0, 0, 0]在这种情况下，num_samples等于dataset的样本总数，为了不重复选取，sampler会将每个样本都返回，这样就失去了weight参数的意义。 从上面的例子可见sampler在样本采样中的作用：如果指定了sampler，shuffle将不再生效，并且sampler.num_samples会覆盖dataset的实际大小，即一个epoch返回的图片总数取决于sampler.num_samples。 计算机视觉工具包：torchvision计算机视觉是深度学习中最重要的一类应用，为了方便研究者使用，pytorch团队专门开发一个视觉工具包torchvision，这个包独立于pytorch。 torchvision主要包含以下三部分。 model：提供深度学习中各种经典网络的网络结构及预训练好的模型，包括AlexNet、VGG系列、ResNet系列、Inception系列等。 datasets：提供常用的数据集加载，设计上都是继承torch.utils.data.Dataset，主要包括MNIST、CIFAR10/100、ImageNet、coco等。 transforms：提供常用的数据预处理操作，主要包括对Tensor及PIL Image对象的操作。 123456789from torchvision import modelsfrom torch import nn# 加载预训练好的模型，如果不存在会下载# 预训练好的模型保存在 ~/.torch/models/下面resnet34 = models.resnet34(pretrained = True,num_classes = 1000)# 修改最后的全连接层为10分类问题（默认是ImageNet上的1000分类）resnet34.fc = nn.Linear(512, 10) Downloading: &quot;https://download.pytorch.org/models/resnet34-333f7ec4.pth&quot; to /root/.cache/torch/checkpoints/resnet34-333f7ec4.pth 100%|██████████| 83.3M/83.3M [00:35&lt;00:00, 2.49MB/s]123456transform = T.Compose([ T.Resize(32), # 缩放图片（Image），保持长宽比不变，最短边为224像素 T.CenterCrop(32), # 从中间取出224 x224的图片 T.ToTensor(), # 将图片转换成Tensor并归一化至[0, 1] T.Normalize(mean = [.5], std = [.5,]) # 标准化至[-1, 1]]) 1234from torchvision import datasets# 指定数据集路径为data，如果数据集不存在则进行下载# 通过train = False获取测试集dataset = datasets.MNIST('data/',download=True, train = False, transform = transform) torchvision还提供了两个常用的函数。一个是make_grid，它能将多张图片拼接在一个网络中；另一个是save_img，它能将Tensor保存成图片。 1len(dataset) 1000012345dataloader = DataLoader(dataset, shuffle = True, batch_size = 16)from torchvision.utils import make_grid, save_imagedataiter = iter(dataloader)img = make_grid(next(dataiter)[0], 4) # 拼成4*4网络图片，且会转成3通道to_img(img) 123img2 = make_grid(next(dataiter)[0], 4) # 拼成4*4网络图片，且会转成3通道save_image(img2, 'a.png')Image.open('a.png') 可视化工具visdomvisdom是facebook专门为pytorch开发的一款可视化工具，开源于2017年3月。 visdom可以创造、组织和共享多种数据的可视化，包括数值、图像、文本，甚至是视频，支持pytorch、torch及numpy。用户可通过编程组织可视化空间或通过用户接口为数据打造仪表盘，检查试验结果和调试代码。visdom中有一下两个重要概念。 env：环境。不同环境的可视化结果相互隔离，互不影响，在使用时如果不指定env，默认使用main。不同用户。不同程序一般使用不同的env。 pane：窗格。窗格可用于可视化图像、数值或打印文本等，其可以拖动、缩放、保存和关闭。一个程序可使用同一个env中的不同pane，每个pane可视化或记录某一信息。“clear”按钮可以清空当前env的所有pane，“save”按钮可将当前env保存成json文件，保存路径位于~/.visdom/目录下。修改env的名字后单击fork，可将当前env另存为新文件。 通过命令pip install visdom即可完成visdom的安装。安装完成，须通过python -m visdom.server命令启动visdom服务，或通过nohup python -m visdom.server &amp;命令将服务放至后台运行。visdom服务是一个web server服务，默认绑定8097端口，客户端与服务器间通过tornado进行非阻塞交互。 使用visdom时有两点需要注意的地方。 需手动指定保存env，可在web界面单击save按钮或在程序中调用save方法，否则visdom服务重启后，env等信息会丢失 客户端与服务器之间的交互采用tornado异步框架，可视化操作不会阻塞当前程序，网络异常也不会导致程序退出。visdom以Plotly为基础。 12345import visdomvis = visdom.Visdom(env = u'test1')x = t.arange(1, 30, 0.01)y = t.sin(x)vis.line(X = x, Y = y, win = 'sinx',opts = &#123;'title':'y = sin(x)'&#125;) WARNING:root:Setting up a new session... &apos;sinx&apos;下面我们逐一分析这几行代码。 vis = visdom.Visdom(env = u’test1’)，用于构建一个客户端，客户端除了制定env外，还可以制定host、post等参数。 vis作为一个客户端对象，可以使用如下常见的画图函数。 line：类似MATLAB中的plot操作，用于记录某些标量的变化，例如损失、标准率等。 image：可视化图片，可以是输入的图片，也可以是GAN生成的图片，还可以是卷积核的信息。 text：用于记录日志等文字信息，支持HTML格式 histgram：可视化分布，主要是查看数据、参数的分布。 scatter：绘制散点图。 bar：绘制柱状图 pie：绘制饼状图 更多参考github主页 visdom同时支持pytorch的tensor和numpy的ndarray两种数据结构，但不支持python的int、float等类型。上述操作的参数一般不同，但有两个参数是绝大多数操作都具备。 win：用于指定pane的名字，如果不指定，visdom将自动分配一个新的pane。如果两次操作指定的win名字一样，新的操作将覆盖当前pane的内容，因此建议每次操作都指定win opts：用来可视化配置，接受一个字典，常见的option包括title、xlabel、ylabel、width等，主要用于设置pane的显示格式。 之前提到过，每次操作会覆盖之前的数据，但我们在训练网络的过程中往往需要不断更新数值，这时就需要指定参数update=’append’来避免覆盖之前的数值。 1234567891011121314import time# append追加数据for ii in range(0, 10): x = t.Tensor([ii]) y = 2 * x time.sleep(0.5) vis.line(X = x, Y= y, win = 'polynomial',name = 'Trace1', update = 'append' if ii &gt; 0 else None) # 新增一条线for ii in np.linspace(0, 5, 50): x = t.Tensor([ii]) y = x ** 2 time.sleep(0.1) vis.line(X = x, Y= y, win = 'polynomial',name = 'Trace2', update = 'append') images的画图功能可分为如下两类。 image接受一个二维或三维向量，HW 或 3H*W ，前者是黑白图像，后者是彩色图像。 images接受一个四维向量NCH*W，C可以是1或3，分别代表黑白和彩色图像。可实现类似torchvision中make_grid的功能，将多张图片拼接在一起。images也可以接受一个二维或三维的向量，此时它所实现的功能与image一致。 12345678# 可视化一张随机的黑白照片vis.image(t.randn(64, 64).numpy())# 可视化一张随机的彩色图片vis.image(t.randn(3, 64, 64).numpy(), win = 'random2')# 可视化36张随机的彩色图片，每行6张vis.images(t.randn(36, 3, 64, 64).numpy(), nrow = 6, win = 'random3',opts = &#123;'title':'random_imgs'&#125;) &apos;random3&apos;12345x = dataset[0][0].unsqueeze(0)for i in range(1,36): x = t.cat((x, dataset[i][0].unsqueeze(0)), dim = 0)vis.images(x, nrow = 6, win = 'MNIST',opts = &#123;'title':'MNIST数据集'&#125;) &apos;MNIST&apos;vis.text用于可视化文本，它支持所有的html标签，同时也遵循着html的语法标签。 1vis.text(u'''&lt;h1&gt;Hello visdom&lt;/h1&gt;&lt;br&gt;visdom是Facebook专门为&lt;b&gt;pytorch&lt;b/&gt;开发一个可视化工具，''',win = 'visdom',opts = &#123;'title':u'visdom简介'&#125;) &apos;visdom&apos;1vis.text('ss', win = 'visdom', append = True, opts = &#123;'title':'平方'&#125;) &apos;visdom&apos;12345# 文本更新vis.text('&lt;b&gt;平方:&lt;/b&gt;', win = 'pingfang',opts = &#123;'title':'pingfang'&#125;)for i in range(1, 20): time.sleep(0.5) vis.text('&lt;b&gt;[info]&#123;&#125;^2=&#123;&#125;&lt;/b&gt;'.format(i, i **2), append = True,win = 'pingfang') GPU略 持久化在pytorch中，以下对象可以持久化到硬盘，并能通过相应的方法加载到内存中。 Tensor Variable nn.Module Optimizer 本质上，上述信息最终都是保存成Tensor。Tensor的保存和加载十分简单，使用t.save和t.load即可完成相应的功能。在save/load时可指定使用pickle模块，在load时还可将GPU tensor映射到CPU或其他GPU上。 我们可以通过t.save(obj, file_name)等方法保存任意可序列化的对象，然后通过obj = t.load(file_name)方法加载保存的数据。 对Module和Optimizer对象，这里建议保存对应的state_dict，而不是直接保存整个Module/Optimizer对象。Optimizer对象保存的是参数即动量信息，通过加载之前的动量信息，能够有效减少模型震荡。 12345678910111213a = t.Tensor(3, 4)if t.cuda.is_available(): a = a.cuda(1)# 把a转为GPU1上的tensor t.save(a, 'a.pth') #加载为b，存储于GPU1上（因为保存时就在GPU1） b = t.load('a.pth') # 加载为c，存储于CPU c = t.load('a.pth', map_location = lambda storage, loc:storage) # 加载为d，存储于GPU0 d = t.load('a.pth', map_location = &#123;'cuda:1':'cuda:0'&#125;) 123from torchvision.models import resnet34model = resnet34() 12# model的state_dict是一个字典list(model.state_dict().keys())[:8] [&apos;conv1.weight&apos;, &apos;bn1.weight&apos;, &apos;bn1.bias&apos;, &apos;bn1.running_mean&apos;, &apos;bn1.running_var&apos;, &apos;bn1.num_batches_tracked&apos;, &apos;layer1.0.conv1.weight&apos;, &apos;layer1.0.bn1.weight&apos;]123# module对象的保存与加载t.save(model.state_dict(), 'resnet32.pth')model.load_state_dict(t.load('resnet32.pth')) &lt;All keys matched successfully&gt;1optimizer = t.optim.Adam(model.parameters(), lr = 0.1) 12t.save(optimizer.state_dict(),'optimizer.pth')optimizer.load_state_dict(t.load('optimizer.pth')) 123456all_data = dict( optimizer = optimizer.state_dict(), model = model.state_dict(), info = u'模型和优化器的所有参数')t.save(all_data, 'all_data.pth') 12all_data = t.load('all_data.pth')all_data.keys() dict_keys([&apos;optimizer&apos;, &apos;model&apos;, &apos;info&apos;])]]></content>
      <categories>
        <category>笔记</category>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【notes】pytorch学习笔记3-神经网络工具箱nn]]></title>
    <url>%2F2019%2F08%2F22%2Fnn.Module%2F</url>
    <content type="text"><![CDATA[神经网络工具箱nnautograd实现了自动微分系统，然而对于深度学习来说过于底层。nn模块是构建与autograd之上的神经网络模块。除了nn之外，我们还会介绍神经网络中常用的工具，比如优化器optim,初始化init等。 nn.Moduletorch.nn的核心数据结构是Module，它是一个抽象的概念，既可以表示神经网络中的某个层（layer），也可以表示一个包含很多层的神经网络。在实际使用中，最常见的做法是继承nn.Module，撰写自己的网络、层。 全连接层，又名仿射层，输出$\boldsymbol{x}$和输入$\boldsymbol{x}$满足$\boldsymbol{y=Wx+b}$，$\boldsymbol{W}$ 和 $\boldsymbol{b}$ 是可学习函数。 123import torch as tfrom torch import nnfrom torch.autograd import Variable as V 12345678910class Linear(nn.Module): def __init__(self, in_features, out_features): super(Linear, self).__init__() #等价于nn.Module.__init__(self) self.w = nn.Parameter(t.randn(in_features, out_features)) self.b = nn.Parameter(t.randn(out_features)) def forward(self, x): x = x.mm(self.w) return x + self.b.expand_as(x) 1234layer = Linear(4, 3)input = V(t.randn(2, 4))output = layer(input)output tensor([[ 2.0269, 5.1465, 1.5603], [-0.6868, -0.8096, -0.6427]], grad_fn=&lt;AddBackward0&gt;)123for name, parameter in layer.named_parameters(): print(name, parameter) w Parameter containing: tensor([[-0.0121, -0.2593, -0.5310], [ 0.2982, -0.2846, -0.0437], [ 0.6220, 1.7351, 0.8025], [ 1.0544, 2.3325, 0.6561]], requires_grad=True) b Parameter containing: tensor([0.2586, 2.3734, 0.5372], requires_grad=True)但需要注意一下几点： 自定义层Linear必须继承nn.Module，并且在其构造函数中需调用nn.Module的构造函数，即super(Linear, self).init()或nn.Module.__init__(self)。 在构造函数__init__中必须自己定义可学习的参数，并封装成Parameter，如在本例中我们把w和b封装成Parameter。Parameter是一种特殊的Variable，但其默认需要求导（requires_grad=True）。 forward函数实现前向传播过程，其输入可以是一个或多个variable，对x的任何操作也必须是variable支持的操作。 无需写反向传播函数，因其前向传播都是对variable进行操作，nn.Module能够利用autograd自动实现反向传播，这一点比Function简单许多。 使用时，直观上可将layer看成数学概念中的函数，调用layer(input)即可得到input对应的结果。它等价于layers.__call__(input)，在__call__函数中，主要调用的是layer.forward(X)，另外还对钩子做了一些处理。所以在实际使用中应尽量使用layer(x)而不是使用layer.forward(x)。 Module能够自动检测到自己的parameter，并将其作为学习参数。除了parameter，Module还包含子Module，主Module能够递归查找子Module中的parameter。 下面看一个稍微复杂的网络：多层感知机。 123456789class Perceptron(nn.Module): def __init__(self, in_features, hidden_features, out_features): nn.Module.__init__(self) self.layer1 = Linear(in_features, hidden_features) self.layer2 = Linear(hidden_features, out_features) def forward(self, x): x = self.layer1(x) x = t.sigmoid(x) return self.layer2(x) 123perceptron = Perceptron(3, 4, 1)for name, param in perceptron.named_parameters(): print(name, param.size()) layer1.w torch.Size([3, 4]) layer1.b torch.Size([4]) layer2.w torch.Size([4, 1]) layer2.b torch.Size([1])注意一下两个知识点。 构造函数__init__中，可利用前面自定义的Linear层（module）作为当前module对象的一个字module，它的可学习参数，也会成为当前module的可学习参数。 在前向传播函数中，我们有意识地将输出变量都命名为x，是为了能让python回收一些中间层的输出，从而节省内存。但并不是所有的中间结果都会被回收，有些variable虽然名字被覆盖，但其在反向传播时仍需要用到，此时python的内存回收模块将通过检查引用计数，不会回收这一部分内存。 module中parameter的全局命名规范如下。 parameter直接命名。例如self.param_name = nn.Parameter(t.randn(3,4))，命名为param_name。 子module中的parameter，会在其名字之前加上当前module的名字，就是sub_module.param_name。 为了方便用户使用，pytorch实现了神经网络中绝大多数的layer，这些layer都继承了nn.Module，封装了可学习参数parameter，并实现了forward函数，且专门针对GPU运算进行了CuDNN优化。具体内容可参考官方文档或在IPython/Jupyter中使用nn.layer。 阅读文档注意： 构造函数的参数，如nn.Linear(in_features, out_features, bias)，需关注着三个参数的作用 属性、可学习参数和子module。如nn.Linear中有weight和bias两个可学习参数，不包含子module 输入输出的形状，如nn.Linear的输入形状是(N, input_features)，输出为(N, output_features)，N是batch_size。若想输入一个数据需要调用unsqueeze(0)函数将数据伪装成batch_size = 1的batch 常用的神经网络层图像相关层图像相关层主要包括卷积层（Conv）、池化层（Pool）等，这些层在实际使用中可分为一维、二维和三维，池化层又分为平均池化（AvgPool）、最大值池化（MaxPool）、自适应池化（AdaptiveAvgPool）等。卷积层除了常用的前向卷积外，还有逆卷积（TransposeConv）。 123456789import torch as tfrom torch import nnfrom torch.autograd import Variable as Vfrom PIL import Imagefrom torchvision.transforms import ToTensor, ToPILImageto_tensor = ToTensor()to_pil = ToPILImage()curry = Image.open('curry')curry 1234567891011# 输入一个batch，batch_size = 1input = to_tensor(curry).unsqueeze(0)# 锐化卷积核kernel = t.ones(3, 3) / -9kernel[1][1] = 1conv = nn.Conv2d(1, 1, (3, 3), 1, bias = False)conv.weight.data = kernel.view(1, 1, 3, 3)out = conv(V(input))to_pil(out.data.squeeze(0)) Shape: Input: $(N, C_{in}, H_{in}, W_{in})$ Output: $(N, C_{out}, H_{out}, W_{out})$ where$H_{out} = \left\lfloor\frac{H_{in} + 2 \times \text{padding}[0] - \text{dilation}[0]\times (\text{kernel_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor$ $W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[1] - \text{dilation}[1]\times (\text{kernel_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor$ 图像的卷积操作还有各种变体，有关各种变体的介绍可以参照此处的介绍https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md 池化层可以看作是一种特殊的卷积层，用来下采样。但池化层没有可学习参数，其weight是固定的。 12pool = nn.AvgPool2d(2, 2)list(pool.parameters()) []12out = pool(V(input))to_pil(out.data.squeeze(0)) 除了卷积层和池化层，深度学习中还将常用到一下几个层 Linear：全连接层 BatchNorm：批规范化层，分为1D、2D和3D。除了标准的BatchNorm之外，还有在风格迁移中常用到的InstanceNorm层。 Dropout：dropout层，用来防止过拟合，同样分为1D、2D和3D。 1234# 输入batch_size=2，维度3input = V(t.randn(2, 3))linear = nn.Linear(3, 4)h = linear(input);h tensor([[-0.4360, 0.3433, -0.1978, -0.3128], [-0.9655, 0.6278, 0.2510, 0.1256]], grad_fn=&lt;AddmmBackward&gt;)12345678910# 4 channel,初始化标准差为4，均值为0bn = nn.BatchNorm1d(4)bn.weight.data = t.ones(4) * 4bn.bias.data = t.zeros(4)bn_out = bn(h)# 注意输出的均值和方差# 方差是标准差的平方，计算无偏方差分母会减1# 使用unbiased=False，分母不减1bn_out.mean(0), bn_out.var(0, unbiased = False) (tensor([-1.1921e-07, 0.0000e+00, 0.0000e+00, 0.0000e+00], grad_fn=&lt;MeanBackward1&gt;), tensor([15.9977, 15.9921, 15.9968, 15.9967], grad_fn=&lt;VarBackward1&gt;))123456# 每个元素以0.5的概率舍弃dropout = nn.Dropout(0.5)o = dropout(bn_out)# 有一半的概率会变成0o tensor([[7.9994, -0.0000, -0.0000, -0.0000], [-0.0000, 7.9980, 7.9992, 7.9992]], grad_fn=&lt;MulBackward0&gt;)以上很多例子中都对module的属性直接操作，其大多数是可学习参数，一般会随着学习的进行而不断改变。实际使用中除非需要使用特殊的初始化，否则尽量不要直接改变参数。 激活函数pytorch实现了常见的激活函数。其他具体的接口信息可参见官方文档，这些激活函数可以作为独立的layer使用。这里介绍最常用的激活函数ReLU，其数学表达式为：$$ReLU(x)=max(0,x)$$ 123456relu = nn.ReLU(inplace = True)input = V(t.randn(2, 3))print(input)output = relu(input)print(output) # 小于0的都被截断为0id(input) == id(output) tensor([[ 0.5049, 0.6093, -0.1565], [-0.9114, -0.9594, 1.0539]]) tensor([[0.5049, 0.6093, 0.0000], [0.0000, 0.0000, 1.0539]]) TrueReLU函数有个inplace参数，如果设为True，如果设为True，它会把输出直接覆盖到输入中，这样可以节省内存/显存。之所以可以覆盖是因为在计算ReLU的反向传播时，只需根据输出就能够推算出反向传播的梯度。但是只有少数的autograd操作支持inplace操作（如variable.sigmoid_()），除非你明确知道自己在做什么，否则一般不要使用inplace操作。 在以上例子里，都是将每一层的输出直接作为下一层的输入，这种网络成为前馈传播网络。对于此种网络，如果每次都写复杂的forward函数会有些麻烦，在此就有两种简化方式，ModuleList和Sequential。其中Sequential是一个特殊的Module，它包含几个子module，前向传播时会将输入一层接一层第传递下去。ModuleList也是一个特殊的Module，可以包含几个子Module，可以像用list一样使用它，但不能直接把输入传给ModuleList。 12345678910111213141516171819202122# Sequential的三种写法net1 = nn.Sequential()net1.add_module('conv', nn.Conv2d(3, 3, 3)) # 输入为(N, C_&#123;in&#125;, H_&#123;in&#125;, W_&#123;in&#125;)，参数为net1.add_module('batchnorm', nn.BatchNorm2d(3)) # 3为(N, C, H, W)中的Cnet1.add_module('activation_layer', nn.ReLU())net2 = nn.Sequential( nn.Conv2d(3, 3, 3), nn.BatchNorm2d(3), nn.ReLU() )from collections import OrderedDictnet3 = nn.Sequential(OrderedDict([ ('conv1', nn.Conv2d(3, 3, 3)), ('bn1', nn.BatchNorm2d(3)), ('relu1', nn.ReLU()),]))print('net1', net1)print('net2', net2)print('net3', net3) net1 Sequential( (conv): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1)) (batchnorm): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (activation_layer): ReLU() ) net2 Sequential( (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1)) (1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) net3 Sequential( (conv1): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1)) (bn1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU() )12# 可根据名字或序号取出子modulenet1.conv, net2[0], net3.conv1 (Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1)), Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1)), Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1)))12input = V(t.rand(1, 3, 4, 4))net1(input), net2(input), net3(input), net3.relu1(net2[1](net1.conv(input))) (tensor([[[[0.0000, 1.4727], [0.1600, 0.0000]], [[0.0000, 0.0000], [0.7015, 1.1069]], [[1.7189, 0.0000], [0.0000, 0.0000]]]], grad_fn=&lt;ReluBackward0&gt;), tensor([[[[0.0000, 1.6957], [0.0000, 0.0000]], [[1.2454, 0.6350], [0.0000, 0.0000]], [[1.0204, 0.4811], [0.1430, 0.0000]]]], grad_fn=&lt;ReluBackward0&gt;), tensor([[[[0.0000, 1.5585], [0.1751, 0.0000]], [[0.0000, 1.4177], [0.1846, 0.0000]], [[0.0000, 0.0000], [1.3537, 0.2417]]]], grad_fn=&lt;ReluBackward0&gt;), tensor([[[[0.0000, 1.4727], [0.1600, 0.0000]], [[0.0000, 0.0000], [0.7015, 1.1069]], [[1.7189, 0.0000], [0.0000, 0.0000]]]], grad_fn=&lt;ReluBackward0&gt;))1234567modulelist = nn.ModuleList([nn.Linear(3, 4), nn.ReLU(), nn.Linear(4, 2)])input = V(t.rand(1, 3))for model in modulelist: input = model(input)# 下面会报错，因为modellist没有实现forward方法# output = modellist(input) 为何不直接使用python中自带的list，而非要多次一举呢？这是因为ModuleList是Module的子类，当在Module中使用它时，就能自动识别为子module。 1234567891011class MyModule(nn.Module): def __init__(self): super(MyModule, self).__init__() self.list = [nn.Linear(3,4), nn.ReLU()] # 直接用list self.module_list = nn.ModuleList([nn.Conv2d(3, 3, 3), nn.ReLU()]) # 用nn.ModuleList def forward(self): pass model = MyModule()model MyModule( (module_list): ModuleList( (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1)) (1): ReLU() ) )123for name, param in model.named_parameters(): print(name, param.size()) module_list.0.weight torch.Size([3, 3, 3, 3]) module_list.0.bias torch.Size([3])可见，list中的子module并不能被主module识别，而ModuleList中的子module能够被主module识别。 除ModuleList之外还有ParameterList，它是一个可以包含多个parameter的类list对象。在实际应用中，使用方式和ModuleList类似。在构造函数__init__中用到list、tuple、dict等对象，一定要思考是否应该用ModuleList或ParameterList代替。 损失函数在深度学习中会用到各种各样的损失函数，这些损失函数可看作是一种特殊的layer，pytorch也将这些损失函数实现为nn.Module的子类。然而在实际使用中通常将这些损失函数专门提取出来，作为独立的一部分。 123456789#batch_size = 3, 计算对应每个类别的分数score = V(t.randn(3, 10)) # (N, C) N是batch_size，C是class的个数# 三个样本分别属于1， 0， 1类，label必须是LongTensorlabel = V(t.Tensor([1, 0, 9])).long()# loss与普通的layer无差异criterion = nn.CrossEntropyLoss()loss = criterion(score, label)loss tensor(2.8392)优化器pytorch将深度学习中常用的优化方法全部封装到torch.optim中，其设计十分灵活，能够很方便地扩展自定义的优化方法。所有的优化方法都是继承基类optim.Optimizer，并实现了自己的优化步骤。下面就以最基本的优化方法————随机梯度下降法（SGD）举例说明。这里需要重点掌握： 优化方法的基本使用方法 如何对模型的不同部分设置不同的学习率 如何调整学习率 12345678910111213141516171819202122232425262728# 首先定义一个LeNet网络class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.features = nn.Sequential( nn.Conv2d(3, 6, 5), nn.ReLU(), nn.MaxPool2d(2, 2), nn.Conv2d(6, 16, 5), nn.ReLU(), nn.MaxPool2d(2, 2) ) self.classifier = nn.Sequential( nn.Linear(16 * 5 * 5, 120), nn.ReLU(), nn.Linear(120, 84), nn.ReLU(), nn.Linear(84, 10), ) def forward(self, x): x = self.features(x) x = x.view(-1, 16 * 5 * 5) x = self.classifier(x) return xnet = Net() 123456789from torch import optimoptimizer = optim.SGD(params = net.parameters(), lr = 1)optimizer.zero_grad() # 梯度清零，等价于net.zero_grad()input = V(t.randn(1, 3, 32, 32))output = net(input)output.backward(output) # fake backwardoptimizer.step() # 执行优化 123# 为不同子网络参数设置不同的学习率，在finetune中经常用到# 如果对某个参数不指定学习率，就是用默认学习率optimizer = optim.SGD([&#123;'params':net.features.parameters()&#125;,&#123;'params':net.classifier.parameters(), 'lr':1e-2&#125;], lr = 1e-5) 123456789# 只为两个全连接层设置较大的学习率，其余层的学习率较小special_layers = nn.ModuleList([net.classifier[0],net.classifier[2]])special_layers_params = list(map(id, special_layers.parameters()))base_params = filter(lambda p: id(p) not in special_layers_params, net.parameters())optimizer = t.optim.SGD([ &#123;'params':base_params&#125;, &#123;'params':special_layers.parameters(), 'lr': 0.01&#125;], lr = 0.001) 调整学习率主要有两种做法。一种是修改optimmizer.param_groups中对应的学习率。另一种是新建优化器（更简单也是更推荐的做法），由于optimizer十分轻量级，构建开销很小，故可以构建新的optimizer。但是新建优化器会重新初始化动量等状态信息，这对使用动量的优化器来说（如带momentum的sgd），可能会造成损失函数在收敛过程中出震荡。 123456# 调整学习率，新建一个optimizerold_lr = 0.1optimizer = optim.SGD([ &#123;'params': net.features.parameters()&#125;, &#123;'params': net.classifier.parameters(), 'lr':old_lr *0.1&#125;],lr = 1e-5) nn.functionalnn中还有一个常用的模块：nn.functional。nn中的大多数layer在functional中都有一个与之相对应的函数。nn.functional中的函数和nn.Module的主要区别在于，用nn.Module实现的layers是一个特殊的类，都是由class Layer(nn.Module)定义，会自动提取可学习的参数；而nn.functional中的函数更像是纯函数，由def function(input)定义。 12345input = V(t.randn(2, 3))model = nn.Linear(3, 4)output1 = model(input)output2 = nn.functional.linear(input, model.weight, model.bias)output1 == output2 tensor([[True, True, True, True], [True, True, True, True]])123b = nn.functional.relu(input)b2 = nn.ReLU()(input)b == b2 tensor([[True, True, True], [True, True, True]])应该什么时候使用nn.Module，什么时候使用nn.functional？如果模型有可学习的参数，最好用nn.Module，否则既可以使用nn.functional也可以使用nn.Module，二者在性能上没有太大差异，具体的使用方式取决于个人喜好。但建议还是使用nn.Dropout而不是nn.functional.dropout，因为dropout在训练和测试两个阶段的行为有所差别，使用nn.Module对象能够通过model.eval操作以一区分。 1234567891011121314151617from torch.nn import functional as Fclass Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.conv2 = nn.Conv2d(6, 16, 5) self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): x = F.max_pool2d(F.relu(self.conv1(x)), 2) x = F.max_pool2d(F.relu(self.conv2(x)), 2) x = x.view(-1, 16 * 5 * 5) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x 123net = Net()x = t.randn(1, 3, 32, 32)net(x) tensor([[-0.0539, 0.0145, 0.0214, 0.0474, -0.0516, 0.0890, 0.0539, 0.0805, 0.0785, -0.1043]], grad_fn=&lt;AddmmBackward&gt;)不具备可学习参数的层（激活层、池化层等），将它们用函数代替，这样可以不用放置在构造函数__init__中。有可学习参数的模块，也可以用functional代替，只不过实现起来较繁琐，需要手动定义参数parameter。 123456789class MyLinear(nn.Module): def __init__(self): super(MyLinear, self).__init__() self.weight = nn.Parameter(t.randn(3, 4)) self.bias = nn.Parameter(t.zeros(3)) def forward(self,input): return F.linear(input, self.weight, self.bias) 123x = t.randn(1, 4)linear = MyLinear()linear(x) tensor([[-0.0678, 2.5530, 0.8512]], grad_fn=&lt;AddmmBackward&gt;)初始化策略在深度学习中参数的初始化十分重要，良好的初始化能使模型收敛更快，并达到更高水平，而糟糕的初始化可能使模型迅速崩溃。pytorch中nn.Module的模块参数都采取了较合理的初始化策略，因此一般不用我们考虑。当然我们可以用自定义的初始化代替系统的默认初始化。自定义初始化尤为重要，因为t.Tensor()返回的是内存中的随机数，很可能会有极大值，这在实际训练网络中会造成溢出或者梯度消失。pytorch中的nn.init模块专门为初始化设计，实现了常用的初始化策略。如果某种初始化策略nn.init不提供，用户也可以自己直接初始化。 123456# 利用nn.init初始化from torch.nn import initlinear = nn.Linear(3, 4)t.manual_seed(1)# 等价于linear.weight.data.normal_(0, std)init.xavier_normal_(linear.weight) Parameter containing: tensor([[ 0.3535, 0.1427, 0.0330], [ 0.3321, -0.2416, -0.0888], [-0.8140, 0.2040, -0.5493], [-0.3010, -0.4769, -0.0311]], requires_grad=True)12345import matht.manual_seed(1)std = math.sqrt(2)/math.sqrt(7.)linear.weight.data.normal_(0, std) tensor([[ 0.3535, 0.1427, 0.0330], [ 0.3321, -0.2416, -0.0888], [-0.8140, 0.2040, -0.5493], [-0.3010, -0.4769, -0.0311]])12345678910# 对模型的所有参数进行初始化for name, params in net.named_parameters(): if name.find('linear') != -1: print(params[0]) # weight print(params[1]) # bias elif name.find('conv') != -1: pass elif name.find('norm') != -1: pass nn.Module深入分析如果想深入地理解nn.Module，研究其原理是很有必要的。首先来看看nn.Module基类的构造函数的源代码： 1234567def __init__(self): self._parameters = OrderedDict() self._modules = OrderedDict() self._buffers = OrderedDict() self._backward_hooks = OrderedDict() self._forward_hooks = OrderedDict() self.training = True 其中每个属性的解释如下： _parameters：字典。保存用户直接设置的parameter，self.param1 = nn.Parameter(t.randn(3, 3))会被检测到，在字典中加入一个key为param，value为对应parameter的item，而self.submodule = nn.Linear(3, 4)中的parameter则不会存于此。 _modules：子module。通过self.submodule = nn.Linear(3, 4)指定的子module会保存于此。 _buffers：缓存。如batchnorm使用momentum机制，每次前向传播需用到上一次前向传播的结果。 _backward_hooks与_forward_hooks：钩子技术，用来提取中间变量，类似variable的hook training：BatchNorm与Dropout层在训练阶段和测试阶段中采取的策略不同，通过判断training值决定前向传播策略。 上述几个属性中，_parameters、_modules和_buffers这三个字典中的键值，都可以通过self.key方式获得，效果等价于self._parameters[‘key’] 123456789101112class Net(nn.Module): def __init__(self): super(Net, self).__init__() # 等价于self.register_parameter('param1', nn.Parameter(t.randn(3, 3))) self.param1 = nn.Parameter(t.rand(3, 3)) self.submodel1 = nn.Linear(3, 4) def foward(self, input): x = self.param1 * input x = self.submodel1(x) return xnet = Net()net Net( (submodel1): Linear(in_features=3, out_features=4, bias=True) )1net._modules OrderedDict([(&apos;submodel1&apos;, Linear(in_features=3, out_features=4, bias=True))])1net._parameters OrderedDict([(&apos;param1&apos;, Parameter containing: tensor([[0.3398, 0.5239, 0.7981], [0.7718, 0.0112, 0.8100], [0.6397, 0.9743, 0.8300]], requires_grad=True))])1net.param1 == net._parameters['param1'] tensor([[True, True, True], [True, True, True], [True, True, True]])12for name, param in net.named_parameters(): print(name, param.size()) param1 torch.Size([3, 3]) submodel1.weight torch.Size([4, 3]) submodel1.bias torch.Size([4])12for name, submodel in net.named_modules(): print(name, submodel) Net( (submodel1): Linear(in_features=3, out_features=4, bias=True) ) submodel1 Linear(in_features=3, out_features=4, bias=True)1234bn = nn.BatchNorm1d(2)input = V(t.rand(3,2), requires_grad = True)output = bn(input)bn._buffers OrderedDict([(&apos;running_mean&apos;, tensor([0.0362, 0.0596])), (&apos;running_var&apos;, tensor([0.9009, 0.9262])), (&apos;num_batches_tracked&apos;, tensor(1))])nn.Module在实际使用中可能层层嵌套，一个module包含若干个子module，每一个子module又包含了更多的子module。为了方便用户访问各个子module，nn.Module实现了很多方法，如函数children可以查看直接子module，函数modules可以查看所有的子module（包括当前module）。与之相对应的还有函数named_children和named_modules，其能够在返回module列表的同时返回它们的名字。 123input = V(t.arange(0, 12).view(3, 4).float())model = nn.Dropout()model(input) tensor([[ 0., 0., 0., 0.], [ 0., 0., 12., 0.], [ 0., 18., 0., 0.]])12model.training = Falsemodel(input) tensor([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.]])对batchnorm、dropout、instancenorm等在训练和测试阶段行为差距较大的层，如果在测试时不将其training值设为False，则可能会有很大影响，这在实际使用中千万注意。虽然可通过直接设置training属性将子module设为train和eval模式，但是这种方式比较繁琐。推荐的做法是调用model.train()函数，它会将当前module及其子module中的所有training属性都设为True。model.eval()函数会把training属性都设为False。 123print(net.training, net.submodel1.training)net.eval()net.training, net.submodel1.training True True (False, False)1list(net.named_modules()) [(&apos;&apos;, Net( (submodel1): Linear(in_features=3, out_features=4, bias=True) )), (&apos;submodel1&apos;, Linear(in_features=3, out_features=4, bias=True))]register_forward_hook和register_backward_hook函数的功能类似于variable的register_hook，可在module前向传播或反向传播时注册钩子。每次前向传播执行结束后会执行钩子函数（hook）。前向传播的钩子函数具有如下形式：hook(module, input, output) -&gt; None，而反向传播则具有如下形式：hook(module, grad_input, grad_ouput) -&gt; Tensor or None。钩子函数不应修改输入和输出，并且在使用后应及时删除，以避免每次都运行钩子增加运行负载。钩子函数主要用在获取某些中间结果的情景，如中间某一层的输出或某一层的梯度。这些结果本应写在forward函数中，但如果在forward函数中加上这些处理，可能会使处理逻辑比较复杂，这时使用钩子技术就更合适。下面考虑一种场景：有一个预训练的模型，需要提取模型的某一层（不是最后一层）的输出作为特征进行分类，希望不修改其原有的模型定义文件，这时就可以利用钩子函数。 123456789model = VGG()features = t.Tensor()def hook(module, input, output): features.copy_(output.data) handle = model.layer8.register_forward_hook(hook)_ = model(input)# 用完hook后删除handle.remove() nn.Module对象在构造函数中的行为看起来有些诡异，想要理解就需要看两个魔法方法__getattr__和__setattr__。在python中有两个常用的builtin方法：getattr和setattr。getattr(obj, ‘attr1’)等价于obj.attr，setattr(obj, ‘name’, value)等价于obj.name = value。 result = obj.name会调用builtin函数getattr(obj, ‘name’)，如果该属性找不到，会调用obj.__getattr__(‘name’) obj.name = value会调用builtin函数setattr(obj, ‘name’, value)，如果obj对象实现了__setattr__方法，setattr会直接调用obj.__setattr__(‘name’, value)。 1234567891011class person(): dict = &#123;'name':'xxx','sex':'boy','age':18&#125; def __getattr__(self,name): return self.dict[name] def __setattr__(self, name , value): self.dict[name] = value return value 12one = person()one.name, one.sex, one.age (&apos;xxx&apos;, &apos;boy&apos;, 18)1one.name = '吴彦祖';one.name &apos;吴彦祖&apos;nn.Module实现了自定义的__setattr__函数，当执行module.name=value时，会在__setattr__中判断value是否为Parameter或nn.Module对象，如果是则将这些对象加到_parameters和_modules两个字典中；如果是其他类型的对象，如Variable、list、dict等，则调用默认的操作，将这个值保存在__dict__中。 123module = nn.Module()module.param = nn.Parameter(t.ones(2, 2))module._parameters,module.param (OrderedDict([(&apos;param&apos;, Parameter containing: tensor([[1., 1.], [1., 1.]], requires_grad=True))]), Parameter containing: tensor([[1., 1.], [1., 1.]], requires_grad=True))1234567submodule1 = nn.Linear(2, 2)submodule2 = nn.Linear(2, 2)module_list = [submodule1, submodule2]# 对于list对象，调用builtin函数，保存在__dict__中module.submodules = module_listprint('_modules:',module._modules)print("__dict__['submodules']:",module.__dict__.get('submodules')) _modules: OrderedDict() __dict__[&apos;submodules&apos;]: [Linear(in_features=2, out_features=2, bias=True), Linear(in_features=2, out_features=2, bias=True)]123module.submodules = nn.ModuleList(module_list)print('_modules:',module._modules)print("__dict__['submodules']:",module.__dict__.get('submodules')) _modules: OrderedDict([(&apos;submodules&apos;, ModuleList( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ))]) __dict__[&apos;submodules&apos;]: None因_modules和_parameters中的item未保存在__dict__中，所以默认的getattr方法无法获取它，因而nn.Module实现了自定义的__getattr__方法。如果默认的getattr无法处理，就调用自定义的__getattr__方法，尝试从_modules、_parameters和_buffers三个字典中获取。 123getattr(module, 'training') # 等价于module.training#error#module.__getattr__('training') True1234module.attr1 = 2getattr(module, 'attr1')# 报错# module.__getattr__('attr1') 21getattr(module, 'param') Parameter containing: tensor([[1., 1.], [1., 1.]], requires_grad=True)在pytorch中保存模型十分简单，所有的module对象都具有state_dict()函数，返回当前Module所有的状态数据。将这些状态数据保存后，下次使用模型时即可利用model.load_state_dict()函数将状态加载进来。优化器（optimizer）也有类似机制，不过一般并不需要保存优化器的运行状态。 123456# 保存模型t.save(net.state_dict(), 'net.pth')# 加载已保存的模型net2 = Net()net2.load_state_dict(t.load('net.pth')) &lt;All keys matched successfully&gt;123t.save(net, 'net_all.pth')net2 = t.load('net_all.pth')net2 /usr/local/lib/python3.6/dist-packages/torch/serialization.py:256: UserWarning: Couldn&apos;t retrieve source code for container of type Net. It won&apos;t be checked for correctness upon loading. &quot;type &quot; + obj.__name__ + &quot;. It won&apos;t be checked &quot; Net( (submodel1): Linear(in_features=3, out_features=4, bias=True) )将Module放在GPU上运行也十分简单，只需一下两步。 model = model.cuda():将模型的所有参数转存到GPU input.cuda():将输入数据放置到GPU上。 至于如何在多个GPU上并行计算，pytorch也提供了两个函数，可实现简单高效的并行GPU计算。 nn.parallel.data_parallel(module, inputs, device_ids = None, output_device = None, dim = 0, module_kwargs = None) class torch.nn.DataParallel(module, device_ids = None, output_device = None, dim = 0) 可见二者的参数十分相似，通过device_ids参数可以指定在哪些GPU上进行优化，output_device指定输出到哪个GPU上。唯一的不同在于前者直接利用多GPU并行计算得出结果，后者则返回一个新的module，能够自动在多GPU上进行并行加速。 123456# method1new_net = nn.DataParallel(net, device_ids = [0, 1])output = new_net(input)# method 2output = nn.parallel.data_parallel(net, input, device_ids = [0, 1]) DataParallel并行的方式，是将输入一个batch的数据均分成多份，分别送到对应的GPU进行计算，然后将各个GPU得到的梯度相加。与Module相关的所有数据也会以浅复制的方式复制多份。 nn和aautograd的关系nn.Module利用的是autograd技术，其主要工作是实现前向传播。在forward函数中，nn.Module对输入的Variable进行的各种操作，本质上都用了autograd技术。这里需要对比autograd.Function和nn.Module之间的区别。 autograd.Function利用Tensor对autograd技术的扩展，为autograd实现了新的运算op，不仅要实现前向传播还要手动实现反向传播。 nn.Module利用了autograd技术，对nn的功能进行扩展，实现了深度学习中更多的层。只需实现前向传播功能，autograd即会自动实现反向传播。 nn.functional是一些autograd操作的集合，是经过封装的函数。 作为两种扩充pytorch接口的方法，我们在实际作用中应该如何选择？如果某一个操作在autograd中尚未支持，那么需要利用Function手动实现对应的前后传播和反向传播。如果某些时候利用autograd接口比较复杂，则可以利用Function将多个操作聚合，实现优化，比直接利用autograd低级别的操作要快。如果只是想在深度学习中怎加某一层，使用nn.Module进行封装则更简单高效。]]></content>
      <categories>
        <category>笔记</category>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[dlrm]]></title>
    <url>%2F2019%2F08%2F21%2Fdlrm%2F</url>
    <content type="text"><![CDATA[一个人在不接触对方的情况下，通过一种特殊的方式，和对方进行一系列的问答。如果在想当长时间内，他无法根据这些问题判断对方是人还是计算机，那么就可以认为这个计算机是智能的。 绪论首先深度学习是机器学习的一个分支。 首先，深度学习问题是一个机器学习问题，指从有限样例中通过算法总结出一般性规律，并可以运用到新的未知数据上。 其次，深度学习采用的模型一般比较复杂，模型常常由许多个线性或者非线性的组件。信息从输入到从模型中输出数据会流过各个组件。因为每个组件都加工过信息，并进而影响后续的组件。当我们得到输出时，我们并不清楚每个组件的贡献是多少。这个问题就叫做贡献度分配问题（Credit Assignment Proble，CAP）。在深度学习中，贡献分配问题是个很关键的问题。 目前可以比较好解决贡献度分配问题的模型是人工神经网络。 深度学习和神经网络并不等价，深度学习还可以采用神经网络以外的模型（比如深度信念网络是一种概率图模型）。 人工智能人工智能就是要让机器的行为看起来就像是人所表现的智能行为一样。 目前，人工智能的主要领域大体上可以分为以下几个方面： 感知：即模拟人的感知能力，对外部刺激信息（视觉和语音等）进行感知和加工。主要研究领域包括语音信息处理和计算机视觉等。 学习：即模拟人的学习能力，主要研究如何从样例或与环境交互中进行学习。主要研究领域包括语音信息处理和计算机视觉等。 认知：即模拟人的认知能力，主要研究领域包括知识表示、自然语言理解、推理、规划、决策等。 人工智能的发展历史发展历程大体上可以分为“推理期”，“知识期”和“学习期”。 推理期1956年达特茅斯会议之后，研究者对人工智能的热情高涨，之后的十几年是人工智能的黄金时期。 早期通过人类的经验，基于逻辑或者事实归纳出来一些规则，然后通过编写程序来让计算机完成一个任务。 但是人们过于乐观，低估了实现人工智能的难度。 之后人工智能的研究陷入帝国，很多人工智能项目的经费也被消减。 人工智能低谷，也叫人工智能冬天（AI Winter），指人工智能史上资金及学术界研究兴趣都大幅减少的时期。人工智能领域经历过好几个技术成熟度曲线，紧接着是失望及批评，以及研究资金断绝，紧接着在几十年后重燃的研究兴趣。1974-80奶奶级1987-93年是两个主要的低谷时期，其他还要几个较小的低谷。 知识期到了20世纪70年代，研究者意识到知识对于人工智能系统的重要性。 出现了各种各样的专家系统，并在特定的专业领域取得了很多成果。专家系统可以简单理解为“知识库+推理机”，是一类具有专门知识和经验的计算机智能系统。 专家系统一般采用知识表示和知识推理等技术来完成通常由领域专家才能解决的复杂问题，因此专家系统也被成为基于知识的系统。一个专家系统具备三要素：领域专家级知识；模拟专家思维；达到专家级的水平。 学习期对于人类的很多智能行为（比如语言理解、图片理解等），我们很难知道其中的原理，也无法描述出这些智能行为背后的“知识”。因此我们很难通过知识推理的方式来实现这些行为的智能系统。 为了解决问题，研究者开始讲研究重点转向让计算机从数据中自己学习。 机器学习的主要目的是设计和分析一些学习算法，让计算机可以从数据中自动分析获得规律，并利用学习到的规律对未知数据进行预测，从而帮助人们完成特定任务。 人工智能发展史]]></content>
      <categories>
        <category>笔记</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【notes】pytorch学习笔记2-autograd部分]]></title>
    <url>%2F2019%2F08%2F20%2Fautograd%2F</url>
    <content type="text"><![CDATA[autogradtorch.autograd是为方便用户使用，专门开发的一套自动求导引擎，它能够根据输入和前向传播过程自动构建计算图，并执行反向传播。 计算图是现代深度学习框架的核心，它为自动求导算法——反向传播提供了理论支持。 Variablepytorch在autograd模块中实现计算图相关功能，autograd中的核心数据结构是Variable。Variable封装了tensor，并记录对tensor的操作记录用来构建计算图。Variale的数据结构如图： graph LR; A[autograd.Variable] --> B(data); A[autograd.Variable] --> C(grad); A[autograd.Variable] --> D(grad_fn); Variable的构造函数需要传入tensor，同时有两个可选参数。 require_grad(bool)：是否需要对该variable进行求导。 volatile(bool)：意为“挥发”，设置为True，构建在该variable之上的图都不会求导，转为推理阶段设计。 Variable支持大部分tensor支持的函数，但其不支持部分inplace函数，因为这些函数会修改tensor自身，而在反向传播中，variable需要缓存原来的tensor来计算梯度。如果想要计算各个Variable的梯度，只需调用根节点variable的backward方法，autograd会自动沿着计算图反向传播，计算每一个叶子节点的梯度。 grad_variables：形状与variable一致，对于y.backward()，grad_variables相当于链式法则$\frac{\partial{z}}{\partial{x}}=\frac{\partial{z}}{\partial{y}}\times\frac{\partial{y}}{\partial{x}}$中的$\frac{\partial{z}}{\partial{y}}$。grad_variables也可以是tensor或序列。 retain_graph：反向传播需要缓存一些中间结果，反向传播之后，这些缓存就被清空，可通过指定这个参数不清空缓存，用来多次反向传播。 create_graph：对反向传播过程再次构建计算图，可通过backward of backward实现求高阶导数。 12from torch.autograd import Variable as Vimport torch as t 123# 从tensor中创建variable，指定需要求导a = V(t.ones(3, 4), requires_grad = True);a tensor([[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]], requires_grad=True)1b = V(t.zeros(3, 4));b tensor([[0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]])1234# 函数的使用和tensor一致# 也可写成c = a + bc = a.add(b)c tensor([[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]], grad_fn=&lt;AddBackward0&gt;)12d = c.sum()d.backward() # 反向传播 1234# 注意二者的区别# 前者在取data后变为tensor，从tensor计算sum得到float# 后者计算sum后仍然是Variablec.data.sum(), c.sum() (tensor(12.), tensor(12., grad_fn=&lt;SumBackward0&gt;))1a.grad tensor([[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]])123# 此处虽然没有指定c需要求导，但c依赖于a，而a需要求导# 因此c的requires_grad属性会自动设为Truea.requires_grad, b.requires_grad, c.requires_grad (True, False, True)123# c.grad是None，c不是叶子节点，他的梯度是用来计算a的梯度# 虽然c.requires_grad = True，但其梯度计算完之后即被释放c.grad is None True接下来看看autograd计算导数和我们手动推导的导数的区别。$$y=x^2e^x$$ 它的导函数是：$$\frac{\partial{y}}{\partial{x}}=2xe^x+x^2e^x$$ 123456def f(x): y = x**2 * t.exp(x) return ydef gradf(x): dx = 2*x*t.exp(x) + x**2*t.exp(x) return dx 123x = V(t.randn(3, 4), requires_grad = True)y = f(x)y tensor([[1.3949e-01, 2.7201e-01, 4.9848e-01, 2.2968e+00], [3.2033e-01, 3.3618e-01, 2.3554e-02, 1.0507e+01], [3.9416e+01, 3.5322e+00, 9.6847e-02, 1.2743e+01]], grad_fn=&lt;MulBackward0&gt;)12y.backward(t.ones(y.size())) # grad_variables形状与y一致x.grad tensor([[ 1.0154, -0.4398, -0.1755, 7.1583], [-0.4095, 1.7961, 0.3532, 24.3531], [76.1412, 10.0143, -0.4190, 28.6505]])12# autograd的计算结果与利用公式手动计算的结果一致gradf(x) tensor([[ 1.0154, -0.4398, -0.1755, 7.1583], [-0.4095, 1.7961, 0.3532, 24.3531], [76.1412, 10.0143, -0.4190, 28.6505]], grad_fn=&lt;AddBackward0&gt;)计算图pytorch中autograd的底层采用了计算图，计算图是一种特殊的有向无环图（DAG），用于记录算子与变量之间的关系。一般用矩形表示算子，椭圆形表示变量。如表达式$\boldsymbol{z}=\boldsymbol{wx}+\boldsymbol{b}$可分解为$\boldsymbol{y}=\boldsymbol{wx}$和$\boldsymbol{z}=\boldsymbol{y}+\boldsymbol{b}$，其计算图如图所示，图中的MUL和ADD都是算子，$\boldsymbol{w}$、$\boldsymbol{x}$、$\boldsymbol{b}$为变量。 graph LR; A((W)) --> C[MUL]; B((X)) --> C[MUL]; C[MUL] --> D((y)); E((b)) --> F[Add]; D((y)) --> F[Add]; F[Add] --> G((z)); 如上有向无环图，$\boldsymbol{X}$和$\boldsymbol{b}$是叶子节点，这些节点通常有用户自己创建，不依赖于其他变量。$\boldsymbol{z}$称为根节点，是计算图的最终目标。利用链式法则很容易求各个叶子节点的梯度。 $$ \frac{\partial{z}}{\partial{b}}=1,\frac{\partial{z}}{\partial{y}}=1\\frac{\partial{y}}{\partial{w}}=x,\frac{\partial{y}}{\partial{x}}=w\\frac{\partial{z}}{\partial{x}}=\frac{\partial{z}}{\partial{y}}\frac{\partial{y}}{\partial{x}}=1w\\frac{\partial{z}}{\partial{w}}=\frac{\partial{z}}{\partial{y}}\frac{\partial{y}}{\partial{w}}=1x $$ 有了计算图上述链式求导可自动利用计算图的反向传播自动完成，其过程如图所示： graph LR; A((dz)) --> B[addBackward]; B[addBackward] --> C((dy)); B[addBackward] --> D((db)); C((dy)) --> E[mulBackward]; E[mulBackward] --> F((dX)); E[mulBackward] --> G((dW)); 图中记录了操作function，每个变量在图中的位置可通过其grad_fn属性在图中的位置推测得到。在反向传播的过程中，autograd沿着这个图从当前变量（根节点z）溯源，可以利用链式求导法则计算所有叶子节点的梯度。 每一个前向传播操作的函数都有与之对应的反向传播函数用来计算输入的各个variable的梯度，这些函数的函数名通常以Backward结尾。 12345x = V(t.ones(1))b = V(t.rand(1), requires_grad = True)w = V(t.rand(1), requires_grad = True)y = w * x # 等价于y = w.mul(x)z = y + b # 等价于z = y.add(b) 1x.requires_grad, b.requires_grad, w.requires_grad (False, True, True)123# 虽然未指定y.requires_grad为True，但由于y依赖于需要求导的w# 故而y.requires_grad为Truey.requires_grad True1x.is_leaf, w.is_leaf, b.is_leaf (True, True, True)1y.is_leaf, z.is_leaf (False, False)123# grad_fn可以查看这个variable的反向传播函数# z是add函数的输出，所以它的反向传播函数是AddBackwardz.grad_fn &lt;AddBackward0 at 0x7f073e390e80&gt;1234#next_function保存grad_fn的输入，grad_fn的输入是一个tuple# 第一个是y，它是乘法（mul）的输出，所以对应的反向传播函数y.grad_fn是MulBackward# 第二个是b，它是叶子节点，由用户创建，grad_fn为None，但是有z.grad_fn.next_functions ((&lt;MulBackward0 at 0x7f073e390828&gt;, 0), (&lt;AccumulateGrad at 0x7f073e390f28&gt;, 0))12# variable的grad_fn对应着图中的functionz.grad_fn.next_functions[0][0] == y.grad_fn True123# 第一个是w，叶子节点，需要求导，梯度是累加的# 第二个是x，叶子节点，不需要求导，所以为Noney.grad_fn.next_functions ((&lt;AccumulateGrad at 0x7f073e390470&gt;, 0), (None, 0))12# 叶子节点的grad_fn是Nonew.grad_fn, x.grad_fn (None, None)计算$\boldsymbol{w}$的梯度时需要用到$\boldsymbol{x}$的数值（$\frac{\partial{y}}{\partial{w}}=x$），这些数值在前向过程中会保存成buffer，在计算完梯度之后会自动清空。为了能够多次反向传播需要指定retain_graph来保留这些buffer。 1y.grad_fn.saved_variables --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) &lt;ipython-input-30-284a59926bb7&gt; in &lt;module&gt; ----&gt; 1 y.grad_fn.saved_variables AttributeError: &apos;MulBackward0&apos; object has no attribute &apos;saved_variables&apos;原因确实是版本问题，PyTorch0.3 中把许多python的操作转移到了C++中，saved_variables 现在是一个c++的对象，无法通过python访问。https://github.com/chenyuntc/pytorch-book/issues/7 可以查看这里进行学习https://github.com/chenyuntc/pytorch-book/blob/0.3/chapter3-Tensor和autograd/Autograd.ipynb,省掉上面的操作 123# 使用retain_graph保存bufferz.backward(retain_graph = True)w.grad tensor([1.])123# 多次反向传播，梯度累加，这也就是w中AccumulateGrad标识的含义z.backward()w.grad tensor([2.])pytorch使用的是动态图，它的计算图在每次前向传播时都是从头开始构建的，所以它能够使用python的控制语句（如for、if等），根据需求创建计算图。这一点在自然语言处理领域中很有用，它意为你不需要事先构建所有可能用到的图的路径，图在运行时才构建。 1234567def abs(x): if x.data[0] &gt; 0: return x else: return -xx = V(t.ones(1), requires_grad = True)y = abs(x)y.backward()x.grad tensor([1.])1234x = V(-1 * t.ones(1), requires_grad = True)y = abs(x)y.backward()print(x.grad) tensor([-1.])123456789def f(x): result = 1 for ii in x: if ii.data &gt; 0: result = ii * result return resultx = V(t.arange(-2, 4).float(), requires_grad = True)y = f(x) # y = x[3] * x[4] * x[5]y.backward()x.grad tensor([0., 0., 0., 6., 3., 2.])变量的requires_grad属性默认为False，如果某一个节点requires_grad被设置为True，那么所有依赖它的节点requires_grad都是True。 with torch.no_grad()内的variable均为不会求导，其优先级高于requires_grad。函数可以用装饰器@torch.no_grad()。可实现一定程度的速度提升，并节省约一半显存，因为其不需要分配空间保存梯度。 详细内容可见：https://pytorch.org/docs/master/autograd.html#locally-disable-grad 1234x = t.tensor([1.], requires_grad = True)with t.no_grad(): y = x * 2y.requires_grad False12345@t.no_grad()def doubler(x): return x * 2z = doubler(x)z.requires_grad False在反向传播过程中非叶子节点的导数计算完之后即被清空。若想查看这些变量的梯度，有以下两种方法： 使用autograd.grad函数 使用hook 推荐使用hook方法，但在实际使用中应尽量避免修改grad的值。 123456x = V(t.ones(3), requires_grad = True)w = V(t.rand(3), requires_grad = True)y = x * w# y依赖于w，而w.requires_grad = Truez = y.sum()x.requires_grad, w.requires_grad, y.requires_grad (True, True, True)123# 非叶子节点grad计算完之后自动清空，y.grad是Nonez.backward()x.grad, w.grad, y.grad (tensor([0.1283, 0.8326, 0.6539]), tensor([1., 1., 1.]), None)12345678# 第一种方法：使用grad获取中间变量的梯度x = V(t.ones(3), requires_grad = True)w = V(t.rand(3), requires_grad = True)y = x * w# y依赖于w，而w.requires_grad = Truez = y.sum()# z对y的梯度，隐式调用backward()t.autograd.grad(z, y) (tensor([1., 1., 1.]),)12345678910111213141516# 第二种方法：使用hook# hook是一个函数，输入是梯度，不应该有返回值def variable_hook(grad): print('y的梯度：\r\n',grad) x = V(t.ones(3), requires_grad = True)w = V(t.rand(3), requires_grad = True)y = x * w# 注册hookhook_handle = y.register_hook(variable_hook)z = y.sum()z.backward()# 除非你每次都要用hook，否则用完之后记得移除hookhook_handle.remove() y的梯度： tensor([1., 1., 1.])最后再来看看variable中grad属性和backward函数grad_variables参数的含义。 variables $\boldsymbol{x}$ 的梯度是目标函数$f(x)$对$\boldsymbol{x}$的梯度，形状与$\boldsymbol{x}$一致。 y.backward(grad_variables)中grad_variables相当于链式法则中的$\frac{\partial{z}}{\partial{x}}=\frac{\partial{z}}{\partial{y}}\frac{\partial{y}}{\partial{x}}$中的$\frac{\partial{z}}{\partial{y}}$。z是目标函数，一般是个标量，故而$\frac{\partial{z}}{\partial{y}}$的形状与$\boldsymbol{y}$的形状一致。z.backward()等价于y.backward(grad_y)。而z.backward()省略了grad_variables参数，是因为z是个标量，而$\frac{\partial{z}}{\partial{z}}=1$ 12345x = V(t.arange(0, 3).float(), requires_grad = True)y = x**2 + x*2z = y.sum()z.backward()x.grad tensor([2., 4., 6.])123456x = V(t.arange(0, 3).float(), requires_grad = True)y = x**2 + x*2z = y.sum()y_grad_variables = V(t.Tensor([1, 1, 1])) y.backward(y_grad_variables)x.grad tensor([2., 4., 6.])值得注意的是，只有对variable的操作才能使用autograd，如果variable的data直接进行操作，将无法使用反向传播。除了参数初始化，一般我们不会直接修改variable.data的值。 在pytorch中计算图的特点总结如下： autograd根据用户对variable的操作构建计算图。对variable的操作抽象为Function。 由用户创建的节点称为叶子节点，叶子节点的grad_fn为None。叶子节点中需要求导的variable，具有AccumulateGrad标识，因其梯度是累加的。 variable默认是不需要求导的，即requires_grad属性默认为False。如果某一个节点requires_grad被设置为True，那么所有依赖它的节点requires_grad都为True。 with torch.no_grad和@torch.no_grad()的作用下的节点都不会求导，优先级比requires_grad高。 多次反向传播时，梯度是累加的。反向传播的中间缓存会被清空，为进行多次反向传播需指定retain_graph=True来保存这些缓存。 非叶子节点的梯度计算完后即被清空，可以使用autograd.grad或hook技术获取非叶子节点梯度的值。 variable的grad与data形状一致，应避免直接修改variable.data，因为对data的直接操作无法利用autograd进行反向传播。 反向传播函数backward的参数grad_variables可以看成链式求导的中间结果，如果是标量，可以省略，默认为1。 pytorch采用动态图设计，可以很方便地查看中间层的输出，动态地设计计算图结构。 扩展autograd目前绝大多数函数都可以使用autograd实现反向求导，但如果需要自己写一个复杂的函数，不支持自动反向求导怎么办？答案是写一个Function，实现它的前向传播和反向传播代码，Function对应于计算图中的矩形，它接受参数，计算并返回结果。下面给出了一个例子。 123456789101112131415161718from torch.autograd import Functionclass MultiplyAdd(Function): @staticmethod def forward(ctx, w, x, b): print('type in forward', type(x)) ctx.save_for_backward(w, x)#存储用来反向传播的参数 output = w*x +b return output @staticmethod def backward(ctx, grad_output): w, x = ctx.saved_tensors # 老版本是saved_variables print('type in backward',type(x)) grad_w = grad_output * x grad_x = grad_output * w grad_b = grad_output * 1 return grad_w, grad_x, grad_b 分析如下： 自定义的Function需要继承autograd.Function，没有构造函数init，forward和backward函数都是静态方法 forward函数的输入和输出都是Tensor，backward函数的输入和输出都是Variable backward函数的输出和forward函数的输入一一对应，backward函数的输入和forward函数的输出一一对应 backward函数的grad_output参数即t.autograd.backward中的grad_variables 如果某一个输入不需要求导，直接返回None，如forward中的输入参数x_requires_grad显然无法对它求导，直接返回None即可 反向传播可能需要利用前向传播的某些中间结果，需要进行保存，否则前向传播结束后这些对象即被释放 12345678910x = V(t.ones(1))w = V(t.rand(1),requires_grad=True)b = V(t.rand(1),requires_grad=True)print('开始前向传播')z = MultiplyAdd.apply(w, x, b)print('开始反向传播')z.backward()# x不需要求导，中间过程还是会计算它的导数，但随后被清空x.grad, w.grad, b.grad 开始前向传播 type in forward &lt;class &apos;torch.Tensor&apos;&gt; 开始反向传播 type in backward &lt;class &apos;torch.Tensor&apos;&gt; (None, tensor([1.]), tensor([1.]))12345678910x = V(t.ones(1))w = V(t.rand(1),requires_grad=True)b = V(t.rand(1),requires_grad=True)print('开始前向传播')z = MultiplyAdd.apply(w, x, b)print('开始反向传播')# 调用MultiplyAdd.backward# 会自动输出grad_w, grad_x, grad_bz.grad_fn.apply(V(t.ones(1))) 开始前向传播 type in forward &lt;class &apos;torch.Tensor&apos;&gt; 开始反向传播 type in backward &lt;class &apos;torch.Tensor&apos;&gt; (tensor([1.]), tensor([0.5986], grad_fn=&lt;MulBackward0&gt;), tensor([1.]))在backward函数里之所以也要对variable进行操作是为了能计算梯度的梯度。 1234x = V(t.Tensor([5]), requires_grad = True)y = x ** 2grad_x = t.autograd.grad(y, x, create_graph = True)grad_x (tensor([10.], grad_fn=&lt;MulBackward0&gt;),)1grad_grad_x = t.autograd.grad(grad_x[0],x);grad_grad_x (tensor([2.]),)]]></content>
      <categories>
        <category>笔记</category>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【notes】pytorch学习笔记1-Tensor部分]]></title>
    <url>%2F2019%2F08%2F19%2Fpytorch-1%2F</url>
    <content type="text"><![CDATA[Tensor和autograd 每个深度学习框架的设计核心是张量和计算图，在pytorch里体现为张量系统（Tensor）和自动微分系统（atutograd）。 Tensor 中文译为张量，可以简单看作一个数组。 与numpy里的ndarrays类似，但tensor支持GPU加速。 基础操作接口角度： torch.function tensor.function 存储角度： 不会修改自身数据，如a.add(b),返回一个值为加法结果的新的tensor。 会修改自身数据，如a.add_(b)，加法的值储存在a中了。 创建Tensor在pytorch中常见的新建tensor的方法： 类别 特点 函数 功能 第一类：基础方法 最灵活 Tensor(*sizes) 基础构造函数 第二类：根据sizes建立 常数型 ones(*sizes) 全1Tensor 常数型 zeros(*sizes) 全0Tensor 常数型 eyes(*sizes) 对角线为1，其他为0 概率分布型 rand/randn(*sizes) 均匀/标准分布 第三类：在一定范围内建立 等差数列型 arange(s,e,step) 从s到e，步长为step 等差数列型 linspace(s,e,steps) 从s到e，均匀切分成steps份 概率分布型 normal(mean,std)/uniform(from,to) 正态分布/均匀分布 概率分布型 randperm(m) 随机分布 其中使用Tensor函数新建tensor是最复杂多变的，它既可以接受一个list，并根据list的数据新建tensor，也可根据指定的形状新建tensor，还能传入其他的tensor。 123# 引入必要的包import torch as tfrom torch.autograd import Variable as V 12# 指定tensor的形状a = t.Tensor(2, 3);a tensor([[7.2443e+22, 4.2016e+30, 9.9708e+17], [7.2296e+31, 5.6015e-02, 4.4721e+21]])12# 用list的数据创建tensorb = t.Tensor([[1,2,3],[4,5,6]]);b tensor([[1., 2., 3.], [4., 5., 6.]])1b.tolist(),type(b.tolist()) # 把tensor转为list ([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], list)tensor.size()返回torch.Size()对象，它是tuple的子类，但其使用方式与tuple略有不同。 1b_size = b.size();b_size torch.Size([2, 3])1b.numel() # numelements前五个字母，b中元素总个数，等价于b.nelement() 612345# 创建一个和b形状一样的tensorc = t.Tensor(b_size)# 创建一个元素为2和3的tensord = t.Tensor((2, 3))c, d # 输出结果不同，明显看出torch.Size()对象和tuple的不同 (tensor([[5.8959e-35, 4.5636e-41, 1.0257e-36], [0.0000e+00, 5.0000e+00, 6.0000e+00]]), tensor([2., 3.]))tensor.shape等价于tensor.size() 1c.shape torch.Size([2, 3])* 需要注意：t.Tensor(*sizes)创建tensor时，系统不会马上分配空间，只会计算内存是否够用，使用到tensor时才会分配，而其他方法是创建后会立马分配空间。 * 1t.ones(2, 3) tensor([[1., 1., 1.], [1., 1., 1.]])1t.zeros(2, 3) tensor([[0., 0., 0.], [0., 0., 0.]])1t.linspace(1, 10 ,3) tensor([ 1.0000, 5.5000, 10.0000])1t.randn(2, 3) tensor([[-0.4864, 0.5022, -0.4059], [ 0.4138, 1.1588, -1.1650]])1t.randperm &lt;function _VariableFunctions.randperm&gt;123# 0到n-1随机排列后的数列n = 10t.randperm(n) tensor([2, 5, 8, 3, 4, 1, 0, 7, 9, 6])1t.eye(2, 3) # 不要求行列数一致 tensor([[1., 0., 0.], [0., 1., 0.]])1t.normal(t.Tensor([0]),t.Tensor([1])) tensor([-0.5517])常用Tensor操作 tensor.view方法可以改变tensor的形状，但要保证前后元素总数一致。前后保持数据一致，返回的新tensor与源tensor共享内存。 在实际应用中可能经常需要增加或减少某个维度，这是squeeze和unsqueeze两个函数排上用场。 12a = t.arange(0, 6)a.view(2, 3) tensor([[0, 1, 2], [3, 4, 5]])12b = a.view(-1, 3) # 当某一维为-1时，会自动计算它的大小b tensor([[0, 1, 2], [3, 4, 5]])1b.shape, b.unsqueeze(1).shape # 注意形状，在第1维上增加“1” (torch.Size([2, 3]), torch.Size([2, 1, 3]))1b.unsqueeze(-2) # -2表示倒数第二个维度 tensor([[[0, 1, 2]], [[3, 4, 5]]])12c = b.view(1, 1, 1, 2, 3)c, c.squeeze(0) # 压缩第0维的1 (tensor([[[[[0, 1, 2], [3, 4, 5]]]]]), tensor([[[[0, 1, 2], [3, 4, 5]]]]))1c.squeeze() # 压缩所有的“1”的维度 tensor([[0, 1, 2], [3, 4, 5]])12a[1] = 100b # a和b共享内存，修改了a，b也变了 tensor([[ 0, 100, 2], [ 3, 4, 5]])resize是另一种改变size的方法，和view不同的地方是resize可以改变尺寸，可以有不同数量的元素。如果新尺寸超过了旧尺寸，会自动分配空间，如果新尺寸小于旧尺寸，之前的数据依旧会保存。 12b.resize_(1, 3)b tensor([[ 0, 100, 2]])12b.resize_(3, 3) # 旧的数据依旧被保存，多出的数据会分配新空间。b tensor([[ 0, 100, 2], [ 3, 4, 5], [7881702260482471202, 8319104481852400229, 7075192647680159593]])索引操作Tensor支持和numpy.ndarray类似的索引操作，语法上也类似。 如无特殊说明，索引出来的结果与原tensor共享内存 1a = t.randn(3,4);a tensor([[ 0.8865, -0.8832, -1.0883, -0.2804], [-0.9056, 0.0635, 0.5528, -0.0222], [ 1.4919, -1.0480, -1.7623, 0.8558]])1a[0] # 第0行 tensor([ 0.8865, -0.8832, -1.0883, -0.2804])1a[:, 0] # 第0列 tensor([ 0.8865, -0.9056, 1.4919])1a[0][2] # 第0行第2个元素，等价于a[0,2] tensor(-1.0883)1a[0, -1] # 第0行最后一个元素 tensor(-0.2804)1a[:2] # 前两行 tensor([[ 0.8865, -0.8832, -1.0883, -0.2804], [-0.9056, 0.0635, 0.5528, -0.0222]])1a[:2, 0:2] # 前两行，第0,1列 tensor([[ 0.8865, -0.8832], [-0.9056, 0.0635]])1a[0:1, :2].shape, a[0, :2].shape # 注意两者的区别是形状不同，但是值是一样的 (torch.Size([1, 2]), torch.Size([2]))123a[a &gt; 1] # 等价于a.masked_select(a&gt;1)# 选择结果与原tensor不共享内存空间 tensor([1.4919])1a[t.LongTensor([0,1])] # 第0行和第1行 tensor([[ 0.8865, -0.8832, -1.0883, -0.2804], [-0.9056, 0.0635, 0.5528, -0.0222]])常用的选择函数： 函数 功能 index_select(input, dim, index) 在指定维度dim上选取，例如选取某行某列 masked_select(input, mask) 例子如上，a[a &gt; 0],使用ByteTensor进行选取 non_zero(input) 非0元素的下标 gather(input, dim, index) 根据index，在dim维度上选取数据，输出的size与index一样 gather是一个比较复杂的操作，对于一个二维的tensor，输出的每个元素如下： 12out[i][j] = input[index[i][j]][j] # dim = 0out[i][j] = input[i][index[i][j]] # dim = 1 1a = t.arange(0, 16).view(4, 4);a tensor([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14, 15]])123# 选取对角线上的元素index = t.LongTensor([[0,1,2,3]])a.gather(0, index) tensor([[ 0, 5, 10, 15]])123# 选取反对角线上的元素index = t.LongTensor([[3, 2, 1, 0]]).t()a.gather(1,index) tensor([[ 3], [ 6], [ 9], [12]])123# 选取反对角线上的元素，注意与上面不同index = t.LongTensor([[3, 2, 1, 0]])a.gather(0, index) tensor([[12, 9, 6, 3]])123# 选取两个对角线上的元素index = t.LongTensor([[0, 1, 2, 3], [3, 2, 1, 0]]).t()b = a.gather(1, index);b tensor([[ 0, 3], [ 5, 6], [10, 9], [15, 12]])gather的逆操作是scatter_, gather把数据从input中按index取出，而scatter_是把取出的数据再放回去。注意scatter_函数是inplace操作。 1234out = input.gather(dim, index)--&gt;近似逆操作out = Tensor()out.scatter_(dim, index) 1234# 把两个对角线元素放回到指定位置c = t.zeros(4, 4)c.scatter_(1, index, b.float()) tensor([[ 0., 0., 0., 3.], [ 0., 5., 6., 0.], [ 0., 9., 10., 0.], [12., 0., 0., 15.]])高级索引高级索引可以看成是普通索引的扩展，但是高级索引操作的结果一般不和原Tensor共享内存。 1x = t.arange(0, 27).view(3, 3, 3);x tensor([[[ 0, 1, 2], [ 3, 4, 5], [ 6, 7, 8]], [[ 9, 10, 11], [12, 13, 14], [15, 16, 17]], [[18, 19, 20], [21, 22, 23], [24, 25, 26]]])1x[[1, 2], [1, 2], [2, 0]] # 元素的个数是列表的长度 元素为x[1,1,2]和x[2,2,0] tensor([14, 24])1x[[2,1,0],[0],[1]] # 元素为最长列表的长度 x[2,0,1] x[1,0,1] x[0,0,1] tensor([19, 10, 1])1x[[0,2],...] # x[0] x[2] tensor([[[ 0, 1, 2], [ 3, 4, 5], [ 6, 7, 8]], [[18, 19, 20], [21, 22, 23], [24, 25, 26]]])Tensor类型默认的Tensor类型为FloatTensor，可通过t.get_default_tensor_type修改默认类型（如果默认类型为GPU tensor，在所有操作都在GPU上进行）。 HalfTensor是专门为GPU版本设计的，同样的元素个数，显存占用只有FloatTensor的一半，所以可以极大地缓解GPU显存不足的问题，但是由于HalfTensor所能表示的数值大小和精度有限，所以可能出现溢出等问题。 数据类型 CPU tensor GPU tensor 32bit浮点 torch.FloatTensor torch.cuda.FloatTensor 64bit浮点 torch.DoubleTensor torch.cuda.DoubleTensor 16半精度浮点 N/A torch.cuda.HalfTensor 8bit无符号整型（0~255） torch.ByteTensor torch.cuda.ByteTensor 8bit有符号整型（-128~127） torch.CharTensor torch.cuda.CharTensor 16bit有符号整型 torch.ShortTensor torch.cuda.ShortTensor 32bit有符号整型 torch.IntTensor torch.cuda.IntTensor 64bit有符号整型 torch.LongTensor torch.cuda.LongTensor 各数据类型之间可以互相转换，type(new_type)是通用的做法，同时还有float、long、half等快捷方法。CPU tensor与GPUtensor之间的互相装换通过tensor.cuda和tensor.cpu的方法实现。Tensor还有一个new方法，用法与t.Tensor一样，会调用该tensor对应类型的构造函数，生成与当前tensor类型一致的tensor。 123456# 设置默认tensor类型, 注意参数是字符串# t.set_default_tensor_type('torch.IntTensor') 会报错# TypeError: only floating-point types are supported as the default type# t.get_default_dtype() 返回 torch.float32# t.set_default_dtype(t.int) 报错 TypeError: only floating-point types are supported as the default type 1a = t.Tensor(2, 3);a tensor([[1.8609e+34, 1.8179e+31, 1.8524e+28], [9.6647e+35, 2.0076e+29, 7.3185e+28]])1b = a.int();b tensor([[-2147483648, -2147483648, -2147483648], [-2147483648, -2147483648, -2147483648]], dtype=torch.int32)1c = a.type_as(b);c tensor([[-2147483648, -2147483648, -2147483648], [-2147483648, -2147483648, -2147483648]], dtype=torch.int32)1d = b.new(2, 3);d tensor([[ 0, 775041082, 960062260], [1697986359, 926101553, 895706424]], dtype=torch.int32)123# 查看函数new的源码a.new?? 逐元素操作这部分操作会对tensor的每个元素进行操作，输入和输出的形状相同。 函数 功能 abs/sqrt/div/exp/fmod/log/pow.. 绝对值/平方根/除法/指数/求余/对数/求幂 cos/sin/asin/atan2/cosh 三角函数 ceil/round/floor/trunc 上取整/四舍五入/下取整/只保留整数部分 clamp(input,min,max) 超过min和max部分截断 sigmod/tanh… 激活函数 对于很多基本的运算，比如加减乘除求余等运算pytorch都实现了运算符重载，可以直接使用运算符。其中camp(x, min, max)的输出满足一个分段函数： $$y_i=\begin{cases}min, &amp; {x_i &lt; min}\\x_i, &amp; {min \leq x_i \leq max}\\max, &amp; {x_i &gt; max}\end{cases}$$ 12a = t.arange(0, 6).view(2, 3).float() # 注意要转换一下类型，否则会报错t.cos(a) tensor([[ 1.0000, 0.5403, -0.4161], [-0.9900, -0.6536, 0.2837]])1a % 3 # 等价于t.fmod(a, 5) tensor([[0., 1., 2.], [0., 1., 2.]])1a ** 2# 等价于t.power(a, 2) tensor([[ 0., 1., 4.], [ 9., 16., 25.]])1234# a中每个元素与3相比取较大的那一个print(a)t.clamp(a, min = 3) tensor([[0., 1., 2.], [3., 4., 5.]]) tensor([[3., 3., 3.], [3., 4., 5.]])归并操作这类操作会使输入形状小于输出形状，并可以沿着某一维度进行制定操作。 函数 功能 mean/sum/median/mode 均值/和/中位数/众数 norm/dist 范数/距离 std/var 标准差/方差 cumsum/cumprod 累加/累乘 几乎每个函数都有一个dim参数，用来制定在那个维度上执行。假设输入的形状是(m, n, k): 如果指定dim = 0，输出的形状为(1, n, k)或者(n, k) 如果指定dim = 1，输出的形状为(m, 1, k)或者(m, k) 如果指定dim = 2，输出的形状为(m, n, 1)或者(m, n) 也就是dim指定哪个维度，那个维度就会变成1，size中是否有1取决于keepdim，keepdim=True会保留1，keepdim默认为False。但是并非总是这样，比如cumsum。 归并运算就是对其他维度取值相同且该维度取值不同元素进行操作。 12b = t.ones(2, 3)b.sum(dim = 0, keepdim = True) tensor([[2., 2., 2.]])1b.sum(dim = 0) #keepdim = False tensor([2., 2., 2.])1b.sum(dim = 1) tensor([3., 3.])123a = t.arange(0, 6).view(2, 3)print(a)a.cumsum(dim = 1) #沿着行累加 tensor([[0, 1, 2], [3, 4, 5]]) tensor([[ 0, 1, 3], [ 3, 7, 12]])cumsum可以理解为以dim这个维度上索引取值相同的看作一个整体，比如dim=0每一行就是一个整体，cumsum运算相当于dim这个维度上取值为n的值加上取值为n-1的值（这个n-1已经进行过前面的运算，不是初始的值）。 比较比较函数有的是逐元素操作，有的是归并操作。 函数 功能 gt/lt/ge/le/eq/ne 大于/小于/大于等于/小于等于/等于/不等 topk 最大的k个数 sort 排序 max/min 比较两个tensor的最大值或最小值 表中第一行的比较操作已经重载，已经可以使用a&gt;=b, a&gt;b, a!=b和a==b，其返回结果为一个ByteTensor,可以用来选取元素(高级索引)。 max和min两个操作比较特殊，以max为例： t.max(tensor):返回tensor中最大的一个数。 t.max(tensor,dim)：指定维上最大的一个数，返回tensor和下标。 t.max(tensor1,tensor2)：比较两个tensor中较大的元素。 tensor和一个数的比较可以用clamp函数。 1a = t.linspace(0, 15, 6).view(2, 3);a tensor([[ 0., 3., 6.], [ 9., 12., 15.]])1b = t.linspace(15, 0, 6).view(2, 3);b tensor([[15., 12., 9.], [ 6., 3., 0.]])1a &gt; b tensor([[False, False, False], [ True, True, True]])1a[a &gt; b] tensor([ 9., 12., 15.])1t.max(a) tensor(15.)1t.max(a, dim = 1) torch.return_types.max( values=tensor([ 6., 15.]), indices=tensor([2, 2]))1t.max(a, b) tensor([[15., 12., 9.], [ 9., 12., 15.]])123# 比较a和10较大的元素t.clamp(a, min=10) tensor([[10., 10., 10.], [10., 12., 15.]])线性代数pytorch的线性函数封装了Blas和Lapack。 函数 功能 trace 对角线元素（矩阵的迹） diag 对角线元素 triu/tril 矩阵的上三角/下三角，可以指定偏移量 mm/bmm 矩阵乘法，batch的矩阵乘法 addmm/addbmm/addmv 矩阵运算 t 转置 dot/cross 内积/外积 inverse 求逆矩阵 svd 奇异值分解 需要注意矩阵装置会导致储存空间不连续，需调用它的.contiguous方法将其转为连续。 12b = a.t()b.is_contiguous(),b (False, tensor([[ 0., 9.], [ 3., 12.], [ 6., 15.]]))1b.contiguous() tensor([[ 0., 9.], [ 3., 12.], [ 6., 15.]])Tensor和Numpytensor和numpy数组之间具有很高的相似性，彼此之间相互操作也十分高效。需要注意，numpy和tensor共享内存。当遇到tensor不支持的操作时，可先转成Numpy数组，处理后再装回tensor，其转换开销很小。 广播法则是科学运算中经常使用的一个技巧，它在快速执行向量化的同时不会占用额外的内存、显存。Numpy的广播法则定义如下： 让所有输入数组都向shape最长的数组看齐，shape中不足的部分通过在前面加1补齐。 两个数组要么在某一个维度的长度一致，要么其中一个为1，否则不能计算。 当输入数组的某个维度的长度为1时，计算时沿着此维度复制扩充成一样的形状。 pytorch当前已经支持了自动广播法则，但建议可以手动通过函数实现广播法则，更直观不易出错。 unsqueeze或者view：为数据的某一维的形状补1，实现法则1。 expand或者expand_as，重复数组，实现法则3；该操作不会复制数组，所以不会占用额外的空间。 注意:repeat实现有expand类似，但是repeat会把相同数据复制多份，因此会占用额外空间。 12a = t.ones(3, 2)b = t.zeros(2, 3, 1) 1234567# 自动广播法则# 第一步：a是二维，b是三维，所以先在较小的a前面补1，# 即：a.unsqueeze(0), a的形状变成(1, 3, 2), b的形状是(2, 3, 1),# 第二步：a和b在第一维和第三维的形状不一样，其中一个为1# 可以利用广播法则扩展，两个形状都变成了(2, 3, 2)(a + b).shape,a + b (torch.Size([2, 3, 2]), tensor([[[1., 1.], [1., 1.], [1., 1.]], [[1., 1.], [1., 1.], [1., 1.]]]))1a.unsqueeze(0).expand(2, 3, 2) + b.expand(2, 3, 2) tensor([[[1., 1.], [1., 1.], [1., 1.]], [[1., 1.], [1., 1.], [1., 1.]]])123import numpy as npa = np.ones([2, 3], dtype = np.float32)a array([[1., 1., 1.], [1., 1., 1.]], dtype=float32)1b = t.from_numpy(a);b tensor([[1., 1., 1.], [1., 1., 1.]])12b = t.Tensor(a) # 也可以直接讲numpy对象传入Tensor，这种情况下若numpy类型不是Float32会新建。b tensor([[1., 1., 1.], [1., 1., 1.]])12c = b.numpy() # a, b, c三个对象共享内存c array([[1., 1., 1.], [1., 1., 1.]], dtype=float32)12# expand不会占用额外空间，只会在需要时才扩充，可极大地节省内存。e = t.Tensor(a).unsqueeze(0).expand(1000000000000, 2, 3) 内部结构tensor分为头信息区（Tensor）和存储区（Storage），信息区主要保存着tensor的形状（size），步长（stride）、数据类型（type）等信息，而真正的数据则保存成连续的数组。 graph LR; A[Tensor A: *size *stride * dimention...] --> C[Storage:*data *size ...]; B[Tensor B: *size *stride * dimention....] --> C[Storage:*data *size ...]; 一般来说，一个tensor有着与之对应的storage，storage是在data之上封装的接口，便于使用。不同的tensor的头信息一般不同，但却可能使用相同的storage。下面我们来看两个例子。 12a = t.arange(0, 6)a.storage() 0 1 2 3 4 5 [torch.LongStorage of size 6]12b = a.view(2, 3)b.storage() 0 1 2 3 4 5 [torch.LongStorage of size 6]123# 一个对象的id值可以看作它在内存中的地址# a和b storage的内存地址一样，即是同一个storageid(b.storage()) == id(a.storage()) True1234# a改变，b也随之改变，因为它们共享storagea[1] = 100b tensor([[ 0, 100, 2], [ 3, 4, 5]])12c = a[2:]c.storage() 0 100 2 3 4 5 [torch.LongStorage of size 6]12c.data_ptr(), a.data_ptr(), c.dtype # data_ptr返回tensor的首元素的内存地址# 可以看出相差16，这是因为2x8相差两个元素，每个元素占8个字节 (61509136, 61509120, torch.int64)12c[0] = -100 # c[0]的内存地址对应a[2]内存地址a tensor([ 0, 100, -100, 3, 4, 5])123d = t.Tensor(c.float().storage())d[0] = 6666b tensor([[ 0, 100, -100], [ 3, 4, 5]])123# 下面4个共享storageid(a.storage()) == id(b.storage()) == id(c.storage()) == id(d.storage()) True1a.storage_offset(), c.storage_offset(), a[3:].storage_offset() (0, 2, 3)12e = b[::2, ::2] # 隔2行/列取一个元素id(e.storage()) == id(a.storage()) True1b.stride(), e.stride() ((3, 1), (6, 2))1e.is_contiguous() False 可见绝大多数操作并不修改tensor的数据，只是修改头信息。这样更节省内存，同时提升了处理的速度。但是，有些操作会导致tensor不连续，这时需调用tensor.contiguous方法将他们变成连续数据，该方法复制数据到新的内存，不再与原来的数据共享storage。 另外高级索引一般不共享内存，而普通索引共享storage。 其他有关Tensor的话题持久化tensor的保存和加载十分简单，使用t.save和t.load即可完成相应功能。在save/load时可以指定使用的pickle模块，在load时还可以将GPU tensor映射到CPU或其他GPU上。 123456789if t.cuda.is_available(): a = a.cuda(1) t.save(a, 'a.pth') # 加载为b，储存于GPU1上（因为保存时就在GPU1上） b = t.load('a.pth') # 加载为c，储存在CPU c = t.load('a.pth', map_location = lambda storage,loc:storage) # 加载为d，储存于GPU0上 d = t.load('a.pth', map_location = &#123;'cuda:1':'cuda:0'&#125;) 向量化向量化计算是一种特殊的并行计算方式，通常是对不同的数据执行同样的一个或一批指令。向量化可极大第提高科学运算的效率。Python有许多操作很低效，尤其是for循环。在科学计算中要极力避免使用Python原生的for循环，尽量使用向量化的数值计算。 12345def for_loop_add(x, y): result = [] for i, j in zip(x, y): result.append(i + j) return t.Tensor(result) 12x = t.zeros(100)y = t.ones(100) 12%timeit -n 10 for_loop_add(x, y)%timeit -n 10 x + y 729 µs ± 414 µs per loop (mean ± std. dev. of 7 runs, 10 loops each) The slowest run took 4.81 times longer than the fastest. This could mean that an intermediate result is being cached. 3.5 µs ± 2.69 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)可见有好几百倍的速度差距，因此在实际使用中应尽量调用内建函数，这些函数底层由C/C++实现，能通过执行底层优化实现高效计算。 此为还需要注意几点： 大多数t.function都有一个参数out，这时产生的结果将保存在out指定的tensor之中 t.set_num_threads可以设置pytorch进行CPU多线程并行计算时所占用的线程数，来限制pytorch所占用的CPU数目。 t.set_printoptions可以用来设置打印tensor时的数值精度和格式。 1a = t.randn(2, 3); a tensor([[-0.1227, -0.0569, -0.6876], [ 1.6025, 0.6995, 0.1694]])1t.set_printoptions(precision = 10);a tensor([[-0.1226951405, -0.0568769276, -0.6875813603], [ 1.6024936438, 0.6995284557, 0.1693879962]])]]></content>
      <categories>
        <category>笔记</category>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[thinkload]读书-疑问]]></title>
    <url>%2F2019%2F08%2F13%2Fdeeplearning-book%2F</url>
    <content type="text"><![CDATA[2019-8-13 为什么要求算法最小化cost function却要以准确率错误率来衡量它？从cost到准确率丢失了衡量差距到底有多大的信息，这两个并不等价啊！ 如何衡量一个被选择的函数在某一目的上达到了多少效果？或者起到了多少作用？ 把输入端之前看成也连接了一个网络，这个网络是一种输出数据的网络，两个网络连接在一起看成一个大网络，而这种网络的生成目的应当是不同于后面我们人类设计网络的目的。但是输出的结果仍能一定程度上的符合人类需求，是不是说网络的每个部分可以存在不同的目标？ 有一个迷宫一样的屋子，同样也是一个挑战，找出从一楼走到天台的最短路径，有一万个人来接受挑战每个走到屋顶的人都被告知自己是否是走的最短路径，每个人都是独立的，有自己的判断标准，当他再次上楼时如何分析他们行为的变化？]]></content>
      <categories>
        <category>thinkload</category>
        <category>花书</category>
      </categories>
      <tags>
        <tag>thinkload</tag>
        <tag>花书</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[thinkload]]></title>
    <url>%2F2019%2F08%2F10%2Fthinkload%2F</url>
    <content type="text"><![CDATA[Thinkload, download something from my mind when thinking. 2019-08-10 要达到完全的确定性，在下定义的角度必然需要无穷个修饰词, 或者说解释无穷个足够的特性，或者说无穷个参数。 另一个角度，把具有存在的物体可以看做一个函数，一个从一个定义域（一个包含所有我们在意的物体的集合）映射到一个可观测领域的函数，对于这个可观测领域的人要准确确定其存在或理解其存在需要理解这个函数的所有参数。 而因为有很多观察角度，就有很多观测领域，对于每个领域的参数又显然是不同的，对一个领域准确注定对另一不同领域就不那么准确。 也可以说函数的参数确定了可观测领域，函数的参数里蕴含了可观测领域的性质，这么看物体本身到底是什么就不那么重要，更重要的是可观测领域的性质，更重要的是观察物体的角度。 那参数是怎么得到的呢？这里发现，我们认识一个新的物体的存在必然需要一些先天的或者一些先验的存在，比如视觉听觉等五感之类的，或者一些产生在大脑的感觉，更进一步是思维灵魂的东西，或者直接说自我的存在—一个观察者自我的存在。 之前思考问题时犯了个错，不该不严谨的将物体给以“个”的量词，使得思考范围被放到离散的物体分布。 可能通过“试错”的方式学习有效，和反复试验取频率作为概率的方法有效这件事有相似性。另外，大数定律的内在含义到底是什么？为什么反复试验起作用？]]></content>
      <categories>
        <category>thinkload</category>
      </categories>
      <tags>
        <tag>thinkload</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【notes】docker学习笔记6-docker容器数据管理]]></title>
    <url>%2F2019%2F08%2F09%2Fdocker-6%2F</url>
    <content type="text"><![CDATA[docker容器的数据管理简介 docker容器的数据卷 docker的数据卷容器 docker数据卷的备份和还原 docker容器的数据卷什么是数据卷(Data Volume)docker的生存周期是与运行的程序相一致的，而我们需要数据持久化，docker容器之间也需要共享一些数据 数据卷是经过特殊设计的目录，可以绕过联合文件系统（UFS)，为一个或多个容器提供访问。 数据卷设计的目的，在于数据持久化，它完全对独立于容器的生存周期，因此docker不会在容器删除时删除其挂载的数据卷，也不会存在类似的垃圾收集机制，对容器引用的数据卷进行处理。 数据卷架构： docker数据卷独立于docker，独立运docker的生存周期。 docker数据卷位于docker的宿主机中文件系统。 docker数据卷既可以是目录也可是文件。 docker数据卷与宿主机进行数据共享。 同一目录或文件可以支持多个容器 数据卷的特点 数据卷在容器启动时初始化，如果容器使用的镜像在挂载点包含了数据，这些数据会拷贝到新初始化的数据卷中。 数据卷可以在容器之间共享和重用 可以对数据卷里的内容直接进行修改 数据卷的变化不会影响镜像的更新 卷会一直存在，即使挂载数据卷的容器已经被删除 数据卷的使用 为容器添加数据卷 1sudo docker run -v ~/container_data:/data -it ubuntu /bin/bash 在本机系统的目录:在容器中映射的目录名 注：这种方式（bind mount)已不推荐使用，应使用volume方式 123456docker volume create my_volume # 创建卷docker volume ls # 卷列表docker volume inspect my_volume #卷信息docker volume rm my_volume # 删除卷docker run -v [卷名]:[容器目录]:[选项列表] -it ubuntu /bin/bash 详情：https://deepzz.com/post/the-docker-volumes-basic.html 为数据卷添加访问权限 1sudo docker run -v [卷名]:[容器目录]:ro(访问权限) -it ubuntu /bin/bash 使用dockerfile构建包含数据卷的镜像dockerfile指令： VOLUME [“/data1”, “/data2”] 不能映射到本地目录，并且运行同一镜像的不同容器所创建的数据卷也是不一样的。 docker的数据卷容器什么是数据卷容器： 命名的容器挂载数据卷，其他容器通过挂载这个容器实现数据共享，挂载数据卷的容器，就叫做数据卷容器 图示： 挂载数据卷容器的方法 1docker run --volumes-from [CONTAINER NAME] 如果数据卷容器删除（即使同时删除挂载的数据卷）后，挂载该数据卷容器的容器的数据目录仍存在且有效。 数据卷容器的作用仅仅是将数据卷挂载的配置传递到挂载了数据卷容器的新容器中。 docker数据卷的备份与还原 数据备份方法 1docker run --volumes-from [container name] -v $(pwd):/backup ubuntu tar cvf /backup/backup.tar [container data volume] 数据还原方法 1docker run --volumes-from [container name] -v $(pwd):/backup ubuntu tar xvf /backup/backup.tar [container data volume]]]></content>
      <categories>
        <category>笔记</category>
        <category>docker</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【notes】docker学习笔记5-dockerfile]]></title>
    <url>%2F2019%2F08%2F08%2Fdocker-5%2F</url>
    <content type="text"><![CDATA[dockerfile指令指令格式注释： # Comment指令： INSTRUCTION argument FROM FROM &lt;image&gt; FROM &lt;image&gt;:&lt;tag&gt; 必须已经存在的镜像，也就是基础镜像 必须是第一条非注释指令 MAINTAINER MAINTAINER &lt;name&gt; 指定镜像的作者信息，包含镜像的所有者和练习方式 RUN构建构成运行的 RUN &lt;command&gt; (shell模式) /bin/sh -c command RUN [“executable”, “param1”, “param2”] (exec模式) RUN[“/bin/bash”, “-c”, “echo hello”] EXPOSE EXPOSE &lt;port&gt; [&lt;port&gt;…] 指定运行该镜像的容器使用的端口，但只是告诉docker会使用特定的端口号，出于安全考虑不会自动打开，在容器运行时仍需要手动指定端口映射。CMD ENTRYPOINT 指定容器启动时运行的命令 CMD [“executable”, “param1”, “param2”] (exec模式) CMD command param1 param2 (shell模式) CMD [“params1”, “params2”] (作为ENTRYPOINT指令的默认参数) 在docker run时如果指定命令的话dockerfile里的cmd命令会被覆盖掉。 ENTRYPOINT [“executable”, “param1”, “param2”] (exec模式) ENTRYPOINT command param1 param2 (shell模式) 默认不会被覆盖，如果需要覆盖需要指定docker run –entrypoint 覆盖 ADD COPY VOLUME设置镜像的目录和文件 ADD &lt;src&gt;…&lt;dest&gt; ADD [“&lt;src”…”“] (适用于文件路径中有空格) COPY &lt;src&gt;…&lt;dest&gt; COPY [“&lt;src”…”“] (适用于文件路径中有空格) 可以使文件地址（构建目录的相对地址），也可以是远程url（推荐使用curl获取文件内容） ADD vs. COPY ADD包含类似tar的解压功能 如果单纯复制文件，docker推荐使用COPY VOLUME [“/data”] 添加卷 WORKDIR ENV USER构建及容器运行时的环境设置 WORKDIR /path/to/workdir (设置工作目录，通常使用绝对路径，否则会一直传递下去) e.g: 1234WORKDIR /aWORKDIR bWORKDIR CRUN pwd # 结果为 /a/b/c ENV &lt;key&gt;&lt;value&gt; ENV &lt;key&gt;=&lt;value&gt; 设置环境变量 USER daemon USER user USER uidUSER user:group USER uid:gidUSER user:gid USER uid:group 指定运行的用户，若不指定则默认root用户。 ONBUILD ONBUILD [INSTRUCTION] 镜像触发器 当一个镜像被其他镜像作为基础镜像时执行 会在构建过程中插入指令 dockerfile构建过程 从基础镜像运行一个容器 执行一条指令，对容器作出修改 执行类似docker commit的操作， 提交一个新的镜像层（中间层镜像） 再基于刚提交的镜像运行一个新的容器 执行dockerfile中的下一条指令，直至所有指令执行完毕 中间层镜像进行调试 注：dockerfile会删除中间层镜像容器但不会删除中间层镜像 构建缓存，构建时会建立缓存，因此第二次执行构建命令会很快，是因为使用了缓存。 不使用缓存 1docker build --no-cache 另一种方法通过更改缓存刷新时间 FROM Ubuntu:14:04MAINTAINER dormancypress user@mail.comENV REFRESH_DATE 2019-08-08RUN apt-get updateRUN apt-get install -y nginxEXPOSE 80 修改REFRESH_DATE时间 查看镜像构建过程 1docker history [image]]]></content>
      <categories>
        <category>笔记</category>
        <category>docker</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【notes】docker学习笔记4-docker客户端与守护进程]]></title>
    <url>%2F2019%2F08%2F07%2Fdocker-4%2F</url>
    <content type="text"><![CDATA[docker的C/S模式 客户端与守护进程通信的接口 命令行接口 remote API： RESTful风格API STDIN STDOUT STDERR 语言参考：https://docs.docker.com/reference/api/docker_remote_api 连接方式 unix:///var/run/docker.sock 默认方式 tcp://host:port fd://socketfd 利用socket进行通信 查看正在运行的守护进程 1ps -ef | grep docker 连接socket进行通信 12nc -U /var/run/docker.sockGET /info HTTP/1.1 以上都是在本地的访问，docker也支持远程访问。 docker守护进程的配置和操作 查看守护进程 12ps -ef | grep dockersudo status docker 守护进程的启动、停止和重启 123sudo service docker startsudo service docker stopsudo service docker restart docker的启动选项 1docker -d [OPTIONS] #所以守护形式运行 运行相关: -D, –debug = false -e, –exec-driver = “native” -g, –graph = “/var/lib/docker” –icc = true -l, –log-level = “info” –label = [] -p, –pidfile = “/var/run/docker.pid” docker服务器连接相关： -G, –group = “docker” -H, –host = [] –tls = false –tlscacert = “/home/sven/.docker/ca.pem” –tlscert = “/home/sven/.docker/cert.pem” –tlskey = “/home/sven/.docker/key.pem” –tlsverify = false RemotAPI相关： –api-enable-cors = false Registry相关： –insecure-registry = [] –registry-mirror = [] 网络设置相关： -b, –bridge = “” –bip = “” –fixed-cidr = “” –fixed-cidr-v6 = “” –dns = [] –dns-search = [] –ip = 0.0.0.0 –ip-forward = true –ip-masq = true –iptables = true –ipv6 = false –mtu = 0 启动配置文件 /etc/default/docker 注：ubuntu 16.04及以上版本使用： 修改/lib/systemd/system/docker.service中的ExecStart 加载配置： 123systemctl daemon-reloadservice docker restartdocker info docker的远程访问 第二台安装docker的服务器 保证Client API与Server API版本一致 修改docker守护进程启动选项 修改服务器端配置 -H tcp://host:post unix:///path/to/socket fd://* or fd//socketfd 守护进程默认配置： -H unix:///var/run/docker.sock 注：我的默认的是 fd:// 改为 tcp: tcp://0.0.0.0:2375 1curl http://ip:2375/info 修改客户端配置 -H tcp://host:post unix:///path/to/socket fd://* or fd//socketfd 默认配置： -H unix:///var/run/docker.sock docker -H tcp//ip:2375 # 太麻烦 export DOCKET_HOST=”tcp://ip:2357” # 使用环境变量 export DOCKET_HOST=”tcp://ip:2357” # 使用本地 怎样在设置了远程连接的服务器也支持本机连接？答：给-H再增加一个方式，-H可以设置多个值。]]></content>
      <categories>
        <category>笔记</category>
        <category>docker</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【notes】docker学习笔记3-docker镜像]]></title>
    <url>%2F2019%2F08%2F06%2Fdocker-3%2F</url>
    <content type="text"><![CDATA[查看和删除镜像 镜像的存储位置：/var/lib/docker 列出镜像1docker images [OPSIONS] [REPOSITORY] -a, –all = false # 显示所有镜像，默认并不显示中间层镜像（没有标签名的镜像） -f, –filter = [] # 过滤条件 –no-trunc = false # 不使用截断的形式来显示数据(默认使用截断显示EID，比文件名短) -q, –quiet = false # 只显示EID 镜像标签和仓库 镜像仓库 区分： REPOSITORY 仓库 REGISTRY 仓库 REGISTRY里会有很多REPOSITORY仓库，每个REPOSITORY里会有一个个独立的镜像。 标签 TAG 镜像的名字 = 镜像仓库名 : 镜像标签 –对应–&gt; 镜像ID ubuntu:latest, ubuntu:14.04, ….. 如果没有指定标签，默认为latest。 同一仓库的不同标签可以对应同一镜像ID，也就是说可以根据需求给同一镜像文件打上不同的标签。 没有标签名的镜像称作中间层镜像。 查看镜像1docker inspect [OPTIONS] CONTIANER|IMAGE [CONTAINER|IMAGE...] -f, –format=”” 删除镜像1docker rmi [OPTIONS] IMAGE [IMAGE...] -f, –force = false 强制删除 –no-prune = false 不删除未打标签的父镜像 对应多个标签的镜像文件可以直接用ID选定所有标签 1docker rmi ID 获取和推送镜像查找镜像 Docker Hub https://registry.hub.docker.com docker search 1docker search [OPTIONS] TERM –automated = false –no-trunc = false -s, stars = 0 只显示最少多少stars的 最多返回25个结果 拉取镜像1docker pull [OPTIONS] NAME [:TAG] -a, –all-tags = false 下载仓库中所有被标记的镜像 推送镜像1docker push username/IMAGE 构建镜像 保存对容器的修改，并再次使用 自定义镜像的能力 以软件的形式打包并分发服务及其运行环境 docker commit通过容器构建 1docker commit [OPTIONS] CONTAINER [REPOSITORY[:TAG]] -a, –author=”” Author e.g., “John Hannibal Smith hannibal@a-team.com“ -m, –message=”” 记录构建的信息 -p, –pause = true 不暂停容器的运行 docker build通过Dockerfile文件构建 dockerfile: #First DockerfileFROM ubuntu:14.04MAINTAINER dormancypress “dormancypress@outlook.comRUN apt-get updateRUN apt-get install -y nginxEXPOSE 80 1docker build [OPTIONS] PATH|URL|- –force-rm = false –no-cache = false –pull=false -q,–quiet = false –rm = true -t, –tag=””]]></content>
      <categories>
        <category>笔记</category>
        <category>docker</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【notes】docker学习笔记2-docker容器]]></title>
    <url>%2F2019%2F08%2F06%2Fdocker-2%2F</url>
    <content type="text"><![CDATA[容器的基本操作启动容器 仅一次命令 1docker run IMAGE [COMMAND] [ARG] 启动交互式容器 1docker run -i -t IMAGE /bin/bash -i –interactive=ture | false 默认是false，为容器始终打开标准输入-t –tty=true | false 默认是false，分配一个终端 自定义容器名字 1docker run --name=自定义名 -i -t IMAGE /bin/bash 重新启动已建立的容器docker start [-i] 容器名 查看容器 不加参数是正在运行的容器，-a是所有容器，-l是最新创建的一个容器。 1docker ps [-a] [-l] 查看容器参数 1docker inspect [ID] or [name] 删除容器1docker rm 容器名 守护式容器什么是守护式容器： 能够长期运行 没有交互式会话 以守护形式运行容器：12docker run -i -t IMAGE /bin/bashCtrl + P Ctrl + Q 附加到运行中的容器1docker attach 容器名 启动守护式容器1docker run -d 镜像名 [COMMAND] [ARG...] 得知容器运行情况1docker logs [-f] [-t] [--tail] 容器名 -f –follows=true | false 默认为false 一直跟踪日志变化并返回结果-t –timestamps=true | false 默认为false 结果加上时间戳–tail= “all” 多少数量的日志 查看运行中容器进程1docker top 容器名 在运行中的容器内启动新进程1docker exec [-d] [-i] [-t] 容器名 [COMMAND] [ARG...] 停止守护式容器 发送指令等待停止 1docker stop 容器名 直接停止容器 1docker kill 容器名 ###在容器中部署静态网站 设置容器的端口映射run [-P] -P , –publish-all = true | false 默认为false 为容器暴露的所有端口设置映射 1docker run -P -t -i ubuntu /bin/bash -p , 指定端口 容器端口 1docker run -p 80 -i -t ubuntu /bin/bash 宿主机端口:容器端口 1docker run -p 8080:80 -i -t ubuntu /bin/bash ip::容器端口 1docker run -p 0.0.0.0:80 -i -t ubuntu /bin/bash ip:宿主机端口:容器端口 1docker run -p 0.0.0.0:8080:80 -i -t ubuntu /bin/bash Nginx部署 创建映射80端口的交互式容器 1docker run -p 80 --name web -it ubuntu /bin/bash 安装Nginx 安装文本编辑器vim 1234apt-get updateapt-get upgradeapt-get install nginx -yapt-get install vim -y 创建静态页面 12mkdir -p /var/www/htmlvim index.html 修改Nginx配置文件 1vim /etc/nginx/sites-enabled/default 运行Nginx 123nginxps -efCtrl P Ctrl Q 验证网站访问 1234docker port web # 查看端口映射情况docker top web # 查看进程运行情况docker inspect web #查看ipcurl http://172.17.0.2]]></content>
      <categories>
        <category>笔记</category>
        <category>docker</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【notes】docker学习笔记1-docker基本组成]]></title>
    <url>%2F2019%2F08%2F06%2Fdocker-1%2F</url>
    <content type="text"><![CDATA[Docker的基本组成 Docker Client 客户端 Docker Daemon 守护进程 Docker Image 镜像 Docker Container 容器 Docker Registry 仓库 Docker客户端/守护进程 C/S架构 docker客户端对服务器的访问： 本地/远程 docker客户端向发送给守护进程请求，守护进程的执行结果还会传回给客户端。 Docker Image镜像 构建和打包阶段。 容器的基石，相当于保存了容器运行需要的源代码。 层叠的层叠文件系统。 bootfs（引导文件系统）-&gt; rootfs(Ubuntu) -&gt; add emacs -&gt; add Apache 联合加载（union mount）:一次加载多个文件系统（add Apache，add emacs），将所有文件系统叠加在一切。镜像可以叠加在一起，位于底部的成为基础镜像（rootfs），add emacs（副镜像）。 Docker Container容器 通过镜像启动。 启动执行阶段。 配置数据和镜像层（bootfs -&gt; ······ -&gt; add emacs) -&gt; 可写层。 写时复制：docker出现变化时都会应用到可写层，先从只读镜像层复制到可写层然后只读层的文件被隐藏。 Docker Registry仓库 保存docker镜像。 分为公有和私有。公有：Docker Hub 图示结构Docker: Docker Image: Docker Container: docker基本指令 查找镜像 1docker search tutorial 下载镜像 1docker pull learn/tutorial 启动一个容器，使用echo命令输出hello world 1docker run learn/tutorial echo 'hello world' 启动一个容器下载ping 1docker run learn/tutorial apt-get install -y ping 查看有哪些容器 1docker ps -l 提交容器，即创建一个新的镜像 1docker commit [docker ID] learn/ping 用新镜像建立一个容器 1docker run learn/ping ping www.baidu.com 查看容器信息 1docker inspect [docker ID] 查看有哪些镜像 1docker image 将镜像保存到docker hub上 1docker push /learn/ping Docker容器相关技术简介Docker依赖的Linux内核特性 Namespaces 命名空间 提供了系统资源的隔离，for轻量级虚拟化服务 五种命名空间： PID 进程隔离 NET 管理网络接口 IPC 管理跨进程通信的访问 MNT 管理挂载点 UTS 隔离内核和版本标识 Control groups 控制组 资源限制（内存上限等） 优先级设定（设定哪些进程组使用哪些资源） 资源计量 资源控制（挂起恢复） Docker容器的能力 文件系统隔离：每个容器都有自己的root文件系统 进程隔离： 每个容器都运行在自己的进程环境中 网络隔离： 容器间的虚拟网络接口和IP地址都是分开的 资源隔离和分组：使用cgroups将CPU和内存之类的资源独立分配给每个Docker容器]]></content>
      <categories>
        <category>笔记</category>
        <category>docker</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用Xshell管理虚拟机Ubuntu]]></title>
    <url>%2F2019%2F08%2F03%2Fxshell-vmware%2F</url>
    <content type="text"><![CDATA[因为使用VM虚拟机太过占用资源，所以我们可以用Xshell连接到虚拟机，来达到节省本机资源的目的。 安装SSH： 123sudo apt-get install openssh-serverservice iptables stop #关闭防火墙service ssh start #开启ssh服务 获得登录需要的ip ,在虚拟机输入： 1ifconfig ens*后面的inet后面的值就是ip。 按照我之前写过的xshell连接的教程 windows系统：Xshell下载安装+连接服务器 建立会话就ok，主机就是刚才你获得的ip，登录用的用户名和密码就是你安装时填的用户名(非root账户)和密码。 之后只需要打开虚拟机后最小化界面，从xshell登入后reboot一下虚拟机，这样从内存角度就能节省将近90多MB。 注意： reboot后就不要在打开VMware了，一直让它最小化直到关闭。]]></content>
      <categories>
        <category>Xshell</category>
      </categories>
      <tags>
        <tag>Xshell</tag>
        <tag>虚拟机</tag>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[photoshop cc 2019安装破解]]></title>
    <url>%2F2019%2F08%2F02%2Fphotoshop-cc-2019-download%2F</url>
    <content type="text"><![CDATA[Photoshop如今已经非常常用的处理图片的软件，本文就是介绍一下photoshop cc 2019安装破解的完整过程。 注：本文参考了http://www.3322.cc/soft/48343.html 下载creative cloud什么是creative cloud？creative cloud相当于adobe系列的一个应用商城，我们可以在里面安装各种adobe系列的软件。下载链接： 官网链接 网盘链接 下载完成直接按提示安装，然后注册adobe账号并登陆。 下载安装photoshop-cc-2019默认的下载位置在c盘，如果想改到其他盘可以点击右上角的三个点，出来菜单再点首选项。 然后点击creative Cloud界面，在安装位置条目处更改到你想安装到的位置。 打开creative cloud，找到photoshop的条目点击试用，photoshop自动下载安装成功。 利用补丁破解安装完成后安全起见先不要打开ps，我们先下载补丁工具。下载链接：网盘链接 其他链接 将压缩包里的adobe.photoshop.cc.2019.20.0.0-patch.exe文件解压到ps安装目录下，就是你刚才修改的安装位置，保证那个位置下有photoshop.exe文件。 然后点击运行补丁（你可能会听到一段诡异的音乐。。。）。 点击应用，等待出现文件补丁已成功完成的提示。 这样就破解完成了，这时再打开ps发现没有试用还有多少天的提醒了。 按照补丁制作者的建议，在 编辑 ==&gt; 首选项 ==&gt; 常规 ==&gt; 停用”主页”屏幕 打钩。 最后做好重启一下ps再试用。 注：这篇文章是我安装后就写了，我在安装完的七天后再次检验是否失效，如果失效我会更新补丁，如果补丁失效可以回来看我是否有更新方法。]]></content>
      <categories>
        <category>photoshop</category>
      </categories>
      <tags>
        <tag>破解</tag>
        <tag>photoshop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Xshell：在本地浏览器使用服务器的jupyter notebook]]></title>
    <url>%2F2019%2F08%2F02%2Fhexo-jupyter%2F</url>
    <content type="text"><![CDATA[有的服务器里只是命令行，无法可视化，可能就无法使用jupyter notebook。其实需要稍微修改一下连接的属性就能在本地浏览器里打开在服务器里启动的jupyter notebook，具体操作如下： 首先右击会话管理器里的服务器标签，在菜单点击属性。 然后点击左侧的隧道，然后再点击添加。 输入两个端口号，我这输入的是jupyter notebook默认的8888端口，然后点确定 然后再取消右下方转发X11连接到的选项，然后点确定。 之后双击会话管理器里的服务器进行连接，在命令行里输入jupyter notebook，启动后在浏览器里访问就会看到jupyter notebook的界面了。]]></content>
      <categories>
        <category>Xshell</category>
      </categories>
      <tags>
        <tag>Xshell</tag>
        <tag>jupyter notebook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[windows系统：Xshell下载安装+连接服务器]]></title>
    <url>%2F2019%2F08%2F01%2FXshellDownload%2F</url>
    <content type="text"><![CDATA[学习深度学习需要足够的计算资源，往往需要连接远程服务器用来计算。本篇文章就介绍一下如何在windows系统里利用xshell连接服务器。 xshell下载安装首先要下载安装包:百度网盘资源。当然也可以去官网下载安装包，选择家庭学校免费版，下载前要填一下姓名邮箱，提交后你会收到带有下载链接的邮件。 点击安装包，然后一路默认下一步就ok。如果不想安装在c盘也可以，在其他盘里专门存xshell的各种文件，安装过程中只需把主文件夹换成你刚才的文件夹就ok。 建立连接 打开xshell后，点击左上角的文件然后点新建。 然后填入服务器名称、主机、端口号,然后点确定。 双击左侧会话管理器里刚建的服务器，在弹出的窗口里填入登录用的用户名，选上记住用户名。 然后输入密码，并选上记住密码。 点击确定以后就能在黑色的shell看到已经登录成功的提示了，然后就可以在shell里进行操作了。 之后登录只需要双击左侧会话管理器里的对应标签即可。 上传下载文件 在Linux主机上，安装上传下载工具包rz及sz，使用sudo apt install lrzsz 进行安装。 从Windows上传文件，上传命令为rz；输入命令后会弹出选择要上传的本地文件的窗口。 从Linux主机下载文件，下载命令为sz ，后面跟要下载的文件名。例如： sz helloworld.py。 然后就会弹出选择要保存到本机位置的窗口。 xshell的基本操作就说这些了，这些的操作已经基本够用了]]></content>
      <categories>
        <category>Xshell</category>
      </categories>
      <tags>
        <tag>Xshell</tag>
        <tag>linux服务器</tag>
        <tag>windows</tag>
      </tags>
  </entry>
</search>
