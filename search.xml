<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[python爬虫：爬取动态网站]]></title>
    <url>%2F2020%2F01%2F13%2Fdongtaipachong%2F</url>
    <content type="text"><![CDATA[介绍本章的内容重点是学习爬取动态网页。之前爬取的网站是静态的网页，内容都在html的代码中，直接分析提取即可。然而现在不少的网站是动态的，其爬取会更复杂一些。下面是百度百科对动态网页的解释。 简单的说，动态网页是先发送请求获取没有实际我们想要的内容的一个网站的空的基本的框架，再通过后续请求往这个空的框架里填充各种数据和内容，网页需要更新内容时，只需要请求需要更新的具体内容，然后把需要更新的网站的部分替换掉，这样就不需要重新加载整个网页，其内容是可以随着时间变化的。所以之前静态网页的方法只能获得一个空的框架，得不到想要的数据。 对于动态网站的爬取主要可以通过两种方法实现： 利用浏览器的开发者工具分析网站，寻找与需要爬取的目标数据对应的隐藏URL，headers等数据，借助requests模块实现请求，进而爬取到数据。 利用模块selenium模拟浏览器行为，即模拟键盘输入、点击、滑动页面等行为，获得网页完成所有请求后的源代码，再利用静态网页的方法进行后续的处理。 我们学习的重点是第一种方法，学会分析网站可以更高效的实现动态网站的爬取。第二种方法较为麻烦，不如第一种需要学习新的模块，而且还有使用指定的浏览器，下载合适版本的驱动，还需要将内容从源代码里提取出来，而使用第一种方法会发现，往往不需要专门去提取数据，所以第二种方法就不再详细叙述。 动态网页的爬取需要积累经验，一个小的细节就决定了程序是否能实现，所以希望读者能去实战各种网页，积累更多的经验，来应对各种各样的网站。 实例：腾讯天气数据动态网页的爬取以爬取腾讯天气数据的实例来讲解。首先搜索腾讯天气，在浏览器里进入腾讯天气的网站 https://tianqi.qq.com/ ，并打开开发者工具，选择Network，重新加载网页（见下图）。 一般动态网页的后续数据加载的方式为XHR或者JS，有时也可能是Doc。就是图中分类的前两项和第七项。使用谷歌浏览器的开发者工具时如果遇到XHR会有暂停，而这个网站没有出现暂停，所以可以得知后续数据的请求是通过JS实现的。所以选择JS来寻找。 然后要做的就是从这些JS请求中找到请求天气数据的请求。这里要用到一条经验，名字是.js结尾的可以不用看，因为这种文件是网站用来请求JavaScript的代码，是不会有我们需要的数据的。排除掉.js文件后，先点击一个文件，点击选择右侧的Preview，然后继续点击浏览一遍剩余的文件的内容，找出目标数据文件即可。 最后找到了目标文件（如上图所示），请求返回的内容是一个天气数据的一个列表，之后需要做的就是利用这一请求的信息（见下图）来实现一个爬虫，来爬取天气数据，返回一个便于操作的字典数据即可。 重点看一下参数部分：source应该就是请求源，值为PC。weather_type为请求的天气数据类型，比如forecast_1h是逐小时数据，forecast_24h是每天的数据，alarm是一些天气预警等。province就是数据对应位置的省，city就是对应的地级市，county就是对应的县级市，这个就根据需求改变即可。callback的内容就涉及了一个小技巧，我们一般会删去它，它的值就对应了这个请求的响应内容中开头部分，一般为jquery加上一串数字，删去它的值就使得响应只是一个字典格式的字符串，没有了jquery那些多余的东西，我们就可以直接用eval函数将返回的字符串转换成python的字典数据了。_的值是一个叫做时间戳的数据，用来标识请求的时间，这个可以通过time模块中的time函数来调用获取。分析完请求的参数就可以通过代码来实现这个程序了。 这一部分是首先引入必要的模块：requests和time。Location是一个用来储存URL部分参数的字典，Time是储存运行程序时的时间戳。 只够根据需求更改URL的参数的值。 headers是表头的数据，用requests的get函数进行请求，html里是响应的text文本，是一个字典格式的字符串，直接用eval函数将字符串转化成python的字典数据，然后直接用python字典数据的知识提取需要的数据就可以。这样一个简单的动态网页的爬虫就完成了。 这个程序完整的源代码为： 对于这个例子来说，我们不用再分析网页源代码，唯一稍费功夫的地方就是去找到那个隐藏的请求的信息，这个方法就需要读者多去练习，提高分析网页请求的能力。 练习任务： 尝试爬取豆瓣喜剧的排行榜数据。 尝试在百度图片里爬取python相关的图片。 迁移拓展完成一个爬虫的思路主要为： 各种爬虫基本上都是在这四步的基础上进行扩展，比如动态网页就是在第一步获取请求信息中，充分运用开发者工具分析找出关键的隐藏请求，之后的处理基本就与静态的爬虫流程无异。所以，当读者想要实现一些特殊爬虫的时候，就是基于这四步，更改这四步中某几步的细节或方法来迁移性地实现新的类型的爬虫，下面就举几个例子来详细叙述一些。 QQ空间动态爬取QQ空间的好友动态爬取是属于动态网站的爬取。首先，第一步是获取请求的信息，在浏览器上登录你的QQ空间，然后打开开发者工具，然后刷新空间里的动态，然后尝试找到获取动态请求的请求，第一步就完成了。第二步就是根据第一步获得的信息来进行代码实现，这里有一点需要注意，你想拿到的是你QQ好友的动态，所以发送的请求里要有你的账号信息才行，而这一些标识你身份的信息，比如你的账号和密码，都已经隐藏在cookie里，所以说，一般headers里的参数要填全，虽然可能有些参数不填不影响请求，但这需要你分析出那些是不必要的参数，所以说，为了省事还是填全为好。第三、四步就是处理响应内容，这个请求的响应内容一般是一个xml文件，合理解码处理后可以获得一个具有HTML格式的字符串，可以直接用Beautifulsoup模块处理或正则表达式等，这里注意一点，一些没有在某个节点下的文本会被beautifulsoup模块直接放到一个标签下，所以响应下的字符串里即使不全是html格式的字符串也可以直接输入给beautifulsoup函数。 手机app的爬虫对于app的爬虫和常见的动态爬虫主要的差别也就出现在第一步，也就是如何获得请求信息。我们知道，依赖网络的app本质上也是一次次的隐藏起来的请求，动态网页我们有开发者工具这种具有抓包功能的工具，它把各种带有请求数据的数据包抓住复制了一份供我们分析。所以为了实现app的爬虫只需要找到一个能过替代开发者工具的抓包工具，只要把请求的数据包抓取下来，有了请求的信息，剩下的就和动态网页的爬虫没有什么区别了。常见的手机抓包工具有：fiddler，charles等这类抓包工具的使用网上都有教程，这里就不赘述了。比如fiddler教程：https://www.cnblogs.com/hzg1981/p/5610530.html总结一下，基本上绝大部分爬虫基本都可以基于那四步实现，所以当遇到特殊的爬虫，可以根据那四步从其他的之前做过的爬虫中迁移性地想出适合特定的条件的爬虫，做到融会贯通，面对各式各样的网站都能应对自如。 练习： 实现QQ空间动态的爬虫 爬取斗鱼直播app每一个推荐分类下的排名前十的直播间的主播名称、房间号、观看人数、房间名称、排名等信息。 补充知识最后再补充一些读者做爬虫时可能遇到的问题。 中文乱码： 使用代码reponse.encoding = reponse.apparent_encoding 请求头‘Accept-Encoding’中的br问题： 去除br或者用brotl包解码https://www.jianshu.com/p/70c3994efcd8?utm_source=oschina-app 有些网站不能频繁请求，可以通过设置时间间隔解决，代码为：time.sleep(1)。 有的网站会有反爬机制，同一个ip发送请求过多会被封ip，这时可以使用代理 ip，即使用requests.get的proxies参数。 获取免费代理ip的网站：https://www.xicidaili.com/ requests设置代理ip的方法：https://www.cnblogs.com/z-x-y/p/9355223.html]]></content>
      <categories>
        <category>python爬虫</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[翻译：pytorch数据加载和处理]]></title>
    <url>%2F2019%2F09%2F04%2FDATA-LOADING-AND-PROCESSING%2F</url>
    <content type="text"><![CDATA[在解决任何机器学习问题时，都需要花费大量的精力来准备数据。PyTorch提供了许多工具来简化数据加载，希望能使代码更具可读性。在这篇教程里，我们将看看如何从非平凡的数据集中加载和预处理/扩增数据。 为了运行这篇教程，请确保一下模块已经安装了： scikit-image：为了图像的输入输出和转化 pandas：为了跟容易解析csv 123456789101112131415from __future__ import print_function, divisionimport osimport torchimport pandas as pdfrom skimage import io, transformimport numpy as npimport matplotlib.pyplot as pltfrom torch.utils.data import Dataset, DataLoaderfrom torchvision import transforms, utils# 忽略警告import warningswarnings.filterwarnings('ignore')plt.ion() # 交互模式 我们将要处理的数据集是人脸姿态。这以为着一张脸被标注成这样： @import “landmarked_face2.png” 总共有68个标注点标注在每张脸上。 注意： 从这里下载数据集，图片在‘data/faces/’目录下。这个数据集是基于imagenet中被标记为‘face’的一些图片通过应用优秀的dlib的姿态估计生成。 数据集带有一个csv文件，其中带有类似于下面的注释： image_name,part_0_x,part_0_y,part_1_x,part_1_y,part_2_x, ... ,part_67_x,part_67_y 0805personali01.jpg,27,83,27,98, ... 84,134 1084239450_e76e00b7e7.jpg,70,236,71,257, ... ,128,312 让我们快速读取csv并且得到(N, 2)数组的标注，N是指的标注点的个数。 12345678910landmarks_frame = pd.read_csv('data/faces/face_landmarks.csv')n = 65img_name = landmarks_frame.iloc[n, 0]landmarks = landmarks_frame.iloc[n, 1:].as_matrix()landmarks = landmarks.astype('float').reshape(-1, 2)print('Image name: &#123;&#125;'.format(img_name))print('Landmarks shape: &#123;&#125;'.format(landmarks.shape))print('First 4 Landmarks: &#123;&#125;'.format(landmarks[:4])) 输出： Image name: person-7.jpg Landmarks shape: (68, 2) First 4 Landmarks: [[32. 65.] [33. 76.] [34. 86.] [34. 97.]] 让我们写一个简单的展示一张图片和它的标注点的帮助函数，使用它来显示一个样本。 12345678910def show_landmarks(image, landmarks): """显示一张带有标注点的函数""" plt.imshow(image) plt.scatter(landmarks[:, 0], landmarks[:, 1], s = 10, marker = '.', c = 'r') plt.pause(0.001) # 暂停一会等待更新plt.figure()show_landmarks(io.imread(os.path.join('data/faces/', img_name)), landmarks)plt.show() @import “sphx_glr_data_loading_tutorial_001.png” Dataset类torch.utils.data.Dataset是表示数据集的抽象类。你的自定义数据集应该继承Dataset并覆盖以下方法： __len__以便len(dataset)返回dataset的大小 __getitem__来支持索引操作，像是dataset[i]用来获得第i个样本 让我们创建一个我们脸部表注数据集的dataset吧。我们将在__init__中读取csv但是留在__getitem__中读取图片。这是为了内存效率因为所有的图片不是一次储存在内存中，而是按需要储存。 我们数据集的赝本将会是字典{&#39;image&#39;:image, &#39;landmarks&#39;:landmarks}。我们数据集将获得一个选填参数transform以便对样本进行所有必要的处理。我们将在下一个章节看到transform的有效性。 12345678910111213141516171819202122232425262728293031323334class FaceLandmarksDataset(Dataset): def __init__(self, csv_file, root_dir, transform = None): """ 参数： csv_file(string):csv file的路径 root_dir(string):所有图片的目录 transform(callable, 选填):被应用到样本上的transforms """ self.landmarks_frame = pd.read_csv(csv_file) self.root_dir = root_dir self.transform = transform def __len__(self): return len(self.landmarks_frame) def __getitem__(self, idx): if torch.is_tensor(idx): idx = idx.tolist() img_name = os.path.join(self.root_dir, self.landmarks_frame.iloc[idx, 0]) images = io.imread(img_name) landmarks = self.landmarks_frame.iloc[idx, 1:] landmarks = np.array([landmarks]) landmarks = landmarks.astype('float').reshape(-1, 2) sample = &#123;'image':image, 'landmarks':landmarks&#125; if self.transform: sample = self.transform(sample) return sample 让我们举例使用这个类并且在迭代这个数据集。我们将打印出前四个样本的大小和它们的标注点。 123456789101112131415161718face_dataset = FaceLandmarksDataset(csv_file = 'data/faces/face_landmarks.csv', root_dir = 'data/faces/)fig = plt.figure()for i in range(len(face_dataset)): sample = face_dataset[i] print(i, sample['image'].shape, sample['landmarks'].shape) ax = plt.subplot(1, 4, i + 1) plt.tight_layout() ax.set_title('Sample # &#123;&#125;'.format(i)) ax.axis('off') show_landmarks(**sample) if i == 3: plt.show() break]]></content>
      <categories>
        <category>翻译</category>
        <category>pytorch文档（1.2.0）</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>翻译</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[翻译：pytorch官网60分钟教程（1.2.0）]]></title>
    <url>%2F2019%2F09%2F03%2Fpytorch-60-minutes%2F</url>
    <content type="text"><![CDATA[译者博客：https://zinw623.github.io/ 什么是PYTORCH?[原英文网站] 它是基于python的科学计算包，读者定位为两种： 替代使用Numpy来使用GPU的功能 最灵活快速的深度学习研究平台 准备开始TensorsTensors是类似于加上能在GPU上进行加速计算功能的Numpy的ndarrays。 12from __future__ import print_functionimport torch 注意： 未初始化的矩阵被声明，但是在使用前不会包含确切的已知的值。当一个未初始化的矩阵被创建，分配内存中当时的任何值都将作为初始值出现。 构建一个5x3的矩阵，未初始化：12x = torch.empty(5, 3)print(x) 输出: tensor([[0., 0., 0.], [0., 0., 0.], [0., 0., 0.], [0., 0., 0.], [0., 0., 0.]]) 构建一个随机的初始化过的矩阵：12x = torch.rand(5, 3)print(x) 输出： tensor([[0.6259, 0.0797, 0.8297], [0.6732, 0.7944, 0.2363], [0.6775, 0.2497, 0.3846], [0.8515, 0.5171, 0.6957], [0.7759, 0.6000, 0.1323]]) 构建一个dtype为long且用0填充的矩阵： 12x = torch.zeros(5, 3, dtype=torch.long)print(x) 输出： tensor([[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]]) 构建一个直接从data里构建tensor： 12x = torch.tensor([5.5, 3])print(x) 输出： tensor([5.5000, 3.0000]) 或者从已有的tensor创建tensor。这些方法将重用输入tensor的内容，例如dtype，除非使用者提供新的值。 12345x = x.new_ones(5, 3, dtype = torch.double) # new_*方法接受了大小（sizes）print(x)x = torch.randn_like(x, dtype = torch.float) # 重写了dtypeprint(x) # 结构具有相同的size 输出： tensor([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.], [1., 1., 1.], [1., 1., 1.]], dtype=torch.float64) tensor([[ 0.5955, -0.2528, -0.2648], [ 0.7689, 0.2396, -0.0121], [ 1.3478, 0.0460, 0.0255], [ 0.1266, -1.1526, -0.5546], [-0.2001, -0.0542, -0.6439]]) 得到它的size（大小）： 1print(x.size()) 输出： torch.Size([5, 3]) 注意： torch.Size事实上是元组，所以它支持所有的元组操作。 操作有多种操作的语法。在下面的例子里，我们将看一下加法操作。 加法：语法1 12y = torch.rand(5, 3)print(x + y) 输出： tensor([[ 1.1550, 0.5950, -0.0519], [ 1.3954, 0.9232, 0.8904], [ 1.7020, 0.8187, 0.0265], [ 0.3831, -0.6057, -0.2829], [ 0.5647, 0.5976, 0.1128]]) 加法：语法2 1print(torch.add(x, y)) 输出： tensor([[ 1.1550, 0.5950, -0.0519], [ 1.3954, 0.9232, 0.8904], [ 1.7020, 0.8187, 0.0265], [ 0.3831, -0.6057, -0.2829], [ 0.5647, 0.5976, 0.1128]]) 加法：提供一个输出向量作为参数 123result = torch.emtpy(5, 3)torch.add(x, y, out = result)print(result) 输出： tensor([[ 1.1550, 0.5950, -0.0519], [ 1.3954, 0.9232, 0.8904], [ 1.7020, 0.8187, 0.0265], [ 0.3831, -0.6057, -0.2829], [ 0.5647, 0.5976, 0.1128]]) 加法：in-place 12# 把x加到y上y.add_(x) 输出： tensor([[ 1.1550, 0.5950, -0.0519], [ 1.3954, 0.9232, 0.8904], [ 1.7020, 0.8187, 0.0265], [ 0.3831, -0.6057, -0.2829], [ 0.5647, 0.5976, 0.1128]]) 注意： 任何改变张量的in-place操作后固定带一个_。例如：x.copy_(y),x.t_()，将改变x的值。 你可以使用类似于标准的numpy索引的所有附加功能。 1print(x[:, 1]) 输出： tensor([-0.2528, 0.2396, 0.0460, -1.1526, -0.0542]) 改变大小：如果你想resize/reshape张量，你可以使用torch.view: 1234x = torch.randn(4, 4)y = x.view(16)z = x.view(-1, 8) # size -1 从其他维度推断print(x.size(), y.size(), z.size()) 输出： torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8]) 如果你有一个元素的张量，可以使用.item()获得python number的值。 123x = torch.randn(1)print(x)print(x.item()) 输出： tensor([-0.8748]) -0.8748161792755127 然后阅读：100+张量操作，here numpy桥将Torch Tensor转换成numpy array，反之也很简单。 Torch Tensor和numpy array潜在地共享内存（如果torch tensor在CPU上），并且改变一个将会使另一个改变。 将NumPy Array转换成Torch Tensor看如何通过改变np array自动地改变Torch Tensor。 123456import numpy as npa = np.ones(5)b = torch.from_numpy(a)np.add(a, 1, out = a)print(a)print(b) 输出： [2. 2. 2. 2. 2.] tensor([2., 2., 2., 2., 2.], dtype=torch.float64) CUDA张量tensors可以通过.to方法被移动到任何设备。 123456789# 只有CUDA可用时运行这个cell# 我们使用"torch.device"控制张量出入GPUif torch.cuda.is_available(): device = torch.device("cuda") # 一个CUDA device对象 y = torch.ones_like(x, device = device) # 直接在GPU上创建tensor x = x.to(device) # 或者使用字符串".to("cuda")" z = x + y print(z) print(z.to("cpu", torch.double)) # ''.to''也能一起改变dtype 输出： tensor([0.1252], device=&#39;cuda:0&#39;) tensor([0.1252], dtype=torch.float64) AUTOGRAD:自动求导[原英文网站] Pytorch所有神经网络的核心是autograd包。让我们简单地看一下这个，然后我们将要去训练我们的第一个神经网络。 autograd包为所有tensors操作提供自动求导。它是一个定义即运行的框架，这以为着你的代码如何运行你的反向传播就如何被定义，每一次迭代都可以不同。 让我们用更简单的术语和一些例子来看看。 Tensortorch.Tensor是包的核心类。如果你设置了它的属性.requires_grad为True，它开始时会追踪所有作用在它之上的操作。当你完成你的计算时你可以通过调用.backward()并且自动地计算所有梯度。这个张量的梯度将会被累积到.grad这个属性里。 为了组织张量追踪历史，你可以调用.detach()来从计算历史中将它分离，并且防止了在未来计算中被追踪。 为了防止追踪历史（并且使用内存），你也可以将代码块包装到with torch.no_grad():。这在当评估模型时非常有帮助，因为模型可能有requires_grad=True的可训练参数，但是我们并不需要梯度。 为了autograd的执行还有另一个非常重要的类 - Function。 Tensor和Function是相互关联的并且建立一个无环图，这图编码了计算的完整历史。每一个张量都有一个.grad_fn属性，参照了创建这个Tensor的Function（除了被用户创建的张量 - 它们的grad_fn is None）。 如果你想要计算衍生物，你可以调用一个Tensor的.backward()方法。如果Tensor是一个标量（即只有一个元素），你不需要为.backward()指定任何参数，然而如果它有更多的元素，你需要指定一个gradient参数，它是一个匹配形状的张量。 1import torch 创建一个张量并且设置requires_grad=True并追踪它的计算。 12x = torch.ones(2, 2, requires_grad=True)print(x) 输出： tensor([[1., 1.], [1., 1.]], requires_grad=True) 进行一个张量计算： 12y = x + 2print(y) 输出： tensor([[3., 3.], [3., 3.]], grad_fn=&lt;AddBackward0&gt;) y作为一个操作的结果被创建，所以它有一个grad_fn。 1print(y.grad_fn) 输出： &lt;AddBackward0 object at 0x7f3772e36588&gt; 对y做更多的操作 1234z = y * y * 3out = z.mean()print(z, out) 输出： tensor([[27., 27.], [27., 27.]], grad_fn=&lt;MulBackward0&gt;) tensor(27., grad_fn=&lt;MeanBackward0&gt;) .requires_grad_( ... )in-place改变已存在张量的requires_grad=True标示。如果没有给定输入默认的标示是False。 1234567a = torch.randn(2, 2)a = ((a * 3) / (a - 1))print(a.requires_grad)a.requires_grad_(True)print(a.requires_grad)b = (a * a).sum()print(b.grad_fn) 输出： False True &lt;SumBackward0 object at 0x7f3772e36dd8&gt; 梯度现在开始反向传播。因为out包含一个单独的标量，out.backward()等价于out.backward(torch.tensor(1.))。 1out.backward() 打印梯度d(out)/dx 1print(x.grad) 输出： tensor([[4.5000, 4.5000], [4.5000, 4.5000]]) 你应当有一个4.5的矩阵。让我们调用out的张量“o”，我们有$o=\frac{1}{4}\sum_{i}z_i,z_i=3(x_i+2)^2$和$z_i\mid_{x_i=1}=27$。然而，$\frac{\partial{o}}{\partial{x_i}}\mid_{x_i=1}=\frac{9}{2}=4.5$。 数学上的，如果你有一个向量值的函数$\vec{y}=f(\vec{x})$，然后$\vec{y}$关于$\vec{x}$的梯度是雅克比矩阵： J = \left[ \begin{matrix} \frac{\partial{y_1}}{\partial{x_1}} & \cdots &\frac{\partial{y_1}}{\partial{x_n}} \\ \vdots & \ddots & \vdots \\ \frac{\partial{y_m}}{\partial{x_1}} & \cdots & \frac{\partial{y_m}}{\partial{x_n}} \\ \end{matrix} \right]总的来说，torch.autograd是为了计算向量-雅克比积(vector-Jacobian product)。那是给定向量$v=(v_1 v_2 \cdots v_m)^T$，计算积$v^T \cdot J$。如果$v$恰好是标量函数$l=g(\vec{y})$的梯度，就是$v=(\frac{\partial{l}}{\partial{y_1}}\cdots\frac{\partial{l}}{\partial{y_m}})^T$，然后通过链式法则，向量-雅克比积将会成为$l$关于$\vec{x}$的梯度： J^T\cdot v = \left[ \begin{matrix} \frac{\partial{y_1}}{\partial{x_1}} & \cdots &\frac{\partial{y_1}}{\partial{x_n}} \\ \vdots & \ddots & \vdots \\ \frac{\partial{y_m}}{\partial{x_1}} & \cdots & \frac{\partial{y_m}}{\partial{x_n}} \\ \end{matrix} \right] \left[ \begin{matrix} \frac{\partial{l}}{\partial{y_1}} \\ \vdots \\ \frac{\partial{l}}{\partial{y_m}} \end{matrix} \right]= \left[ \begin{matrix} \frac{\partial{l}}{\partial{x_1}} \\ \vdots \\ \frac{\partial{l}}{\partial{x_n}} \end{matrix} \right]（注意：$v^T\cdot J$给出了一个行向量，这个可以通过$J^T\cdot v$得到列向量。） 向量-雅克比积的这个特征使得给一个非标量输出的模型输入一个的外部梯度非常便捷。 现在让我们看看一个向量-雅克比积的例子。 1234567x = torch.randn(3, requires_grad = True)y = x * 2while y.data.norm() &lt; 1000: y = y * 2print(y) 输出： tensor([-1350.9803, 805.9799, -188.3773], grad_fn=&lt;MulBackward0&gt;) 现在y不在是标量，torch.autograd不能直接计算完整的雅克比矩阵，但是如果我们只是想要向量-雅克比积，只需简单地输入一个向量给backward作为参数。 1234v = torch.tensor([0.1, 1.0, 0.0001], dtype = torch.float)y.backward(v)print(x.grad) 输出： tensor([1.0240e+02, 1.0240e+03, 1.0240e-01]) 你也能通过把代码块封装到with torch.no_grad():中来停止autograd追踪带有.requires_grad=True的张量。 12345print(x.requires_grad)print((x ** 2).requires_grad)with torch.no_grad(): print((x ** 2).requires_grad) 输出: True True False 神经网络[原英文网站] 神经网络可以通过torch.nn包来构建。 现在你看一眼autograd，nn依赖于autograd来定义模型并且对他们求导。一个nn.Module包括层（layers)和一个forward(input)方法，这个方法会返回outupt。 例如，看这个分类数字图像的网络： 卷积网络 这是一个简单的前馈神经网络。它接受一个输入，将它输入穿过许多层然后一个接着另一个，然后最后给出一个输出。 一个典型的神经网络的训练流程如下： 定义一个带有许多可学习的参数（或权重）的神经网络。 在输入的训练集上迭代。 通过网络处理输入。 计算损失（loss 输出与正确有多远）。 反向传播求网络的梯度。 更新网络权重，典使用一个简单典型的更新规则：weight = weight - learning_rate * gradient 定义网络让我们来定义网络： 1234567891011121314151617181920212223242526272829303132import torchimport torch.nn as nnimport torch.nn.functional as Fclass Net(nn.Module): def __init__(self): super(Net, self).__init__() # 1是图片通道，6是输出通道，3x3正方形卷积 # 核（kernel） self.conv1 = nn.Conv2d(1, 6, 3) self.conv2 = nn.Conv2d(6, 16, 3) # 一个仿射操作：y = wx + b self.fc1 = nn.Linear(16 * 6 * 6, 128) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): # 在(2, 2)窗口上的最大池化 x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) # 如果是正方形size可以指定一个数 x = F.max_pool2d(F.relu(self.conv2(x)), 2) x = x.view(-1, self.num_flat_feature(x)) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return num_featuresnet = Net()print(net) 输出： Net( (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1)) (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1)) (fc1): Linear(in_features=576, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=84, bias=True) (fc3): Linear(in_features=84, out_features=10, bias=True) ) 你只需要定义一个forward函数，backward函数（计算梯度的函数）会自动使用autograd定义。你在forward函数里使用任何张量操作。 模型的可学习参数通过net.parameters()获得 123params = list(net.parameters())print(len(params))print(params[0].size()) # conv1的权重 输出： 10 torch.Size([6, 1, 3, 3]) 让我们尝试一个随机的32x32的输入。注意：这个网络（LeNet）期望输入的大小是32x32。在MNIST数据集上使用这个网络，请把数据集的图片的大小转换成32x32。 123input = torch.randn(1, 1, 32, 32)out = net(input)print(out) 输出： tensor([[ 0.0057, -0.0952, 0.1055, -0.0916, -0.1350, 0.0857, -0.0890, 0.0326, -0.0554, 0.1451]], grad_fn=&lt;AddmmBackward&gt;) 清零所有参数的梯度的缓冲和用随机的梯度进行反向传播。 12net.zero_grad()out.backward(torch.randn(1, 10)) 注意： torch.nn只支持mini-batches(小的批处理)。整个torch.nn包只支持输入是样本的mini-batch，而不是单一的样本。 例如，nn.Conv2d将接受一个样本数x通道数x高x宽的四维张量。 如果你有一个单一的样本，只需使用input.unsqueeze(0)来增加一个伪batch维度。 在进行下一步之前，让我们重新回顾你目前看过的所有的类。 回顾： torch.Tensor - 一个支持像是backward()的自动求导的多维数组。也保留关于这个张量的梯度。 nn.Module - 神经网络模块。封装参数的便捷方式。帮助他们移动到GPU，导出，加载等。 nn.Parameter - 一种Tensor，当它被分配给一个Module时，它会被自动注册为一个参数。 autograd.Function - 执行前向和一个自动求导操作的反向定义。每个Tensor操作创建至少一个单一的Function节点，与创建Tensor的和编码它的历史的函数。 至此，我们讨论了： 定义一个神经网络 执行了输入和调用反向传播 还剩下： 计算损失 更新网络的权重 损失函数损失函数接受一对输入（output, target)，和计算输出与目标之间距离的值。 在nn包下有很多不同的损失函数。一个简单的损失是：nn.MSELoss,这个是计算输入和目标的均方误差。 例如： 1234567output = net(input)target = torch.randn(10)target = target.view(1, -1)criterion = nn.MSELoss()loss = criterion(output, target)print(loss) 输出： tensor(0.9991, grad_fn=&lt;MseLossBackward&gt;) 现在，如果你使用它的.grad_fn属性沿着loss的方向向后移动，你将看到像这样的计算图： 1234input -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; view -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear -&gt; MSELoss -&gt; loss 所以，当我们调用loss.backward()，整个图对loss求导，并且图中所有有requires_grad=True的张量有一个累积梯度的.grad张量。 为了说明这一点，我们后退几步： 123print(loss.grad_fn) # MSELossprint(loss.grad_fn.next_functions[0][0]) # Linearprint(loss.grad_fn.next_functions[0][0].next_function[0][0]) # Relu 输出： &lt;MseLossBackward object at 0x7ff716c28630&gt; &lt;AddmmBackward object at 0x7ff716c28400&gt; &lt;AccumulateGrad object at 0x7ff716c28400&gt; 反向传播为了反向传播误差我们必须做的全部只是调用loss.backward()。不过，您需要清除现有的梯度，否则梯度将累积为现有梯度。 现在，我们将调用loss.backward()，看一下conv1的偏置梯度在反向之前和之后的区别。 123456789net.zero_grad() # 清零所有参数的梯度缓冲print('conv1.bias.grad before backward')print(net.conv1.bias.grad)loss.backward()print('conv1.bias.grad after backward')print(net.conv1.bias.grad) 输出： conv1.bias.grad before backward tensor([0., 0., 0., 0., 0., 0.]) conv1.bias.grad after backward tensor([ 0.0081, 0.0029, 0.0248, -0.0054, 0.0051, 0.0008]) 现在，我们看如何使用损失函数。 我们剩下需要学习的是： 更新网络的权重 更新参数在实践中使用的最简单的更新规则是随机梯度下降（SGD)。 123learning_rate = 0.01for f in net.parameters(): f.data.sub_(f.grad.data * learning_rate) 然而，当你使用神经网络时，你想要使用不同的更新规则像是SGD，Nesterov-SGD，Adam，RMSProp等等。为了其中这些，你可以使用一个小的包：torch.optim来执行所有这些方法。使用它非常简单： 1234567891011import torch.optim as optim# 创建你的优化器optimizer = optim.SGD(net.parameters(), lr = 0.01)# 在训练循环里optimizer.zero_grad() # 清零梯度缓冲output = net(input)loss = criterion(output, target)loss.backward()optimizer.step() # 更新 注意： 观察到必须是手动地使用optimizer.zero_grad()清零梯度缓冲的。这是因为梯度是累积的，这个在反向传播里解释了。 训练一个分类器[原英文网站] 就是这个。你已经看过如何定义神经网络，计算损失和更新网络的参数。 现在你可能会思考了， 那数据呢？总体来说，当你不得不处理图片，文本，音频，视频数据，你可以使用标准的python库来把它们加载成numpy数组，然后你可以将数组转化成torch.*Tensor。 对于图像，像是Pillow,OpenCV包是有效的 对于音频，有scipy或librosa包 对于文本，原始的python或基于Cython加载，或NLTK和SpaCy都是有效的。 特别的对于视觉方面，我们已经创建了一个叫做torchvision的包，它提供了常见数据集（Imagenet，CIFAR10，MNIST等等）的数据加载器和图片的数据处理器，也就是torchvision.datasets和torch.utils.data.DataLoader。 这提供了很大的便利和避免了编写样本代码。 对于这个教程，我们将使用CIFAR10数据集。它有类别：飞机、汽车、鸟、猫、鹿、狗、青蛙、马、船、卡车。CIFAR-10中的图片都是3x32x32的，也就是32x32像素大小、三颜色通道的图片。 cifar10 训练一个图片分类器我们将依次进行下面的几步： 使用torchvision加载和标准化CIFAR10的训练集和测试集 定义卷积神经网络 定义损失函数 在训练集上训练神经网络 在测试集上测试网络 1.加载和规范化CIFAR10使用torchvision，非常简单地加载CIFAR10 123import torchimport torchvisionimport torchvision.transforms as transforms torchvision datasets输出是在范围[0, 1]的PILImage的图片。我们可以将他们处理成规范化过的范围在[-1, 1]的张量。 1234567891011transform = transforms.Compose( [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])trainset = torchvision.datasets.CIFAR10(root='./data', train = True, download = True, transform=transform)trainloader = torch.utils.data.DataLoader(trainset, batch_size = 4, shuffle=True, num_workers = 2)testset = torchvision.datasets.CIFAR10(root='./data', train = True, download = True, transform = transform)testloader = torch.utils.data.DataLoader(testset, batch_size = 4, shuffle=False, num_workers = 2)classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck') 输出： Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz Extracting ./data/cifar-10-python.tar.gz to ./data Files already downloaded and verified 让我们展示一些训练集图片。 1234567891011121314151617181920import matplotlib.pyplot as pltimport numpy as np# 展示图片的函数def imshow(img): img = img / 2 + 0.5 npimg = img.numpy() plt.imshow(np.transpose(npimg, (1, 2, 0))) plt.show()# 获得一些随机的训练集图片dataiter = iter(trainloader)images, labels = dataiter.next()# 展示图片imshow(torchvision.utils.make_grid(images))# 打印标签print(' '.join('%5s' % classes[labels[j]] for j in range(4))) 输出： frog ship cat plane 2.定义一个卷积神经网络从之前的神经网络章节复制神经网络，并且修改成三通道的图片（它之前定义的是一通道的图片）。 12345678910111213141516171819202122232425import torch.nn as nnimport torch.nn.functional as Fclass Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.pool = nn.MaxPool2d(2, 2) self.conv2 = nn.Conv2d(6, 16, 5) self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = x.view(-1, 16 * 5 * 5, 120) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return xnet = Net() 3.定义损失函数和优化器让我们使用分类交叉熵损失和带有动量的SGD。 1234import torch.optim as optimcriterion = nn.CrossEntropyLoss()optimizer = optim.SGD(net.parameters(), lr = 0.001, momentum = 0.9) 4.训练网络这时事情变得有趣起来。我们简单地循环一下我们的数据迭代器，把输入喂给网络并且做优化。 1234567891011121314151617181920212223for epoch in range(2): # 循环几次数据集 running_loss = 0.0 for i, data in enumerate(trainloader, 0): # 获得输入；数据是[输入，标签]的列表 inputs, labels = data # 清零参数的梯度 optimizer.zero_grad() # 前向+反向+优化 outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() # 打印统计信息 running_loss += loss.item() if i % 2000 == 1999: # 每2000个mini-batches打印一次 print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000)) running_loss = 0.0print('Finished Training') 输出： [1, 2000] loss: 2.169 [1, 4000] loss: 1.808 [1, 6000] loss: 1.659 [1, 8000] loss: 1.553 [1, 10000] loss: 1.488 [1, 12000] loss: 1.455 [2, 2000] loss: 1.379 [2, 4000] loss: 1.346 [2, 6000] loss: 1.320 [2, 8000] loss: 1.305 [2, 10000] loss: 1.275 [2, 12000] loss: 1.262 Finished Training 5.在测试集上测试网络我们已经在训练集上训练了两次网络。但是我们需要检测网络是否有学习。 我们将通过网络预测输出的类别标签检测这个，并且和事实做对比。如果预测是正确的，我们向正确预测的列表里添加该样本。 OK，第一步。让我们从测试集中取出图片来展示熟悉一下。 123456dataiter = iter(testloader)images, labels = dataiter.next()# 打印图片imshow(torch.vision.utils.make_grid(images))print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4))) 输出： GroundTruth: cat ship ship plane ok，现在让我们看看神经网络认为上面的例子是什么： 1output = net(images) 输出是这10类的信念值（energies）。越高信念值的一类，模型越认为图片是这一类。所以，让我们最高信念值的标签： 12_, predicted = torch.max(output, 1)print('Predicted:', ' '.join('%5s' % classes[predicted[j]] for j in range(4))) 输出： Predicted: cat plane plane ship 结果看起来不错。 让我们看一下网络在整个数据集上表现怎么样。 123456789101112correct = 0total = 0with torch.no_grad(): for data in testloader: images, labels = data outputs = net(images) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum().item()print('Accuracy of the network on the 10000 test images: %d %%' % ( 100 * correct / total)) 输出： Accuracy of the network on the 10000 test images: 54 % 看起来比碰运气（10%的准确率，从10类中随机选一类）要好的多。像是网络已经学习了一些东西。 嗯~~~，哪些类表现的好，哪些类表现的不好呢： 1234567891011121314151617class_correct = list(0. for i in range(10))class_total = list(0. for i in range(10))with torch.no_grad(): for data in testloader: images, labels = data outputs = net(images) _, predicted = torch.max(outputs, 1) c = (predicted == labels).squeeze() for i in range(4): label = labels[i] class_correct[label] += c[i].item() class_total[label] += 1for i in range(10): print('Accuracy of %5s : %2d %%' % ( classes[i], 100 * class_correct[i] / class_total[i])) 输出： Accuracy of plane : 46 % Accuracy of car : 63 % Accuracy of bird : 50 % Accuracy of cat : 37 % Accuracy of deer : 40 % Accuracy of dog : 51 % Accuracy of frog : 70 % Accuracy of horse : 48 % Accuracy of ship : 76 % Accuracy of truck : 64 % ok，那下一步干什么？ 我们如何在GPU上运行神经网络？ 在GPU上训练就像如何把张量移动到GPU上，把神经网络移动到GPU上。 如果我们有可用的CUDA， 首先让我们定义设备为第一个可得到的cuda设备。 12345device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")# 假设我们有一个CUDA机器，然后我们打印CUDA设备：print(device) 输出： cuda:0 这个章节的剩余内容都假设device是一个CUDA设备。 然后这些方法将在所有module上递归运行，把它们的参数和缓冲转换成CUDA tensors。 1net.to(device) 记得你也要将每次迭代的输入和标签都送到GPU： 1inputs, labels = data[0].to(device), data[1].to(device) 为什么我没有注意到与CPU相比的巨大加速?因为你的网络真的很小。 练习: 尝试增加网络的宽度（第一个nn.Conv2d的参数2和第二个nn.Conv2d的参数1必须相等），看看你得到怎样的加速。 已完成的目标: 高层次地理解了pytorch张量的库和神经网络 训练了一个小的图片分类的神经网络 在多GPU上训练如果你想使用你全部的GPU看到巨大的加速，请参考下一章节 选读：数据并行[原英文网站] 在这个教程里，我们将使用DataParallel学习如何使用多个GPUS（译者注：一机多卡） pytorch很容易实现多GPU。你能把模型放到一个GPU上： 12device = torch.device("cuda:0")model.to(device) 然后你可以把你的所有数据放到GPU上： 1mytensor = my_tensor.to(device) 请注意只是调用my_tensor.to(device)返回一个新的在GPU上的my_tensor的拷贝，而不是覆盖my_tensor。你需要分配一个新的tensor并使用这个在GPU上的tensor。 在多GPU上执行正向和反向传播是很自然的。然而，pytorch将默认使用一个GPU。你可以使用DataParallel使你的模型并行运行来简单地在多GPU上运行你的操作。 1model = nn.DataParallel(model) 这就是本教程的核心。我们将在下面更详细地探讨它。 引入和参数引入pytorch模块和定义参数。 12345678910import torchimport torch.nn as nnfrom torch.utils.data import Dataset, DataLoader# Parameters and DataLoadersinput_size = 5output_size = 2batch_size = 30data_size = 100 设备： 1device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu") 虚拟数据集建立一个虚拟（随机）的数据集，你只需执行getitem。 1234567891011121314class RandomDataset(Dataset): def __init__(self, size, length): self.len = length self.data = torch.randn(length, size) def __getitem__(self, index): return self.data[index] def __len__(self): return self.lenrand_loader = DataLoader(dataset=RandomDataset(input_size, data_size), batch_size=batch_size, shuffle=True) 简单的模型作为小样，我们的模型只需获得输入，执行线性操作，然后得出输出。然而，你能使用DataParallel在任何模型上(CNN, RNN, Capsule Net等等。) 我们在模型中放置了一个print语句来监视输入和输出张量的大小。请注意批号为0的打印内容。 12345678910111213class Model(nn.Module): # Our model def __init__(self, input_size, output_size): super(Model, self).__init__() self.fc = nn.Linear(input_size, output_size) def forward(self, input): output = self.fc(input) print("\tIn Model: input size", input.size(), "output size", output.size()) return output 创建模型和数据并行这是本教程的核心内容。首先，我们需要创建一个模型实例和检查是否有多个GPU。如果我们有多个GPU，我们可以使用nn.DataParallel封装你的模型。然后我们把模型通过model.to(device)放到GPU上。 1234567model = Model(input_size, output_size)if torch.cuda.device_count() &gt; 1: print("Let's use", torch.cuda.device_count(), "GPUs!") # dim = 0 [30, xxx] -&gt; [10, ...], [10, ...], [10, ...] on 3 GPUs model = nn.DataParallel(model)model.to(device) 输出： Let&#39;s use 2 GPUs! 运行模型现在我们看一下输入和输出张量的大小。 12345for data in rand_loader: input = data.to(device) output = model(input) print("Outside: input size", input.size(), "output_size", output.size()) 输出： In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2]) In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2]) Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2]) 结果如果你没有或一个GPU，当你批处理30个输入，模型会得到30个预期的输出。但是如果你有多GPU，然后你会得到像这样的结果。 2 GPUs如果你有2个，你将看到： Let&#39;s use 2 GPUs! In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2]) In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2]) Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2]) 3GPUs如果你有3个GPU，你将看到： Let&#39;s use 3 GPUs! In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2]) 8GPUs如果你有8个，你将看到： Let&#39;s use 8 GPUs! In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2]) Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2]) 总结DataParallel会自动地分割你的数据，并将工作订单发送到多个gpu上的多个模型。在每个模型完成它们的工作后，DataParallel收集并合并结果，然后返回给你。 更多的信息可以查看https://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html.]]></content>
      <categories>
        <category>翻译</category>
        <category>pytorch文档（1.2.0）</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>翻译</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【翻译】pytorch中文文档（1.2.0）- Package Reference/torch.autograd]]></title>
    <url>%2F2019%2F08%2F28%2Fpytorch-doc-torch-autograd%2F</url>
    <content type="text"><![CDATA[AUTOMATIC DIFFERENTIATION PACKAGE - TORCH.AUTOGRADtorch.autograd提供了许多实现任意标量值函数自动求导的类和函数。它只需要对已有的代码最小的改动 —— 你只需要声明Tensor的关键词requires_grad=True表明梯度需要被计算。 torch.autograd.backward(tensors, grad_tensors=None, retain_graph=None, create_graph=False, grad_variables=None) [源码] 计算所给张量关于图的叶子节点的梯度的总和。图通过链式法则求导。如果tensors是非标量（也就是data是超过一个元素）且需要梯度，然后雅克比向量积（jacobian vector product）将会被计算，这种情况下需要函数另外指定grad_tensors。它应当是一个匹配长度的序列，包含了雅克比向量积里的向量，通常是需要求导的函数关于相应张量的梯度（None对于不需要求导的张量也是可接受的值。这个函数会累积函数到叶子节点，在调用前你可能需要清零。 参数： tensor( tensor序列 ) ：将要被计算导数的tensors grad_tensors( tensor序列或None ) ：雅克比向量积的向量，通常对相应张量的各元素求梯度。对于标量张量或无需求导的张量使用None值。如果一个None值对所有grad_tensors都是可接受的，那么这个参数是可选的。 retain_graph( _bool, 可选 ) ：如果为False，用于计算grad的图将被释放。请注意，几乎在所有情况下，都不需要将此选项设置为True，而且通常可以以更有效的方式解决此问题。默认值为create_graph的值。 create_graph(bool, 可选 ) ：如果为True，导数的图将会被构建，允许计算更高阶的导数。默认值为False。 torch.autograd.grad(output,inputs, grad_outputs=None, retain_grad=None, create_graph=False,only_inputs=True,allow_unused=False) [源码] 计算和返回output关于inputs的梯度的和。grad_outputs应当是长度和output相同的序列，包含了雅克比向量积中的向量，通常需要预计算关于每个outputs的梯度。如果output不需要grad，那么梯度可以是None。如果only_inputs是True，函数将只返回一个关于指定inputs的梯度的列表。如果是False，那么剩余叶子节点的梯度也将会计算，并且将会被累积到它们的.grad属性。 参数： output( tensor序列 ) ：被导函数的输出 inputs( tensor序列 ) ：关于的inputs，梯度会被返回，不会累积到.grad属性 grad_outputs( tensor序列 ) ：雅克比向量积的向量。通常是关于每个输入的梯度。标量张量或不需要梯度的可用None指定。如果None对所有grad_tensors可接受，则此参数可选。默认为None。 retain_graph( _bool, 可选 ) ：如果为False，用于计算grad的图将被释放。请注意，几乎在所有情况下，都不需要将此选项设置为True，而且通常可以以更有效的方式解决此问题。默认值为create_graph的值。 create_graph(bool, 可选) ：如果为True，导数的图将会被构建，允许计算更高阶的导数。默认值为False。 allow_unused( bool, 可选 ) ：如果为False，当计算输出出错时（因此他们的梯度永远是0）指明不使用的inputs。默认为False 局部禁用梯度计算CLASS torch.autograd.no_grad [源码] 禁用梯度计算的上下文管理器 当你确定不会调用 Tensor.backward() ，禁用梯度计算在推断时很有效。它将会减少计算带来的内存消耗。否则requires_grad = True 在这个模式下，即使输入是有requires_grad=True，也会是require_grad=False的计算结果。 当使用enable_grad上下文管理器，这种模式将无效。 这种上下文管理器也可以是管理线程局部，它将会在其他线程失效。 也可以作为装饰器。 例如： 1234567891011&gt;&gt;&gt; x = torch.tensor([1], requires_grad = True)&gt;&gt;&gt; with torch.no_grad():··· y = x * 2&gt;&gt;&gt; y.requires_gradFalse&gt;&gt;&gt; @torch.no_grad()··· def doubler(x):··· return x * 2&gt;&gt;&gt; z = doubler(x)&gt;&gt;&gt; z.requires_gradFalse CLASS torch.autograd.enable_grad [源码] 启动梯度计算的上下文管理器 如果使用no_grad或set_grad_enabled利用过的话，可以使用来启动梯度计算。 这种上下文管理器也可以是管理线程局部，它将会在其他线程失效。 也可以作为装饰器。 例如： 123456789101112131415&gt;&gt;&gt; x = torch.tensor([1], requires_grad = True)&gt;&gt;&gt; with torch.no_grad():··· with torch.enable_grad():··· y = x * 2&gt;&gt;&gt; y.requires_gradTrue&gt;&gt;&gt; y.backward()&gt;&gt;&gt; x.grad&gt;&gt;&gt; @torch.enable_grad()··· def doubler(x):··· return x * 2&gt;&gt;&gt; with torch.no_grad()··· z = doubler(x)&gt;&gt;&gt; z.requires_gradTrue CLASS torch.autograd.set_grad_enabled(mode) 设置是否进行梯度计算的上下文管理器。 set_grad_enabled将启动或禁用梯度，通过mode来确定。它可以被用作上下文管理器也可以是函数。 当使用enable_grad上下文管理器，set_grad_enable(False)将无效。 上下文管理器也可以管理线程局部，它将会在其他线程无效。 参数： mode(bool)：表示决定了梯度计算启动（True）或禁用（False）。这可以条件性地控制梯度计算的使用。 例如： 12345678910&gt;&gt;&gt; x = torch.tensor([1], requires_grad=True)&gt;&gt;&gt; is_train = False&gt;&gt;&gt; with torch.set_grad_enabled(is_train):··· y = x * 2&gt;&gt;&gt; y.requires_gradFalse&gt;&gt;&gt; torch.set_grad_enabled(False)&gt;&gt;&gt; y = x * 2&gt;&gt;&gt; y.requires_gradFalse Tensors的In-place操作在autograd中使用in-place操作是困难的事，并且我们在大多数情况下不鼓励使用。Autograd的缓存区积极地释放和重用非常高效,很少场合in-place操作能明显地降低内存的使用。如果不是你的操作在很大的内存压力下，你可能永远不会使用它们。 In-place正确性检查所有Tensors记录应用于它们的in-place操作，并且如果执行过程检测张量为了反向传播而被保存在某个函数中，但是后来被in-place修改，一旦反向传播开始将会抛出错误。这确保如果你使用了in-place操作而没有看到任何错误，你会确定计算梯度是正确的。 Variable(弃用)WARNING: Variable API被弃用。Variables对张量自动求导没必要。自动求导把Tensors的requires_grad设为True会自动支持。 另外，现在构造用工厂方法像是torch.randn(),torch.zeros,torch.ones()来构造tensors（通过requires_grad=True）,以及类似下面的： 1autograd_tensor = torch.randn((2, 3, 4), requires_grad=True) Tensor autograd 的函数CLASS torch.Tensorbackward(gradient=None, retain_graph=None, create_graph=False) [源码] 计算关于叶子节点的当前张量的梯度。 图是通过链式法则求导。如果张量是非标量（也就是超过一个元素）并且需要梯度，函数需要另外指定gradient。它需要是匹配类型和位置的一个张量，这个张量包含了关于self的梯度。 这个函数会累积梯度到叶子节点 —— 你可能需要在调用它前清零梯度。 参数： gradient( Tensor或None )：关于这个tensor的梯度。如果它是一个张量，除非create_graph为真，它将自动转换成一个不需要grad的张量。对于标量张量或不需要grad的张量，可以指定为None值。如果一个None值是可接受的，那么这个参数是可选的。 retain_graph( _bool, 可选 ) ：如果为False，用于计算grad的图将被释放。请注意，几乎在所有情况下，都不需要将此选项设置为True，而且通常可以以更有效的方式解决此问题。默认值为create_graph的值。 create_graph(bool, 可选 ) ：如果为True，导数的图将会被构建，允许计算更高阶的导数。默认值为False。 detach() 返回一个从当前图分离出的张量。 结果不需要梯度。 注意： 返回的张量将和原来的张量共享内存。In-place对它们之一的修改将会被看到，而且可能触发正确性检查的错误。重要提示:从前，in-place对于size/stride/storage(像是resize_/resize_as_/set_/transpose_)会同时改变返回的向量和原始向量。现在，这种inplace不在会改变原始向量，并且相反会触发错误。并且对于稀疏张量也是如此。 detach_() 从已经创建过的图中分离张量，做成叶子节点。视图（views）不能分离。 grad 这个属性默认是None，当第一次调用backward()会计算self.的梯度变成张量。这个属性将包含被计算的梯度并且未来调用backward()将会累积到其中。 if_leaf 按照惯例所有requires_grad为False的张量都将是叶子节点(leaf)。 对于requires_grad为True的张量，如果它是被使用者创建的它将会也是叶子节点。这意思是它们不是某个操作的结果，所以grad_fn是None。 只有叶子节点的张量在调用backward()时填充它们的grad。为了填充非叶子节点的grad，你可以使用retain_grad()。 例如： 1234567891011121314151617181920212223&gt;&gt;&gt; a = torch.rand(10, requires_grad = True)&gt;&gt;&gt; a.is_leafTrue&gt;&gt;&gt; b = torch.rand(10, requires_grad = True).cuda()&gt;&gt;&gt; b.is_leafFalse# b是被从cpu移入cuda的操作创建的&gt;&gt;&gt; c = torch.rand(10, requires_grad = True) + 2&gt;&gt;&gt; c.is_leafFalse# c被加法操作创建&gt;&gt;&gt; d = torch.rand(10).cuda()&gt;&gt;&gt; d.is_leafTrue# d不需要梯度所以没有操作创建它（被autograd引擎追踪）&gt;&gt;&gt; e = torch.rand(10).cuda().requires_grad()&gt;&gt;&gt; e.is_leafTrue# e需要梯度并且没有操作创建它&gt;&gt;&gt; f = torch.rand(10, requires_grad=True, device = "cuda")&gt;&gt;&gt; f.is_leafTrue# f需要梯度，没有操作创建它 register_hook(hook) [源码] 注册一个反向传播hook 这个hook在每次求关于相应张量的导数时被调用。这个hook应当有下面的签名： hook(grad) -&gt; Tensor or None 这个hook不能修改它的变量，但是它能选择性地返回一个新的梯度，这个新的梯度可以用作代替grad。 这个函数返回了一个handle,通过方法handle.remove()来从模块中移出hook。 例如： 123456789&gt;&gt;&gt; v = torch.tensor([0., 0., 0.], requires_grad = True)&gt;&gt;&gt; h = v.register_hook(lambda grad: grad * 2) # 加倍梯度&gt;&gt;&gt; v.backward(torch.tensor([1., 2., 3.]))&gt;&gt;&gt; v.grad246[torch.FloatTensor of size (3, )] requires_grad 如果需要计算这个Tensor的梯度就设为True，否则为False。 注意： 张量需要计算梯度这一事实并不意味着`grad`属性一定会被填充，详情请看`is_leaf`。 retain_grad() [源码] 启动非叶子节点张量的.grad属性。 函数CLASS torch.autograd.Function [源码]记录操作历史并且定义求导操作的公式。 每一次对Tensor进行计算，就创建一个新的函数对象，这个函数对象执行了计算并且记录了发生了什么。历史以函数的有向无环图（DAG)的形式保留下来，通过边缘来表示依赖关系（input &lt;- output）。然后，当backward被调用时，图按照拓扑排序执行，通过调用每个Function对象的backward()方法，把返回的梯度传递给下一个Functions。 一般，使用者与functions交互的唯一方法就是创建子类和定义新的操作。这个是扩展torch.autograd的推荐方法。 每个函数对象只能使用一次（在一次前向传播中）。 例如： 123456789101112&gt;&gt;&gt; class Exp(Function):&gt;&gt;&gt; &gt;&gt;&gt; @staticmethod&gt;&gt;&gt; def forward(ctx, i):&gt;&gt;&gt; result = i.exp()&gt;&gt;&gt; ctx.save_for_backward(result)&gt;&gt;&gt; return result&gt;&gt;&gt;&gt;&gt;&gt; @staticmethod&gt;&gt;&gt; def backward(ctx, grad_output):&gt;&gt;&gt; result = ctx.saved_tensors&gt;&gt;&gt; return grad_output * result STATIC backward(ctx, *grad_outputs) [源码] 定义为求导计算定义公式 这个函数会被所有子类覆盖。 它必须接受一个上下文ctx作为第一个变量，跟着许多forward()的输出，并且它应当返回与forward()的输入相同的张量。每一个变量都是关于给定输出的梯度，每一个返回的值都应当是相应输入的梯度。 在前向传播时上下文被用作检索被保存的张量。它也有个属性ctx.needs_input_grad是个bool值的元组，表示每个输入是否需要梯度。例如，如果forward()的第一个输入需要计算关于输出的梯度，那么backward()将有ctx.needs_input_grad[0] = True。 STATIC forward(ctx, args, *kwargs) [源码] 执行操作。 这个函数会被所有子类覆盖。 它必须接受一个上下文ctx作为第一个变量，跟着任意数量的变量（tensors或其他类型）。 上下文被用作储存张量，这些张量将会在反向传播里被恢复。 数值梯度检查torch.autograd.gradcheck(func, inputs, eps=1e-06,atol=1e-05,rtol=0.001,raise_exception=True, check_sparse_nnz=False,nondet_tol=0.0) [源码] 通过分析关于张量的梯度的有限差分来检查梯度计算]]></content>
      <categories>
        <category>翻译</category>
        <category>pytorch文档（1.2.0）</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>翻译</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【翻译】pytorch中文文档（1.2.0）- Notes部分]]></title>
    <url>%2F2019%2F08%2F26%2Fpytorch-doc-notes%2F</url>
    <content type="text"><![CDATA[autograd机制 对应的英文版文档https://pytorch.org/docs/stable/notes/autograd.html 这篇笔记将会展示自动求导是如何工作和如何记录操作的概述，没有绝对的必要去理解这些全部内容，但是我们推荐最好熟悉它，因为它会帮助你写出更有效率、更简洁的程序，并且在调试时会帮助到你。 在反向传播（backwards）时排除子图每一个张量都有一个标示：requires_grad，它使得在梯度计算时精细地排除子图并且变得更有效率。 requires_grad如果一个操作仅有一个输入且需要梯度，那么它的输出也需要梯度。相反，只有所有的输入都不需要梯度，输出才会不需要梯度。如果子图中所有的张量都不需要梯度，那么反向传播就不会再其中执行。 12345x = torch.randn(5, 5) # requires_grad = False by defaulty = torch.randn(5, 5) # requires_grad = False by defaultz = torch.randn((5, 5), requires_grad = True)a = x + ya.requires_grad False 当你想要冻结你模型的一部分或者你提前知道你将不使用一些参数的梯度。例如，如果你想要微调一个预训练过的卷积神经网络，将要冻结的部分的requires_grad标示切换就足够了，并且知道计算到最后一层才会被保存到中间缓存区，其中仿射变换将使用需要梯度的权重，并且网络的输出也将会需要它们。 1234567891011model = torchvision.models.resnet18(pretrained = True)for param in model.parameters(): param.requires_grad = False# Replace the last fully-connected layer# Parameters of newly constructed modules have # requires_grad = True by defaultmodel.fc = nn.Linear(512, 100)# Optimize only the classifieroptimizer = optim.SGD(model.fc.parameters(), lr = 1e-2, momentum = 0.9) autograd如何编码历史信息Autograd是反向的自动求导系统。概念上，autograd记录了一张图，这张图记录了所有的操作，当你运行这些操作时它们会产生数据。得到的这张图是一个有向无环图，图的叶子节点是输入张量，根节点是输出张量。通过从根节点到叶子节点跟踪这张图，你可以自动地使用链式法则来计算梯度。 在内部，autograd将会将这张图表示为 Function 对象组成的图（真正的表达），函数可以通过 apply() 来求图的值。当计算前向传播时，autograd同时做到执行请求操作和建立用来表示计算梯度的函数的图（每一个 torch.Tensor 的 .grad_fn 属性是进入这张（用来计算梯度的）图的入口）。当前向传播完成后，我们求出这场图在反向传播时的值来计算梯度。 一件需要注意的重要的事是图在每次迭代时会重新建立图，并且这允许使用任意的python控制语句，即使这些语句每次迭代都会改变图的整个形状和大小。你不必在启动训练之前编写出所有可能的路径-what you run is what you differentiate（你运行什么就会对什么求导）。 autograd的In-place操作在autograd中使用in-place操作是困难的事，并且我们在大多数情况下不鼓励使用。Autograd的缓存区积极地释放和重用非常高效,很少场合in-place操作能明显地降低内存的使用。如果不是你的操作在很大的内存压力下，你可能永远不会使用它们。 对于限制in-place操作的适用范围有两个主要的原因： in-place操作能潜在地覆盖梯度计算所需要的值。 每一个in-place操作确实需要实施重写计算图。out-of-place版本仅是分配新的对象并且保持对旧图的引用。而in-place操作需要把所有输入的creator改为代表这些操作的Function。这会比较棘手，特别是有很多Tensors共享相同的内存（storage）（例如通过索引或转置创建），并且如果被修改的输入的储存（storage）被其他的任何的Tensor引用，那么in-place会抛出错误。 In-place正确性检查每一个tensor都保留一个版本记数器（version counter），当张量在任何操作中被使用后，它每次都会递增。当Function为反向传播保存任何张量时，这些保留的张量的版本计数器也会被保存。一旦你是用self.saved_tensors它将会被检查，并且如果它大于被保存的值将会抛出错误。这确保了如果你是用in-place操作并且没有看到任何操作，你就能确定被计算出的梯度是正确的。 广播语义 对应的英文版文档：https://pytorch.org/docs/stable/notes/broadcasting.html 许多pytorch的操作支持NumPy广播语义。 简而言之，如果pytorch操作支持广播，那么它的张量参数会被自动地扩展成相等的大小（无需复制数据） 一般语义如果满足以下规则，那么两个张量是可广播的： 每个张量至少有一个维度。 当迭代维度的大小时，从末尾（trailing）的维度开始，维度大小必须相等，或者它们中的一个的维度大小为1，或者它们中的一个的维度不存在。例如： 12345678910111213141516171819202122x = torch.empty(5, 7, 3)y = torch.empty(5, 7, 3)# 相同形状的永远是可广播的（上面的规则永远成立）x = torch.empty((0,))y = torch.empty(2, 2)# x和y不是可广播的，因为x没有至少一个维度# 可以排列出的维度x = torch.empty(5, 3, 4, 1)y = torch.empty( 3, 1, 1)# x和y是可广播的# 第一末尾的维度：都是1。# 第二靠近尾部的维度：y是1# 第三靠近尾部的维度：x的大小等于y的大小# 第四靠近尾部的维度：y的该维度不存在# 但是：x = torch.empty(5,2,4,1)y = torch.empty( 3,1,1)# x和y是不可广播的# 因为第三靠近尾部的维度x的2不等于y的3 如果两个张量x，y是“可广播的”，那么结果张量的大小是按照下面的方法计算的： 如果x和y的维度的长度不相等，就在维度个数更少的张量的维度前面加1，使两个张量的维度相等。 然后，对于每个维度的大小，最后得出的结果的维度大小是x和y中维度大小最大的那一个的值。 例如： 1234# 可以列出各维度来使阅读更容易x = torch.empty(5, 1, 4, 1)y = torch.empty( 3, 1, 1)(x + y).size() torch.Size([5, 3, 4, 1]) 1234# 但是没有必要:x = torch.empty(1)y = torch.empty(3, 1, 7)(x + y).size() torch.Size([3, 1, 7]) 123x = torch.empty(5, 2, 4, 1)y = torch.empty(3, 1, 1)(x + y).size() --------------------------------------------------------------------------- RuntimeError Traceback (most recent call last) &lt;ipython-input-9-d19949393c3d&gt; in &lt;module&gt; 1 x = torch.empty(5, 2, 4, 1) 2 y = torch.empty(3, 1, 1) ----&gt; 3 (x + y).size() RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1 In-place语义一个复杂的问题是in-place操作不允许in-place张量由于广播而改变形状。 例如： 123x = torch.empty(5, 3, 4, 1)y = torch.empty(3, 1, 1)(x.add_(y)).size() torch.Size([5, 3, 4, 1]) 1234567# 但是：x = torch.empty(1, 3, 1)y = torch.empty(3, 1, 7)(x.add_(y)).size()# 一般广播会将x的size改成(3, 3, 7)# 但是x为in-place张量 --------------------------------------------------------------------------- RuntimeError Traceback (most recent call last) &lt;ipython-input-16-300828b970da&gt; in &lt;module&gt; 2 x = torch.empty(1, 3, 1) 3 y = torch.empty(3, 1, 7) ----&gt; 4 (x.add_(y)).size() 5 6 # 一般广播会将x的size改成(3, 3, 7) RuntimeError: output with shape [1, 3, 1] doesn&#39;t match the broadcast shape [3, 3, 7] 反向传播兼容性pytorch的较早版本允许在不同形状的张量上执行逐点函数（pointwise functions），只要这些张量的元素数量相等。然后逐点函数将把各张量看作一维的张量然后执行。pytorch现在支持广播而“一维”逐点计算已经被弃用了，并且在张量不可广播但元素数目一样的情况下将生成Python警告。 注意，如果两个张量形状不同，但可广播且元素数目相同，则引入广播会导致反向传播不兼容的变化。 1torch.add(torch.ones(4, 1), torch.randn(4)) tensor([[2.8629, 0.4929, 0.8330, 0.1047], [2.8629, 0.4929, 0.8330, 0.1047], [2.8629, 0.4929, 0.8330, 0.1047], [2.8629, 0.4929, 0.8330, 0.1047]]) 这个例子之前生成size为[4,1]的张量，但是现在生成了一个size为[4,4]的张量。为了帮助识别代码里出现由于广播而导致的反向传播不兼容的情况，可以将torch.utils.backcompat.broadcast_warning设为True，在这种情况下将生成Python警告 1torch.utils.backcompat.broadcast_warning.enable = True 1torch.add(torch.ones(4, 1), torch.ones(4)) tensor([[2., 2., 2., 2.], [2., 2., 2., 2.], [2., 2., 2., 2.], [2., 2., 2., 2.]]) 译者注： 文档里说运行会出现下面这个warning，但是实际运行没用出现，咱也不知道为啥，咱也不知道问谁。 __main__:1: UserWarning: self and other do not have the same shape, but are broadcastable, and have the same number of elements.Changing behavior in a backwards incompatible manner to broadcasting rather than viewing as 1-dimensional. 未完待译。。。]]></content>
      <categories>
        <category>翻译</category>
        <category>pytorch文档（1.2.0）</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>翻译</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【翻译】pytorch中文文档（1.2.0）- Package Reference/torch]]></title>
    <url>%2F2019%2F08%2F26%2Fpytorch-doc-torch%2F</url>
    <content type="text"><![CDATA[TORCHtorch package包含多维张量和定义好的数学运算的数据结构。另外，它提供了许多实用程序用于有效的序列化张量和任意类型，以及其他有用的实用程序。 它支持CUDA环境，使你能在NVIDIA GPU上进行你的张量计算，这要求你compute capability&gt;=3.0。 Tensors _torch.is_tensor(obj)_ [源码] 如果是一个PyTorch tensor返回True _Parameters_ obj(_Object_)-要测试的对象 _torch.is_storage(obj)_ [源码] 如果obj是pytorch storage对象就返回True _Parameters_ obj( _Object_ ) - 要测试的对象 _torch.is_floating_point(input)_ ——&gt; (_bool_) 如果input的数据类型是浮点型数据（即torch.float64, torch.float32和torch.float16中的一种），就返回True。 _Parameters input(_Tensor_) —— 要测试的pytorch张量 _torch.set_default_dtype(d)_ [源码] 把默认的浮点数类型的dtype设为d。这种类型将被用做torch.tensor()类型推断的默认浮点类型。这个默认的浮点dtype是初始为torch.float32 _Parameters_ d(torch.dtype) —— 将被设为默认的浮点型dtype 例如: 12345&gt;&gt;&gt; torch.tensor([1.2, 3]).dtype # 初始默认的浮点型是torch.float32torch.float32&gt;&gt;&gt; torch.set_default_dtype(torch.float64)&gt;&gt;&gt; torch.tensor([1.2, 3]).dtype # 新的浮点型tensortorch.float64 torch.get_default_dtype() ——&gt; torch.dtype 获得当前默认浮点型torch.dtype。 例如： 123456789&gt;&gt;&gt; torch.get_default_dtype() # 初始默认的浮点型为torch.float32torch.float32&gt;&gt;&gt; torch.set_default_dtype(torch.float64)&gt;&gt;&gt; torch.get_default_dtype() # 默认改为torch.float64torch.float64&gt;&gt;&gt; torch.set_default_tensor_type(torch.FloatTensor) # 设置tensor type也会影响这个&gt;&gt;&gt; torch.get_default_dtype() # 改为了对应于torch.FloatTensor对应的dtype：torch.float32torch.float32 torch.set_default_tensor_type( _t_ ) [源码] 设置默认的torch.Tensor类型为浮点型tensor类型t。这种类型也将会被勇作为torch.tensor()类型推断的默认浮点型类型。 默认的浮点型tensor type是初始化为torch.FloatTensor。 _Parameters_ t( _type or string_ ) —— 浮点型tensor type或者它的名字 例如： 12345&gt;&gt;&gt; torch.tensor([1.2, 3]).dtype # 初始的默认浮点型是torch.float32torch.float32&gt;&gt;&gt; torch.set_default_tensor_type(torch.DoubleTensor)&gt;&gt;&gt; torch.tensor([1.2, 3]).dtype # 新的浮点型tensortorch.float64 torch.numel( _input_ ) ——&gt; int 返回input张量里元素的数量 _Parameters_ input( _Tensor_ ) —— 输入的向量 例如： 123456&gt;&gt;&gt; a = torch.randn(1, 2, 3, 4, 5)&gt;&gt;&gt; torch.numel(a)120&gt;&gt;&gt; a = torch.zeros(4, 4)&gt;&gt;&gt; torch.numel(a)16 torch.set_printoptions( _precision=None, threshold=None, edgeitems=None, linewidth=None, profile=None, sci_mode=None_ ) [源码] 为print设置选项。从NumPy中无耻地拿出的项目 _Parameters_ precision —— 输出浮点型精度的位数（默认为4） threshold —— 输出时的阈值，当数组总和超过阈值，会被截断输出（默认为1000） edgeitems —— 每个维度统计的数组条目的数量（默认为3） linewidth —— 为了插入换行符每行设置的字符数（默认为80）。Thresholded矩阵将会忽略这个参数 profile —— Sane为了好的打印的默认设置，可以用以上任何选项覆盖掉（可以是 _default,short,full_ 中任意一个） sci_mode —— 是（True）否（False）使用科学符号。如果指定是None（默认），那么值将会被_Formatter定义。 torch.set_flush_denormal( _mode_ ) ——&gt; bool 禁用CPU上的非规格化的浮点数 如果你的系统支持非规格化数字（flushing denormal numbers）并且成功配置非规格化模式（flush denormal mode）将会返回True。set_flush_denormal()仅使用在支持SSE3的x86架构。 _Parameters_ mode( _boor_ ) —— 控制是否使用非规格化模式（flush denormal mode） 例如： 123456789&gt;&gt;&gt; torch.set_flush_denormal(True)True&gt;&gt;&gt; torch.tensor([1e-323], dtype = torch.float32)tensor([0,], dtype=torch.float64)&gt;&gt;&gt; torch.set_flush_denormal(False)True&gt;&gt;&gt; torch.tensor([1e-323], dtype=torch.float64)tensor(9.88131e-324 * [ 1.0000], dtype=torch.float64) Creation Ops NOTE 随机抽样创建操作被列在Random sampling之下，包括：torch.rand() torch.rand_like() torch.randn_like() torch.randint() torch.randint_like() torch.randperm() 你可能也使用torch.empty()并使用In-place random sampling方法来更广泛的分布范围中取样的值来创建torch.Tensors torch.tensor(data, dtype=None, device=None, requires_grad=False, pin_memory=False) ——&gt; Tensor 通过data构造一个张量 WARNING torch.tensor()总是拷贝data。如果你有一个Tensordata而且想避免拷贝，请使用torch.Tensor.requires_grad_()或torch.Tensor.detach()。如果你有一个NumPyndarray并且向避免拷贝，请使用torch.as_tensor()。 WARNING 当data是一个tensor x，torch.tensor()会读取出“the data”，无论之前传入过什么，并且构建一个leaf variable（）。因此torch.tensor(x)等价于x.clone().detach()，torch.tensor(x, requires_grad = True)等价于x.clone().detach().requires_grad_(True)。对于等价的操作，推荐使用clone()和detach()。 _Parameters_ data( _array_like_ ) —— 为tensor初始化data。可以是list，tuple，Numpy ndarray，scalar，和其他类型。 dtype(torch.dtype, optional) —— 返回tensor期望的数据类型。默认：如果为None，从data中推断数据类型 device(torch.device, optional) —— 返回tensor期望使用的硬件。默认：如果是None，对当前张量类型使用当前硬件（参考torch.set_default_tensor_type())。device可以是提供CPU张量类型的CPU和支持CUDA张量类型的CUDA设备。 requires_grad( _bool_, optional) —— 如果自动求导应当记录返回张量的操作。默认：False pin_memory( _bool_, optional) —— 如果设置，返回的向量分配到锁页内存（pinned memory)。这仅对CPU张量有效。默认：False。例如： 1234567891011121314151617&gt;&gt;&gt; torch.tensor([[0.1, 1.2], [2.2, 3.1], [4.9, 5.2]])tensor([[ 0.1000, 1.2000], [ 2.2000, 3.1000], [ 4.9000, 5.2000]])&gt;&gt;&gt; torch.tensor([0, 1]) # 基于data判断类型tensor([ 0, 1])&gt;&gt;&gt; torch.tensor([[0.11111, 0.222222, 0.3333333]], dtype=torch.float64, device=torch.device('cuda:0')) # 创建一个torch.cuda.DoubleTensortensor([[ 0.1111, 0.2222, 0.3333]], dtype=torch.float64, device='cuda:0')&gt;&gt;&gt; torch.tensor(3.14159) # 创建一个标量tensor(3.1416)&gt;&gt;&gt; torch.tensor([]) # 创建一个空张量（size为(0,))tensor([]) Indexing, Slicing, Joining, Mutating OpsGeneratorsRandom samplingIn-place random samplingQuasi-random samplingSerializationParallelismLocally disabling gradient computationMath operationsPointwise OpsReduction OpsComparision OpsSpectral OpsOther OperationsBLAS and LAPACK OperationsUtilities]]></content>
      <categories>
        <category>翻译</category>
        <category>pytorch文档（1.2.0）</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>翻译</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【notes】pytorch学习笔记4-pytorch常用工具]]></title>
    <url>%2F2019%2F08%2F23%2Fpytorch-tools%2F</url>
    <content type="text"><![CDATA[pytorch中常见的工具在训练神经网络的过程中需要用到很多工具、其中最重要的三部分是数据、可视化和GPU加速。本章主要介绍pytorch在这几方面常用的工具，合理使用这些工具能极大地提高编码效率。 数据处理在解决深度学习问题的过程中，往往需要花费大量的精力去处理数据，包括图像、文本、语音或其他二进制数据等。数据的处理对训练神经网络来说十分重要，良好的数据处理不仅会加速模型训练，也会提高模型效果。 数据加载在pytorch中，数据加载可以通过自定义的数据集对象实现。数据集对象被抽象成Dataset类，实现自定义的数据集需要继承Dataset，并实现两个Python魔法方法。 __getitem__：返回一条数据或一个样本。obj[index]等价于obj.__getitem__(index)。 __len__：返回样本的数量。len(obj)等价于obj.__len__()。 这里我们用Kaggle经典挑战赛“Dogs vs.Cat” 12%env LS_COLORS = None!tree --charset ascii data/dogcat/ env: LS_COLORS=None data/dogcat/ |-- cat.0.jpg |-- cat.1.jpg |-- cat.2.jpg |-- cat.3.jpg |-- cat.4.jpg |-- cat.5.jpg |-- cat.6.jpg |-- dog.0.jpg |-- dog.1.jpg |-- dog.10.jpg |-- dog.1000.jpg `-- dog.10000.jpg 0 directories, 12 files 123import torch as tfrom torch.utils import dataimport matplotlib.pyplot as plt 123import osfrom PIL import Imageimport numpy as np 12345678910111213141516class DogCat(data.Dataset): def __init__(self, root): imgs = os.listdir(root) # 所有图片的绝对路径 self.imgs = [os.path.join(root, img) for img in imgs] def __getitem__(self, index): img_path = self.imgs[index] # dog-&gt;1 cat-&gt;0 label = 1 if 'dog' in img_path.split('/')[-1] else 0 pil_img = Image.open(img_path) array = np.asarray(pil_img) data = t.from_numpy(array) return data, label def __len__(self): return len(self.imgs) 1234dataset = DogCat('./data/dogcat/')img, label = dataset[0]for img, label in dataset: print(img.size(), img.float().mean(), label) torch.Size([375, 499, 3]) tensor(116.7904) 1 torch.Size([499, 327, 3]) tensor(133.5602) 1 torch.Size([144, 175, 3]) tensor(166.6151) 0 torch.Size([292, 269, 3]) tensor(157.4856) 1 torch.Size([375, 499, 3]) tensor(96.8243) 0 torch.Size([375, 499, 3]) tensor(120.7302) 1 torch.Size([280, 300, 3]) tensor(71.6653) 0 torch.Size([396, 312, 3]) tensor(131.8400) 0 torch.Size([303, 400, 3]) tensor(129.1319) 0 torch.Size([374, 500, 3]) tensor(119.7826) 0 torch.Size([412, 263, 3]) tensor(152.9542) 1 torch.Size([414, 500, 3]) tensor(156.6921) 0 通过上面的代码，我们学习了如何自定义自己的数据集，并可以依次获取。但这里返回的数据不适用实际使用，因其具有如下两方面问题： 返回样本的形状之一，每张图片的大小不一样，这对于需要去batch训练的神经网络来说很不友好。 返回样本的数值较大，为归一化至[-1, 1]针对上述问题，pytorch提供了torchvision。它是一个视觉工具包，提供了很多视觉图像处理的工具，其中transforms模块提供了对PIL Image对象和Tensor对象的常用操作。 针对上述问题，pytorch提供torchvision。它是一个视觉工具包，提供了很多视觉图像处理的工具，其中transforms模块提供了对PIL Image对象和Tensor对象的常用操作。对PIL Image的常见操作如下。 Resize：调整图片尺寸 CenterCrop、RandomCrop、RandomSizedCrop：裁剪图片。 Pad：填充。 ToTensor：将PIL Image对象转成Tensor，会自动将[0, 225]归一化至[0, 1]。 对Tensor的常见操作如下。 Normalize：标准化，即减均值，除以标准差。 ToPILImage：将Tensor转为PIL Image对象。 如果要对图片进行多个操作，可通过Compose将这些操作拼接起来，类似于nn.Sequential。注意，这些操作定义后是以对象的形式存在，真正使用时需要调用它的__call__方法，类似于nn.Module。下面利用这些操作来优化上面的dataset。 1234567891011121314151617181920212223242526272829import osfrom PIL import Imageimport numpy as npfrom torchvision import transforms as Ttransform = T.Compose([ T.Resize(224), # 缩放图片（Image），保持长宽比不变，最短边为224像素 T.CenterCrop(224), # 从中间取出224 x224的图片 T.ToTensor(), # 将图片转换成Tensor并归一化至[0, 1] T.Normalize(mean = [.5, .5, .5], std = [.5, .5 , .5]) # 标准化至[-1, 1]])class DogCat(data.Dataset): def __init__(self, root, transforms = None): imgs = os.listdir(root) self.imgs = [os.path.join(root, img) for img in imgs] self.transforms = transforms def __getitem__(self, index): img_path = self.imgs[index] label = 0 if 'dog' in img_path.split('/')[-1] else 1 data = Image.open(img_path) if self.transforms: data = self.transforms(data) return data, label def __len__(self): return len(self.imgs) 12345from torchvision.transforms import ToPILImagedataset = DogCat('./data/dogcat/', transforms = transform)img, label = dataset[0]for img, label in dataset: print(img.size(), label) torch.Size([3, 224, 224]) 0 torch.Size([3, 224, 224]) 0 torch.Size([3, 224, 224]) 1 torch.Size([3, 224, 224]) 0 torch.Size([3, 224, 224]) 1 torch.Size([3, 224, 224]) 0 torch.Size([3, 224, 224]) 1 torch.Size([3, 224, 224]) 1 torch.Size([3, 224, 224]) 1 torch.Size([3, 224, 224]) 1 torch.Size([3, 224, 224]) 0 torch.Size([3, 224, 224]) 1 除了上述操作之外，transforms还可以通过Lambda封装自定义的转换策略。例如，想对PIL Image进行随机旋转，则可写成trans = T.Lambda(lambda img: img.rotate(random() * 360))。 torchvision已经预先实现了常用的Dataset，包括前面使用过的CIFAR-10，以及ImageNet、COCO、MNIST、LSUN等数据集，可通过调用torchvision.datasets下相应对象来调用相关数据集。还有一个经常使用到的Dataset——ImageFolder，它的实现和上述DogCat很相似。ImageFolder假设所有的文件按文件夹保存，每个文件夹下储存同一个类别的图片，文件夹名为类名，其构造函数如下：1ImageFolder(root, transform = None, target_transform = None, loader = default_loader) 它主要有以下四个参数。 root：在root指定的路径下寻找图片。 transform：对PIL Image进行转换操作，transform的输入是使用loader读取图片的返回对象。 target_transform：对label的转换。 loader：指定加载图片的函数，默认操作是读取为PIL Image对象。 label是按照文件夹名顺序排序后存成字典的，即{类名:类序号}，一般来说最好直接将文件夹命名为从0开始的数字，这样会和ImageFolder实际的label一致，如果不是这种命名规范，建议通过self.class_to_idx属性了解label和文件夹名的映射关系。 1!tree --charset ASCII data/dogcat2/ data/dogcat2/ |-- cat | |-- cat.0.jpg | |-- cat.1.jpg | |-- cat.2.jpg | |-- cat.3.jpg | `-- cat.4.jpg `-- dog |-- dog.0.jpg |-- dog.1.jpg |-- dog.10.jpg |-- dog.1000.jpg `-- dog.10000.jpg 2 directories, 10 files 12from torchvision.datasets import ImageFolderdataset = ImageFolder('data/dogcat2/') 1dataset.class_to_idx {&#39;cat&#39;: 0, &#39;dog&#39;: 1} 1dataset.imgs [(&#39;data/dogcat2/cat/cat.0.jpg&#39;, 0), (&#39;data/dogcat2/cat/cat.1.jpg&#39;, 0), (&#39;data/dogcat2/cat/cat.2.jpg&#39;, 0), (&#39;data/dogcat2/cat/cat.3.jpg&#39;, 0), (&#39;data/dogcat2/cat/cat.4.jpg&#39;, 0), (&#39;data/dogcat2/dog/dog.0.jpg&#39;, 1), (&#39;data/dogcat2/dog/dog.1.jpg&#39;, 1), (&#39;data/dogcat2/dog/dog.10.jpg&#39;, 1), (&#39;data/dogcat2/dog/dog.1000.jpg&#39;, 1), (&#39;data/dogcat2/dog/dog.10000.jpg&#39;, 1)] 123# 没有任何transform，所以返回的还是PIL Image对象dataset[0][1] # 第一维是第几张图，第二维为1是label，为0是Image对象dataset[0][0] 12345678# 加上transformnormalize = T.Normalize(mean = [0.4, 0.4, 0.4], std = [0.2, 0.2, 0.2])transform = T.Compose([ T.RandomResizedCrop(224), T.RandomHorizontalFlip(), T.ToTensor(), normalize,]) 1dataset = ImageFolder('data/dogcat2/', transform = transform) 1dataset.class_to_idx 123# 深度学习中图片数据一般保存为CHW，即通道数 x 高 x 宽for img, index in dataset: print(img.size(), index) torch.Size([3, 224, 224]) 0 torch.Size([3, 224, 224]) 0 torch.Size([3, 224, 224]) 0 torch.Size([3, 224, 224]) 0 torch.Size([3, 224, 224]) 0 torch.Size([3, 224, 224]) 1 torch.Size([3, 224, 224]) 1 torch.Size([3, 224, 224]) 1 torch.Size([3, 224, 224]) 1 torch.Size([3, 224, 224]) 1 123to_img = T.ToPILImage()# 0.2和0.4是标准差和均值的近似to_img(dataset[0][0]*0.2 + 0.4) Dataset只负责数据的抽象，一次调用__getattr__只返回一个样本。而训练神经网络是对一个batch的数据进行操作，同时还需要对数据进行shuffle和并行加速等。对此，pytorch提供了DataLoader帮助我们实现这些功能。DataLoader的函数定义如下。1DataLoader(dataset, batch_size = 1, shuffle = False, sampler = None, num_workers = 0, collate_fn = default_collate, pin_memory = False, drop_last = False) dataset：加载的数据集（Dataset对象） batch_size：batch size（批大小） shuffle：是否将数据打乱 sampler：样本抽样，后续会详细介绍。 num_workers：使用多进程加载的进程数，0代表不使用多进程。 collate_fn：如何将多个样本数据拼接成一个batch，一般使用默认的拼接方式即可。 pin_memory：是否将数据保存在pin memory区，pin memory中的数据转到GPU会快一些。 drop_last：dataset中的数据个数可能不是batch_size的整数倍，drop_last为True会将多出来不足一个batch的数据丢失。 1from torch.utils.data import DataLoader 1dataloader = DataLoader(dataset, batch_size = 3, shuffle = True, num_workers = 0, drop_last = False) 123dataiter = iter(dataloader)imgs, labels = next(dataiter)imgs.size() torch.Size([3, 3, 224, 224]) dataloader是一个可迭代对象，我们可以向使用迭代器一样使用它，利用：12for batch_datas, batch_labels in dataloader: train() 或12dataiter = iter(dataloader)batch_datas, batch_labels = next(dataiter) 在数据处理中，有时会出现某个样本无法读取等问题，例如某张图片损坏。这时__getitem__函数中将出现异常，此时最好的解决方案即是将出错的样本剔除。如果遇到这种情况实在无法处理，则可以返回None对象，然后在Dataloader中实现自定义的collate_fn，将空对象过滤掉。但要注意，在这种情况下dataloader返回的一个batch的样本数目会少于batch_size。对丢弃样本异常图片而言，这种做法会更好一些，因为它能保证每个batch样本的数目仍是batch_size。但在大多数情况下，最好的方式还是对数据进行彻底清除。 DataLoader里并没有太多的魔法方法，它封装了python的标准库multiprocessing使其能够实现多进程加速。在Dataset和DataLoader的使用方面有以下建议。 高负载的操作放在__getitem__中，如加载图片 dataset中应尽量只包含只读对象，避免修改任何可变对象。 第一点是因为多进程会并行地调用__getitem__函数，将负载高的放在__getitem__函数中能够实现并行加速。第二点是因为dataloader使用多进程加载，如果在Dataset中使用了可变对象，可能会有意想不到的冲突。在多线程/多进程中，修改一个可变对象需要加锁，但是dataloader的设计使得其很难加锁（在实际使用中也应尽量避免锁的存在）。如果一定要修改可变对象，建议使用python标准库queue使用python multiprocessing库的另一个问题是，在使用多进程时，如果主程序异常终止（比如用“Ctrl+C”快捷键强行退出），相应的数据加载进程可能无法正常退出。这时需要手动强行终止进程。 1ps x | grep &lt;cmdline&gt; | awk '&#123;print $1&#125;' | xargs kill ps x:获取当前用户的所有进程。 grep : 找到已经停止的pytorch程序的进程，例如你是通过python train.py启动的，那就需要些grep ‘python train.py’。 awk ‘{print $1}’：获取进程的pid xargs kill：终止进程，根据需要可能要写成xargs kill -9强制终止进程。 pytorch还单独提供一个sampler模块，用来对数据进行采样。常用的有随机采样器RandomSampler，当dataloader的shuffle参数为True时，就是调用的这个。这里介绍一个很有用的采样方法：WeightedRandomSampler，它会根据每个样本的权重选取数据，在样本比例不均衡的问题中，可用它进行重采样。 构建WeightedRandomSampler时需提供两个参数：每个样本的权重weights、共选取的样本总数num_samples，以及一个可选参数replacement。权重越大的样本被选中的概率越大，待选取的样本数量一般小于全部的样本数目。replacement用于指定是否可以重复选取某一个样本，默认为True，即允许在一个epoch中重复采样某一个数据。如果设为False，则当某一类样本被全部选取完，但其样本数目仍未达到num_samples时，sampler将不会从该类中选取数据，此时可能导致weights参数失效。 1!tree --charset ASCII data/dogcat/ data/dogcat/ |-- cat.0.jpg |-- cat.1.jpg |-- cat.2.jpg |-- cat.5.jpg |-- cat.6.jpg |-- dog.0.jpg |-- dog.1.jpg |-- dog.10.jpg |-- dog.1000.jpg `-- dog.10000.jpg 0 directories, 10 files 123456dataset = DogCat('data/dogcat',transforms = transform)# 狗的图片被取出的概率是猫的概率的两倍# 两类图片被取出的概率与weights的绝对大小无关，只和比值有关weights = [2 if label == 1 else 1 for data, label in dataset]weights [1, 1, 2, 1, 1, 2, 2, 2, 2, 1] 12345from torch.utils.data.sampler import WeightedRandomSamplersampler = WeightedRandomSampler(weights, num_samples=12, replacement=True)dataloader = DataLoader(dataset, batch_size = 3, sampler=sampler)for datas, labels in dataloader: print(labels.tolist()) [0, 0, 1] [0, 1, 1] [1, 1, 1] [0, 0, 0] 一共只有10个样本，却返回了12个，说明样本被重复返回，这就是replacement参数的作用 12345# replacement改为Falsesampler = WeightedRandomSampler(weights, num_samples=10, replacement=False)dataloader = DataLoader(dataset, batch_size = 5, sampler=sampler)for datas, labels in dataloader: print(labels.tolist()) [0, 1, 0, 1, 1] [1, 1, 0, 0, 0] 在这种情况下，num_samples等于dataset的样本总数，为了不重复选取，sampler会将每个样本都返回，这样就失去了weight参数的意义。 从上面的例子可见sampler在样本采样中的作用：如果指定了sampler，shuffle将不再生效，并且sampler.num_samples会覆盖dataset的实际大小，即一个epoch返回的图片总数取决于sampler.num_samples。 计算机视觉工具包：torchvision计算机视觉是深度学习中最重要的一类应用，为了方便研究者使用，pytorch团队专门开发一个视觉工具包torchvision，这个包独立于pytorch。 torchvision主要包含以下三部分。 model：提供深度学习中各种经典网络的网络结构及预训练好的模型，包括AlexNet、VGG系列、ResNet系列、Inception系列等。 datasets：提供常用的数据集加载，设计上都是继承torch.utils.data.Dataset，主要包括MNIST、CIFAR10/100、ImageNet、coco等。 transforms：提供常用的数据预处理操作，主要包括对Tensor及PIL Image对象的操作。 123456789from torchvision import modelsfrom torch import nn# 加载预训练好的模型，如果不存在会下载# 预训练好的模型保存在 ~/.torch/models/下面resnet34 = models.resnet34(pretrained = True,num_classes = 1000)# 修改最后的全连接层为10分类问题（默认是ImageNet上的1000分类）resnet34.fc = nn.Linear(512, 10) Downloading: &quot;https://download.pytorch.org/models/resnet34-333f7ec4.pth&quot; to /root/.cache/torch/checkpoints/resnet34-333f7ec4.pth 100%|██████████| 83.3M/83.3M [00:35&lt;00:00, 2.49MB/s] 123456transform = T.Compose([ T.Resize(32), # 缩放图片（Image），保持长宽比不变，最短边为224像素 T.CenterCrop(32), # 从中间取出224 x224的图片 T.ToTensor(), # 将图片转换成Tensor并归一化至[0, 1] T.Normalize(mean = [.5], std = [.5,]) # 标准化至[-1, 1]]) 1234from torchvision import datasets# 指定数据集路径为data，如果数据集不存在则进行下载# 通过train = False获取测试集dataset = datasets.MNIST('data/',download=True, train = False, transform = transform) torchvision还提供了两个常用的函数。一个是make_grid，它能将多张图片拼接在一个网络中；另一个是save_img，它能将Tensor保存成图片。 1len(dataset) 10000 12345dataloader = DataLoader(dataset, shuffle = True, batch_size = 16)from torchvision.utils import make_grid, save_imagedataiter = iter(dataloader)img = make_grid(next(dataiter)[0], 4) # 拼成4*4网络图片，且会转成3通道to_img(img) 123img2 = make_grid(next(dataiter)[0], 4) # 拼成4*4网络图片，且会转成3通道save_image(img2, 'a.png')Image.open('a.png') 可视化工具visdomvisdom是facebook专门为pytorch开发的一款可视化工具，开源于2017年3月。 visdom可以创造、组织和共享多种数据的可视化，包括数值、图像、文本，甚至是视频，支持pytorch、torch及numpy。用户可通过编程组织可视化空间或通过用户接口为数据打造仪表盘，检查试验结果和调试代码。visdom中有一下两个重要概念。 env：环境。不同环境的可视化结果相互隔离，互不影响，在使用时如果不指定env，默认使用main。不同用户。不同程序一般使用不同的env。 pane：窗格。窗格可用于可视化图像、数值或打印文本等，其可以拖动、缩放、保存和关闭。一个程序可使用同一个env中的不同pane，每个pane可视化或记录某一信息。“clear”按钮可以清空当前env的所有pane，“save”按钮可将当前env保存成json文件，保存路径位于~/.visdom/目录下。修改env的名字后单击fork，可将当前env另存为新文件。 通过命令pip install visdom即可完成visdom的安装。安装完成，须通过python -m visdom.server命令启动visdom服务，或通过nohup python -m visdom.server &amp;命令将服务放至后台运行。visdom服务是一个web server服务，默认绑定8097端口，客户端与服务器间通过tornado进行非阻塞交互。 使用visdom时有两点需要注意的地方。 需手动指定保存env，可在web界面单击save按钮或在程序中调用save方法，否则visdom服务重启后，env等信息会丢失 客户端与服务器之间的交互采用tornado异步框架，可视化操作不会阻塞当前程序，网络异常也不会导致程序退出。visdom以Plotly为基础。 12345import visdomvis = visdom.Visdom(env = u'test1')x = t.arange(1, 30, 0.01)y = t.sin(x)vis.line(X = x, Y = y, win = 'sinx',opts = &#123;'title':'y = sin(x)'&#125;) WARNING:root:Setting up a new session... &#39;sinx&#39; 下面我们逐一分析这几行代码。 vis = visdom.Visdom(env = u’test1’)，用于构建一个客户端，客户端除了制定env外，还可以制定host、post等参数。 vis作为一个客户端对象，可以使用如下常见的画图函数。 line：类似MATLAB中的plot操作，用于记录某些标量的变化，例如损失、标准率等。 image：可视化图片，可以是输入的图片，也可以是GAN生成的图片，还可以是卷积核的信息。 text：用于记录日志等文字信息，支持HTML格式 histgram：可视化分布，主要是查看数据、参数的分布。 scatter：绘制散点图。 bar：绘制柱状图 pie：绘制饼状图 更多参考github主页 visdom同时支持pytorch的tensor和numpy的ndarray两种数据结构，但不支持python的int、float等类型。上述操作的参数一般不同，但有两个参数是绝大多数操作都具备。 win：用于指定pane的名字，如果不指定，visdom将自动分配一个新的pane。如果两次操作指定的win名字一样，新的操作将覆盖当前pane的内容，因此建议每次操作都指定win opts：用来可视化配置，接受一个字典，常见的option包括title、xlabel、ylabel、width等，主要用于设置pane的显示格式。 之前提到过，每次操作会覆盖之前的数据，但我们在训练网络的过程中往往需要不断更新数值，这时就需要指定参数update=’append’来避免覆盖之前的数值。 1234567891011121314import time# append追加数据for ii in range(0, 10): x = t.Tensor([ii]) y = 2 * x time.sleep(0.5) vis.line(X = x, Y= y, win = 'polynomial',name = 'Trace1', update = 'append' if ii &gt; 0 else None) # 新增一条线for ii in np.linspace(0, 5, 50): x = t.Tensor([ii]) y = x ** 2 time.sleep(0.1) vis.line(X = x, Y= y, win = 'polynomial',name = 'Trace2', update = 'append') images的画图功能可分为如下两类。 image接受一个二维或三维向量，HW 或 3H*W ，前者是黑白图像，后者是彩色图像。 images接受一个四维向量NCH*W，C可以是1或3，分别代表黑白和彩色图像。可实现类似torchvision中make_grid的功能，将多张图片拼接在一起。images也可以接受一个二维或三维的向量，此时它所实现的功能与image一致。 12345678# 可视化一张随机的黑白照片vis.image(t.randn(64, 64).numpy())# 可视化一张随机的彩色图片vis.image(t.randn(3, 64, 64).numpy(), win = 'random2')# 可视化36张随机的彩色图片，每行6张vis.images(t.randn(36, 3, 64, 64).numpy(), nrow = 6, win = 'random3',opts = &#123;'title':'random_imgs'&#125;) &#39;random3&#39; 12345x = dataset[0][0].unsqueeze(0)for i in range(1,36): x = t.cat((x, dataset[i][0].unsqueeze(0)), dim = 0)vis.images(x, nrow = 6, win = 'MNIST',opts = &#123;'title':'MNIST数据集'&#125;) &#39;MNIST&#39; vis.text用于可视化文本，它支持所有的html标签，同时也遵循着html的语法标签。 1vis.text(u'''&lt;h1&gt;Hello visdom&lt;/h1&gt;&lt;br&gt;visdom是Facebook专门为&lt;b&gt;pytorch&lt;b/&gt;开发一个可视化工具，''',win = 'visdom',opts = &#123;'title':u'visdom简介'&#125;) &#39;visdom&#39; 1vis.text('ss', win = 'visdom', append = True, opts = &#123;'title':'平方'&#125;) &#39;visdom&#39; 12345# 文本更新vis.text('&lt;b&gt;平方:&lt;/b&gt;', win = 'pingfang',opts = &#123;'title':'pingfang'&#125;)for i in range(1, 20): time.sleep(0.5) vis.text('&lt;b&gt;[info]&#123;&#125;^2=&#123;&#125;&lt;/b&gt;'.format(i, i **2), append = True,win = 'pingfang') GPU略 持久化在pytorch中，以下对象可以持久化到硬盘，并能通过相应的方法加载到内存中。 Tensor Variable nn.Module Optimizer 本质上，上述信息最终都是保存成Tensor。Tensor的保存和加载十分简单，使用t.save和t.load即可完成相应的功能。在save/load时可指定使用pickle模块，在load时还可将GPU tensor映射到CPU或其他GPU上。 我们可以通过t.save(obj, file_name)等方法保存任意可序列化的对象，然后通过obj = t.load(file_name)方法加载保存的数据。 对Module和Optimizer对象，这里建议保存对应的state_dict，而不是直接保存整个Module/Optimizer对象。Optimizer对象保存的是参数即动量信息，通过加载之前的动量信息，能够有效减少模型震荡。 12345678910111213a = t.Tensor(3, 4)if t.cuda.is_available(): a = a.cuda(1)# 把a转为GPU1上的tensor t.save(a, 'a.pth') #加载为b，存储于GPU1上（因为保存时就在GPU1） b = t.load('a.pth') # 加载为c，存储于CPU c = t.load('a.pth', map_location = lambda storage, loc:storage) # 加载为d，存储于GPU0 d = t.load('a.pth', map_location = &#123;'cuda:1':'cuda:0'&#125;) 123from torchvision.models import resnet34model = resnet34() 12# model的state_dict是一个字典list(model.state_dict().keys())[:8] [&#39;conv1.weight&#39;, &#39;bn1.weight&#39;, &#39;bn1.bias&#39;, &#39;bn1.running_mean&#39;, &#39;bn1.running_var&#39;, &#39;bn1.num_batches_tracked&#39;, &#39;layer1.0.conv1.weight&#39;, &#39;layer1.0.bn1.weight&#39;] 123# module对象的保存与加载t.save(model.state_dict(), 'resnet32.pth')model.load_state_dict(t.load('resnet32.pth')) &lt;All keys matched successfully&gt; 1optimizer = t.optim.Adam(model.parameters(), lr = 0.1) 12t.save(optimizer.state_dict(),'optimizer.pth')optimizer.load_state_dict(t.load('optimizer.pth')) 123456all_data = dict( optimizer = optimizer.state_dict(), model = model.state_dict(), info = u'模型和优化器的所有参数')t.save(all_data, 'all_data.pth') 12all_data = t.load('all_data.pth')all_data.keys() dict_keys([&#39;optimizer&#39;, &#39;model&#39;, &#39;info&#39;])]]></content>
      <categories>
        <category>笔记</category>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【notes】pytorch学习笔记3-神经网络工具箱nn]]></title>
    <url>%2F2019%2F08%2F22%2Fnn.Module%2F</url>
    <content type="text"><![CDATA[神经网络工具箱nnautograd实现了自动微分系统，然而对于深度学习来说过于底层。nn模块是构建与autograd之上的神经网络模块。除了nn之外，我们还会介绍神经网络中常用的工具，比如优化器optim,初始化init等。 nn.Moduletorch.nn的核心数据结构是Module，它是一个抽象的概念，既可以表示神经网络中的某个层（layer），也可以表示一个包含很多层的神经网络。在实际使用中，最常见的做法是继承nn.Module，撰写自己的网络、层。 全连接层，又名仿射层，输出$\boldsymbol{x}$和输入$\boldsymbol{x}$满足$\boldsymbol{y=Wx+b}$，$\boldsymbol{W}$ 和 $\boldsymbol{b}$ 是可学习函数。 123import torch as tfrom torch import nnfrom torch.autograd import Variable as V 12345678910class Linear(nn.Module): def __init__(self, in_features, out_features): super(Linear, self).__init__() #等价于nn.Module.__init__(self) self.w = nn.Parameter(t.randn(in_features, out_features)) self.b = nn.Parameter(t.randn(out_features)) def forward(self, x): x = x.mm(self.w) return x + self.b.expand_as(x) 1234layer = Linear(4, 3)input = V(t.randn(2, 4))output = layer(input)output tensor([[ 2.0269, 5.1465, 1.5603], [-0.6868, -0.8096, -0.6427]], grad_fn=&lt;AddBackward0&gt;) 123for name, parameter in layer.named_parameters(): print(name, parameter) w Parameter containing: tensor([[-0.0121, -0.2593, -0.5310], [ 0.2982, -0.2846, -0.0437], [ 0.6220, 1.7351, 0.8025], [ 1.0544, 2.3325, 0.6561]], requires_grad=True) b Parameter containing: tensor([0.2586, 2.3734, 0.5372], requires_grad=True) 但需要注意一下几点： 自定义层Linear必须继承nn.Module，并且在其构造函数中需调用nn.Module的构造函数，即super(Linear, self).init()或nn.Module.__init__(self)。 在构造函数__init__中必须自己定义可学习的参数，并封装成Parameter，如在本例中我们把w和b封装成Parameter。Parameter是一种特殊的Variable，但其默认需要求导（requires_grad=True）。 forward函数实现前向传播过程，其输入可以是一个或多个variable，对x的任何操作也必须是variable支持的操作。 无需写反向传播函数，因其前向传播都是对variable进行操作，nn.Module能够利用autograd自动实现反向传播，这一点比Function简单许多。 使用时，直观上可将layer看成数学概念中的函数，调用layer(input)即可得到input对应的结果。它等价于layers.__call__(input)，在__call__函数中，主要调用的是layer.forward(X)，另外还对钩子做了一些处理。所以在实际使用中应尽量使用layer(x)而不是使用layer.forward(x)。 Module能够自动检测到自己的parameter，并将其作为学习参数。除了parameter，Module还包含子Module，主Module能够递归查找子Module中的parameter。 下面看一个稍微复杂的网络：多层感知机。 123456789class Perceptron(nn.Module): def __init__(self, in_features, hidden_features, out_features): nn.Module.__init__(self) self.layer1 = Linear(in_features, hidden_features) self.layer2 = Linear(hidden_features, out_features) def forward(self, x): x = self.layer1(x) x = t.sigmoid(x) return self.layer2(x) 123perceptron = Perceptron(3, 4, 1)for name, param in perceptron.named_parameters(): print(name, param.size()) layer1.w torch.Size([3, 4]) layer1.b torch.Size([4]) layer2.w torch.Size([4, 1]) layer2.b torch.Size([1]) 注意一下两个知识点。 构造函数__init__中，可利用前面自定义的Linear层（module）作为当前module对象的一个字module，它的可学习参数，也会成为当前module的可学习参数。 在前向传播函数中，我们有意识地将输出变量都命名为x，是为了能让python回收一些中间层的输出，从而节省内存。但并不是所有的中间结果都会被回收，有些variable虽然名字被覆盖，但其在反向传播时仍需要用到，此时python的内存回收模块将通过检查引用计数，不会回收这一部分内存。 module中parameter的全局命名规范如下。 parameter直接命名。例如self.param_name = nn.Parameter(t.randn(3,4))，命名为param_name。 子module中的parameter，会在其名字之前加上当前module的名字，就是sub_module.param_name。 为了方便用户使用，pytorch实现了神经网络中绝大多数的layer，这些layer都继承了nn.Module，封装了可学习参数parameter，并实现了forward函数，且专门针对GPU运算进行了CuDNN优化。具体内容可参考官方文档或在IPython/Jupyter中使用nn.layer。 阅读文档注意： 构造函数的参数，如nn.Linear(in_features, out_features, bias)，需关注着三个参数的作用 属性、可学习参数和子module。如nn.Linear中有weight和bias两个可学习参数，不包含子module 输入输出的形状，如nn.Linear的输入形状是(N, input_features)，输出为(N, output_features)，N是batch_size。若想输入一个数据需要调用unsqueeze(0)函数将数据伪装成batch_size = 1的batch 常用的神经网络层图像相关层图像相关层主要包括卷积层（Conv）、池化层（Pool）等，这些层在实际使用中可分为一维、二维和三维，池化层又分为平均池化（AvgPool）、最大值池化（MaxPool）、自适应池化（AdaptiveAvgPool）等。卷积层除了常用的前向卷积外，还有逆卷积（TransposeConv）。 123456789import torch as tfrom torch import nnfrom torch.autograd import Variable as Vfrom PIL import Imagefrom torchvision.transforms import ToTensor, ToPILImageto_tensor = ToTensor()to_pil = ToPILImage()curry = Image.open('curry')curry 1234567891011# 输入一个batch，batch_size = 1input = to_tensor(curry).unsqueeze(0)# 锐化卷积核kernel = t.ones(3, 3) / -9kernel[1][1] = 1conv = nn.Conv2d(1, 1, (3, 3), 1, bias = False)conv.weight.data = kernel.view(1, 1, 3, 3)out = conv(V(input))to_pil(out.data.squeeze(0)) Shape: Input: $(N, C_{in}, H_{in}, W_{in})$ Output: $(N, C_{out}, H_{out}, W_{out})$ where$H_{out} = \left\lfloor\frac{H_{in} + 2 \times \text{padding}[0] - \text{dilation}[0]\times (\text{kernel_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor$ $W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[1] - \text{dilation}[1]\times (\text{kernel_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor$ 图像的卷积操作还有各种变体，有关各种变体的介绍可以参照此处的介绍https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md 池化层可以看作是一种特殊的卷积层，用来下采样。但池化层没有可学习参数，其weight是固定的。 12pool = nn.AvgPool2d(2, 2)list(pool.parameters()) [] 12out = pool(V(input))to_pil(out.data.squeeze(0)) 除了卷积层和池化层，深度学习中还将常用到一下几个层 Linear：全连接层 BatchNorm：批规范化层，分为1D、2D和3D。除了标准的BatchNorm之外，还有在风格迁移中常用到的InstanceNorm层。 Dropout：dropout层，用来防止过拟合，同样分为1D、2D和3D。 1234# 输入batch_size=2，维度3input = V(t.randn(2, 3))linear = nn.Linear(3, 4)h = linear(input);h tensor([[-0.4360, 0.3433, -0.1978, -0.3128], [-0.9655, 0.6278, 0.2510, 0.1256]], grad_fn=&lt;AddmmBackward&gt;) 12345678910# 4 channel,初始化标准差为4，均值为0bn = nn.BatchNorm1d(4)bn.weight.data = t.ones(4) * 4bn.bias.data = t.zeros(4)bn_out = bn(h)# 注意输出的均值和方差# 方差是标准差的平方，计算无偏方差分母会减1# 使用unbiased=False，分母不减1bn_out.mean(0), bn_out.var(0, unbiased = False) (tensor([-1.1921e-07, 0.0000e+00, 0.0000e+00, 0.0000e+00], grad_fn=&lt;MeanBackward1&gt;), tensor([15.9977, 15.9921, 15.9968, 15.9967], grad_fn=&lt;VarBackward1&gt;)) 123456# 每个元素以0.5的概率舍弃dropout = nn.Dropout(0.5)o = dropout(bn_out)# 有一半的概率会变成0o tensor([[7.9994, -0.0000, -0.0000, -0.0000], [-0.0000, 7.9980, 7.9992, 7.9992]], grad_fn=&lt;MulBackward0&gt;) 以上很多例子中都对module的属性直接操作，其大多数是可学习参数，一般会随着学习的进行而不断改变。实际使用中除非需要使用特殊的初始化，否则尽量不要直接改变参数。 激活函数pytorch实现了常见的激活函数。其他具体的接口信息可参见官方文档，这些激活函数可以作为独立的layer使用。这里介绍最常用的激活函数ReLU，其数学表达式为： ReLU(x)=max(0,x)123456relu = nn.ReLU(inplace = True)input = V(t.randn(2, 3))print(input)output = relu(input)print(output) # 小于0的都被截断为0id(input) == id(output) tensor([[ 0.5049, 0.6093, -0.1565], [-0.9114, -0.9594, 1.0539]]) tensor([[0.5049, 0.6093, 0.0000], [0.0000, 0.0000, 1.0539]]) True ReLU函数有个inplace参数，如果设为True，如果设为True，它会把输出直接覆盖到输入中，这样可以节省内存/显存。之所以可以覆盖是因为在计算ReLU的反向传播时，只需根据输出就能够推算出反向传播的梯度。但是只有少数的autograd操作支持inplace操作（如variable.sigmoid_()），除非你明确知道自己在做什么，否则一般不要使用inplace操作。 在以上例子里，都是将每一层的输出直接作为下一层的输入，这种网络成为前馈传播网络。对于此种网络，如果每次都写复杂的forward函数会有些麻烦，在此就有两种简化方式，ModuleList和Sequential。其中Sequential是一个特殊的Module，它包含几个子module，前向传播时会将输入一层接一层第传递下去。ModuleList也是一个特殊的Module，可以包含几个子Module，可以像用list一样使用它，但不能直接把输入传给ModuleList。 12345678910111213141516171819202122# Sequential的三种写法net1 = nn.Sequential()net1.add_module('conv', nn.Conv2d(3, 3, 3)) # 输入为(N, C_&#123;in&#125;, H_&#123;in&#125;, W_&#123;in&#125;)，参数为net1.add_module('batchnorm', nn.BatchNorm2d(3)) # 3为(N, C, H, W)中的Cnet1.add_module('activation_layer', nn.ReLU())net2 = nn.Sequential( nn.Conv2d(3, 3, 3), nn.BatchNorm2d(3), nn.ReLU() )from collections import OrderedDictnet3 = nn.Sequential(OrderedDict([ ('conv1', nn.Conv2d(3, 3, 3)), ('bn1', nn.BatchNorm2d(3)), ('relu1', nn.ReLU()),]))print('net1', net1)print('net2', net2)print('net3', net3) net1 Sequential( (conv): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1)) (batchnorm): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (activation_layer): ReLU() ) net2 Sequential( (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1)) (1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) net3 Sequential( (conv1): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1)) (bn1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU() ) 12# 可根据名字或序号取出子modulenet1.conv, net2[0], net3.conv1 (Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1)), Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1)), Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))) 12input = V(t.rand(1, 3, 4, 4))net1(input), net2(input), net3(input), net3.relu1(net2[1](net1.conv(input))) (tensor([[[[0.0000, 1.4727], [0.1600, 0.0000]], [[0.0000, 0.0000], [0.7015, 1.1069]], [[1.7189, 0.0000], [0.0000, 0.0000]]]], grad_fn=&lt;ReluBackward0&gt;), tensor([[[[0.0000, 1.6957], [0.0000, 0.0000]], [[1.2454, 0.6350], [0.0000, 0.0000]], [[1.0204, 0.4811], [0.1430, 0.0000]]]], grad_fn=&lt;ReluBackward0&gt;), tensor([[[[0.0000, 1.5585], [0.1751, 0.0000]], [[0.0000, 1.4177], [0.1846, 0.0000]], [[0.0000, 0.0000], [1.3537, 0.2417]]]], grad_fn=&lt;ReluBackward0&gt;), tensor([[[[0.0000, 1.4727], [0.1600, 0.0000]], [[0.0000, 0.0000], [0.7015, 1.1069]], [[1.7189, 0.0000], [0.0000, 0.0000]]]], grad_fn=&lt;ReluBackward0&gt;)) 1234567modulelist = nn.ModuleList([nn.Linear(3, 4), nn.ReLU(), nn.Linear(4, 2)])input = V(t.rand(1, 3))for model in modulelist: input = model(input)# 下面会报错，因为modellist没有实现forward方法# output = modellist(input) 为何不直接使用python中自带的list，而非要多次一举呢？这是因为ModuleList是Module的子类，当在Module中使用它时，就能自动识别为子module。 1234567891011class MyModule(nn.Module): def __init__(self): super(MyModule, self).__init__() self.list = [nn.Linear(3,4), nn.ReLU()] # 直接用list self.module_list = nn.ModuleList([nn.Conv2d(3, 3, 3), nn.ReLU()]) # 用nn.ModuleList def forward(self): pass model = MyModule()model MyModule( (module_list): ModuleList( (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1)) (1): ReLU() ) ) 123for name, param in model.named_parameters(): print(name, param.size()) module_list.0.weight torch.Size([3, 3, 3, 3]) module_list.0.bias torch.Size([3]) 可见，list中的子module并不能被主module识别，而ModuleList中的子module能够被主module识别。 除ModuleList之外还有ParameterList，它是一个可以包含多个parameter的类list对象。在实际应用中，使用方式和ModuleList类似。在构造函数__init__中用到list、tuple、dict等对象，一定要思考是否应该用ModuleList或ParameterList代替。 损失函数在深度学习中会用到各种各样的损失函数，这些损失函数可看作是一种特殊的layer，pytorch也将这些损失函数实现为nn.Module的子类。然而在实际使用中通常将这些损失函数专门提取出来，作为独立的一部分。 123456789#batch_size = 3, 计算对应每个类别的分数score = V(t.randn(3, 10)) # (N, C) N是batch_size，C是class的个数# 三个样本分别属于1， 0， 1类，label必须是LongTensorlabel = V(t.Tensor([1, 0, 9])).long()# loss与普通的layer无差异criterion = nn.CrossEntropyLoss()loss = criterion(score, label)loss tensor(2.8392) 优化器pytorch将深度学习中常用的优化方法全部封装到torch.optim中，其设计十分灵活，能够很方便地扩展自定义的优化方法。所有的优化方法都是继承基类optim.Optimizer，并实现了自己的优化步骤。下面就以最基本的优化方法————随机梯度下降法（SGD）举例说明。这里需要重点掌握： 优化方法的基本使用方法 如何对模型的不同部分设置不同的学习率 如何调整学习率 12345678910111213141516171819202122232425262728# 首先定义一个LeNet网络class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.features = nn.Sequential( nn.Conv2d(3, 6, 5), nn.ReLU(), nn.MaxPool2d(2, 2), nn.Conv2d(6, 16, 5), nn.ReLU(), nn.MaxPool2d(2, 2) ) self.classifier = nn.Sequential( nn.Linear(16 * 5 * 5, 120), nn.ReLU(), nn.Linear(120, 84), nn.ReLU(), nn.Linear(84, 10), ) def forward(self, x): x = self.features(x) x = x.view(-1, 16 * 5 * 5) x = self.classifier(x) return xnet = Net() 123456789from torch import optimoptimizer = optim.SGD(params = net.parameters(), lr = 1)optimizer.zero_grad() # 梯度清零，等价于net.zero_grad()input = V(t.randn(1, 3, 32, 32))output = net(input)output.backward(output) # fake backwardoptimizer.step() # 执行优化 123# 为不同子网络参数设置不同的学习率，在finetune中经常用到# 如果对某个参数不指定学习率，就是用默认学习率optimizer = optim.SGD([&#123;'params':net.features.parameters()&#125;,&#123;'params':net.classifier.parameters(), 'lr':1e-2&#125;], lr = 1e-5) 123456789# 只为两个全连接层设置较大的学习率，其余层的学习率较小special_layers = nn.ModuleList([net.classifier[0],net.classifier[2]])special_layers_params = list(map(id, special_layers.parameters()))base_params = filter(lambda p: id(p) not in special_layers_params, net.parameters())optimizer = t.optim.SGD([ &#123;'params':base_params&#125;, &#123;'params':special_layers.parameters(), 'lr': 0.01&#125;], lr = 0.001) 调整学习率主要有两种做法。一种是修改optimmizer.param_groups中对应的学习率。另一种是新建优化器（更简单也是更推荐的做法），由于optimizer十分轻量级，构建开销很小，故可以构建新的optimizer。但是新建优化器会重新初始化动量等状态信息，这对使用动量的优化器来说（如带momentum的sgd），可能会造成损失函数在收敛过程中出震荡。 123456# 调整学习率，新建一个optimizerold_lr = 0.1optimizer = optim.SGD([ &#123;'params': net.features.parameters()&#125;, &#123;'params': net.classifier.parameters(), 'lr':old_lr *0.1&#125;],lr = 1e-5) nn.functionalnn中还有一个常用的模块：nn.functional。nn中的大多数layer在functional中都有一个与之相对应的函数。nn.functional中的函数和nn.Module的主要区别在于，用nn.Module实现的layers是一个特殊的类，都是由class Layer(nn.Module)定义，会自动提取可学习的参数；而nn.functional中的函数更像是纯函数，由def function(input)定义。 12345input = V(t.randn(2, 3))model = nn.Linear(3, 4)output1 = model(input)output2 = nn.functional.linear(input, model.weight, model.bias)output1 == output2 tensor([[True, True, True, True], [True, True, True, True]]) 123b = nn.functional.relu(input)b2 = nn.ReLU()(input)b == b2 tensor([[True, True, True], [True, True, True]]) 应该什么时候使用nn.Module，什么时候使用nn.functional？如果模型有可学习的参数，最好用nn.Module，否则既可以使用nn.functional也可以使用nn.Module，二者在性能上没有太大差异，具体的使用方式取决于个人喜好。但建议还是使用nn.Dropout而不是nn.functional.dropout，因为dropout在训练和测试两个阶段的行为有所差别，使用nn.Module对象能够通过model.eval操作以一区分。 1234567891011121314151617from torch.nn import functional as Fclass Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.conv2 = nn.Conv2d(6, 16, 5) self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): x = F.max_pool2d(F.relu(self.conv1(x)), 2) x = F.max_pool2d(F.relu(self.conv2(x)), 2) x = x.view(-1, 16 * 5 * 5) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x 123net = Net()x = t.randn(1, 3, 32, 32)net(x) tensor([[-0.0539, 0.0145, 0.0214, 0.0474, -0.0516, 0.0890, 0.0539, 0.0805, 0.0785, -0.1043]], grad_fn=&lt;AddmmBackward&gt;) 不具备可学习参数的层（激活层、池化层等），将它们用函数代替，这样可以不用放置在构造函数__init__中。有可学习参数的模块，也可以用functional代替，只不过实现起来较繁琐，需要手动定义参数parameter。 123456789class MyLinear(nn.Module): def __init__(self): super(MyLinear, self).__init__() self.weight = nn.Parameter(t.randn(3, 4)) self.bias = nn.Parameter(t.zeros(3)) def forward(self,input): return F.linear(input, self.weight, self.bias) 123x = t.randn(1, 4)linear = MyLinear()linear(x) tensor([[-0.0678, 2.5530, 0.8512]], grad_fn=&lt;AddmmBackward&gt;) 初始化策略在深度学习中参数的初始化十分重要，良好的初始化能使模型收敛更快，并达到更高水平，而糟糕的初始化可能使模型迅速崩溃。pytorch中nn.Module的模块参数都采取了较合理的初始化策略，因此一般不用我们考虑。当然我们可以用自定义的初始化代替系统的默认初始化。自定义初始化尤为重要，因为t.Tensor()返回的是内存中的随机数，很可能会有极大值，这在实际训练网络中会造成溢出或者梯度消失。pytorch中的nn.init模块专门为初始化设计，实现了常用的初始化策略。如果某种初始化策略nn.init不提供，用户也可以自己直接初始化。 123456# 利用nn.init初始化from torch.nn import initlinear = nn.Linear(3, 4)t.manual_seed(1)# 等价于linear.weight.data.normal_(0, std)init.xavier_normal_(linear.weight) Parameter containing: tensor([[ 0.3535, 0.1427, 0.0330], [ 0.3321, -0.2416, -0.0888], [-0.8140, 0.2040, -0.5493], [-0.3010, -0.4769, -0.0311]], requires_grad=True) 12345import matht.manual_seed(1)std = math.sqrt(2)/math.sqrt(7.)linear.weight.data.normal_(0, std) tensor([[ 0.3535, 0.1427, 0.0330], [ 0.3321, -0.2416, -0.0888], [-0.8140, 0.2040, -0.5493], [-0.3010, -0.4769, -0.0311]]) 12345678910# 对模型的所有参数进行初始化for name, params in net.named_parameters(): if name.find('linear') != -1: print(params[0]) # weight print(params[1]) # bias elif name.find('conv') != -1: pass elif name.find('norm') != -1: pass nn.Module深入分析如果想深入地理解nn.Module，研究其原理是很有必要的。首先来看看nn.Module基类的构造函数的源代码： 1234567def __init__(self): self._parameters = OrderedDict() self._modules = OrderedDict() self._buffers = OrderedDict() self._backward_hooks = OrderedDict() self._forward_hooks = OrderedDict() self.training = True 其中每个属性的解释如下： _parameters：字典。保存用户直接设置的parameter，self.param1 = nn.Parameter(t.randn(3, 3))会被检测到，在字典中加入一个key为param，value为对应parameter的item，而self.submodule = nn.Linear(3, 4)中的parameter则不会存于此。 _modules：子module。通过self.submodule = nn.Linear(3, 4)指定的子module会保存于此。 _buffers：缓存。如batchnorm使用momentum机制，每次前向传播需用到上一次前向传播的结果。 _backward_hooks与_forward_hooks：钩子技术，用来提取中间变量，类似variable的hook training：BatchNorm与Dropout层在训练阶段和测试阶段中采取的策略不同，通过判断training值决定前向传播策略。 上述几个属性中，_parameters、_modules和_buffers这三个字典中的键值，都可以通过self.key方式获得，效果等价于self._parameters[‘key’] 123456789101112class Net(nn.Module): def __init__(self): super(Net, self).__init__() # 等价于self.register_parameter('param1', nn.Parameter(t.randn(3, 3))) self.param1 = nn.Parameter(t.rand(3, 3)) self.submodel1 = nn.Linear(3, 4) def foward(self, input): x = self.param1 * input x = self.submodel1(x) return xnet = Net()net Net( (submodel1): Linear(in_features=3, out_features=4, bias=True) ) 1net._modules OrderedDict([(&#39;submodel1&#39;, Linear(in_features=3, out_features=4, bias=True))]) 1net._parameters OrderedDict([(&#39;param1&#39;, Parameter containing: tensor([[0.3398, 0.5239, 0.7981], [0.7718, 0.0112, 0.8100], [0.6397, 0.9743, 0.8300]], requires_grad=True))]) 1net.param1 == net._parameters['param1'] tensor([[True, True, True], [True, True, True], [True, True, True]]) 12for name, param in net.named_parameters(): print(name, param.size()) param1 torch.Size([3, 3]) submodel1.weight torch.Size([4, 3]) submodel1.bias torch.Size([4]) 12for name, submodel in net.named_modules(): print(name, submodel) Net( (submodel1): Linear(in_features=3, out_features=4, bias=True) ) submodel1 Linear(in_features=3, out_features=4, bias=True) 1234bn = nn.BatchNorm1d(2)input = V(t.rand(3,2), requires_grad = True)output = bn(input)bn._buffers OrderedDict([(&#39;running_mean&#39;, tensor([0.0362, 0.0596])), (&#39;running_var&#39;, tensor([0.9009, 0.9262])), (&#39;num_batches_tracked&#39;, tensor(1))]) nn.Module在实际使用中可能层层嵌套，一个module包含若干个子module，每一个子module又包含了更多的子module。为了方便用户访问各个子module，nn.Module实现了很多方法，如函数children可以查看直接子module，函数modules可以查看所有的子module（包括当前module）。与之相对应的还有函数named_children和named_modules，其能够在返回module列表的同时返回它们的名字。 123input = V(t.arange(0, 12).view(3, 4).float())model = nn.Dropout()model(input) tensor([[ 0., 0., 0., 0.], [ 0., 0., 12., 0.], [ 0., 18., 0., 0.]]) 12model.training = Falsemodel(input) tensor([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.]]) 对batchnorm、dropout、instancenorm等在训练和测试阶段行为差距较大的层，如果在测试时不将其training值设为False，则可能会有很大影响，这在实际使用中千万注意。虽然可通过直接设置training属性将子module设为train和eval模式，但是这种方式比较繁琐。推荐的做法是调用model.train()函数，它会将当前module及其子module中的所有training属性都设为True。model.eval()函数会把training属性都设为False。 123print(net.training, net.submodel1.training)net.eval()net.training, net.submodel1.training True True (False, False) 1list(net.named_modules()) [(&#39;&#39;, Net( (submodel1): Linear(in_features=3, out_features=4, bias=True) )), (&#39;submodel1&#39;, Linear(in_features=3, out_features=4, bias=True))] register_forward_hook和register_backward_hook函数的功能类似于variable的register_hook，可在module前向传播或反向传播时注册钩子。每次前向传播执行结束后会执行钩子函数（hook）。前向传播的钩子函数具有如下形式：hook(module, input, output) -&gt; None，而反向传播则具有如下形式：hook(module, grad_input, grad_ouput) -&gt; Tensor or None。钩子函数不应修改输入和输出，并且在使用后应及时删除，以避免每次都运行钩子增加运行负载。钩子函数主要用在获取某些中间结果的情景，如中间某一层的输出或某一层的梯度。这些结果本应写在forward函数中，但如果在forward函数中加上这些处理，可能会使处理逻辑比较复杂，这时使用钩子技术就更合适。下面考虑一种场景：有一个预训练的模型，需要提取模型的某一层（不是最后一层）的输出作为特征进行分类，希望不修改其原有的模型定义文件，这时就可以利用钩子函数。 123456789model = VGG()features = t.Tensor()def hook(module, input, output): features.copy_(output.data) handle = model.layer8.register_forward_hook(hook)_ = model(input)# 用完hook后删除handle.remove() nn.Module对象在构造函数中的行为看起来有些诡异，想要理解就需要看两个魔法方法__getattr__和__setattr__。在python中有两个常用的builtin方法：getattr和setattr。getattr(obj, ‘attr1’)等价于obj.attr，setattr(obj, ‘name’, value)等价于obj.name = value。 result = obj.name会调用builtin函数getattr(obj, ‘name’)，如果该属性找不到，会调用obj.__getattr__(‘name’) obj.name = value会调用builtin函数setattr(obj, ‘name’, value)，如果obj对象实现了__setattr__方法，setattr会直接调用obj.__setattr__(‘name’, value)。 1234567891011class person(): dict = &#123;'name':'xxx','sex':'boy','age':18&#125; def __getattr__(self,name): return self.dict[name] def __setattr__(self, name , value): self.dict[name] = value return value 12one = person()one.name, one.sex, one.age (&#39;xxx&#39;, &#39;boy&#39;, 18) 1one.name = '吴彦祖';one.name &#39;吴彦祖&#39; nn.Module实现了自定义的__setattr__函数，当执行module.name=value时，会在__setattr__中判断value是否为Parameter或nn.Module对象，如果是则将这些对象加到_parameters和_modules两个字典中；如果是其他类型的对象，如Variable、list、dict等，则调用默认的操作，将这个值保存在__dict__中。 123module = nn.Module()module.param = nn.Parameter(t.ones(2, 2))module._parameters,module.param (OrderedDict([(&#39;param&#39;, Parameter containing: tensor([[1., 1.], [1., 1.]], requires_grad=True))]), Parameter containing: tensor([[1., 1.], [1., 1.]], requires_grad=True)) 1234567submodule1 = nn.Linear(2, 2)submodule2 = nn.Linear(2, 2)module_list = [submodule1, submodule2]# 对于list对象，调用builtin函数，保存在__dict__中module.submodules = module_listprint('_modules:',module._modules)print("__dict__['submodules']:",module.__dict__.get('submodules')) _modules: OrderedDict() __dict__[&#39;submodules&#39;]: [Linear(in_features=2, out_features=2, bias=True), Linear(in_features=2, out_features=2, bias=True)] 123module.submodules = nn.ModuleList(module_list)print('_modules:',module._modules)print("__dict__['submodules']:",module.__dict__.get('submodules')) _modules: OrderedDict([(&#39;submodules&#39;, ModuleList( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ))]) __dict__[&#39;submodules&#39;]: None 因_modules和_parameters中的item未保存在__dict__中，所以默认的getattr方法无法获取它，因而nn.Module实现了自定义的__getattr__方法。如果默认的getattr无法处理，就调用自定义的__getattr__方法，尝试从_modules、_parameters和_buffers三个字典中获取。 123getattr(module, 'training') # 等价于module.training#error#module.__getattr__('training') True 1234module.attr1 = 2getattr(module, 'attr1')# 报错# module.__getattr__('attr1') 2 1getattr(module, 'param') Parameter containing: tensor([[1., 1.], [1., 1.]], requires_grad=True) 在pytorch中保存模型十分简单，所有的module对象都具有state_dict()函数，返回当前Module所有的状态数据。将这些状态数据保存后，下次使用模型时即可利用model.load_state_dict()函数将状态加载进来。优化器（optimizer）也有类似机制，不过一般并不需要保存优化器的运行状态。 123456# 保存模型t.save(net.state_dict(), 'net.pth')# 加载已保存的模型net2 = Net()net2.load_state_dict(t.load('net.pth')) &lt;All keys matched successfully&gt; 123t.save(net, 'net_all.pth')net2 = t.load('net_all.pth')net2 /usr/local/lib/python3.6/dist-packages/torch/serialization.py:256: UserWarning: Couldn&#39;t retrieve source code for container of type Net. It won&#39;t be checked for correctness upon loading. &quot;type &quot; + obj.__name__ + &quot;. It won&#39;t be checked &quot; Net( (submodel1): Linear(in_features=3, out_features=4, bias=True) ) 将Module放在GPU上运行也十分简单，只需一下两步。 model = model.cuda():将模型的所有参数转存到GPU input.cuda():将输入数据放置到GPU上。 至于如何在多个GPU上并行计算，pytorch也提供了两个函数，可实现简单高效的并行GPU计算。 nn.parallel.data_parallel(module, inputs, device_ids = None, output_device = None, dim = 0, module_kwargs = None) class torch.nn.DataParallel(module, device_ids = None, output_device = None, dim = 0) 可见二者的参数十分相似，通过device_ids参数可以指定在哪些GPU上进行优化，output_device指定输出到哪个GPU上。唯一的不同在于前者直接利用多GPU并行计算得出结果，后者则返回一个新的module，能够自动在多GPU上进行并行加速。 123456# method1new_net = nn.DataParallel(net, device_ids = [0, 1])output = new_net(input)# method 2output = nn.parallel.data_parallel(net, input, device_ids = [0, 1]) DataParallel并行的方式，是将输入一个batch的数据均分成多份，分别送到对应的GPU进行计算，然后将各个GPU得到的梯度相加。与Module相关的所有数据也会以浅复制的方式复制多份。 nn和aautograd的关系nn.Module利用的是autograd技术，其主要工作是实现前向传播。在forward函数中，nn.Module对输入的Variable进行的各种操作，本质上都用了autograd技术。这里需要对比autograd.Function和nn.Module之间的区别。 autograd.Function利用Tensor对autograd技术的扩展，为autograd实现了新的运算op，不仅要实现前向传播还要手动实现反向传播。 nn.Module利用了autograd技术，对nn的功能进行扩展，实现了深度学习中更多的层。只需实现前向传播功能，autograd即会自动实现反向传播。 nn.functional是一些autograd操作的集合，是经过封装的函数。 作为两种扩充pytorch接口的方法，我们在实际作用中应该如何选择？如果某一个操作在autograd中尚未支持，那么需要利用Function手动实现对应的前后传播和反向传播。如果某些时候利用autograd接口比较复杂，则可以利用Function将多个操作聚合，实现优化，比直接利用autograd低级别的操作要快。如果只是想在深度学习中怎加某一层，使用nn.Module进行封装则更简单高效。]]></content>
      <categories>
        <category>笔记</category>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【notes】pytorch学习笔记2-autograd部分]]></title>
    <url>%2F2019%2F08%2F20%2Fautograd%2F</url>
    <content type="text"><![CDATA[autogradtorch.autograd是为方便用户使用，专门开发的一套自动求导引擎，它能够根据输入和前向传播过程自动构建计算图，并执行反向传播。 计算图是现代深度学习框架的核心，它为自动求导算法——反向传播提供了理论支持。 Variablepytorch在autograd模块中实现计算图相关功能，autograd中的核心数据结构是Variable。Variable封装了tensor，并记录对tensor的操作记录用来构建计算图。Variale的数据结构如图： graph LR; A[autograd.Variable] --> B(data); A[autograd.Variable] --> C(grad); A[autograd.Variable] --> D(grad_fn); Variable的构造函数需要传入tensor，同时有两个可选参数。 require_grad(bool)：是否需要对该variable进行求导。 volatile(bool)：意为“挥发”，设置为True，构建在该variable之上的图都不会求导，转为推理阶段设计。 Variable支持大部分tensor支持的函数，但其不支持部分inplace函数，因为这些函数会修改tensor自身，而在反向传播中，variable需要缓存原来的tensor来计算梯度。如果想要计算各个Variable的梯度，只需调用根节点variable的backward方法，autograd会自动沿着计算图反向传播，计算每一个叶子节点的梯度。 grad_variables：形状与variable一致，对于y.backward()，grad_variables相当于链式法则$\frac{\partial{z}}{\partial{x}}=\frac{\partial{z}}{\partial{y}}\times\frac{\partial{y}}{\partial{x}}$中的$\frac{\partial{z}}{\partial{y}}$。grad_variables也可以是tensor或序列。 retain_graph：反向传播需要缓存一些中间结果，反向传播之后，这些缓存就被清空，可通过指定这个参数不清空缓存，用来多次反向传播。 create_graph：对反向传播过程再次构建计算图，可通过backward of backward实现求高阶导数。 12from torch.autograd import Variable as Vimport torch as t 123# 从tensor中创建variable，指定需要求导a = V(t.ones(3, 4), requires_grad = True);a tensor([[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]], requires_grad=True) 1b = V(t.zeros(3, 4));b tensor([[0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]]) 1234# 函数的使用和tensor一致# 也可写成c = a + bc = a.add(b)c tensor([[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]], grad_fn=&lt;AddBackward0&gt;) 12d = c.sum()d.backward() # 反向传播 1234# 注意二者的区别# 前者在取data后变为tensor，从tensor计算sum得到float# 后者计算sum后仍然是Variablec.data.sum(), c.sum() (tensor(12.), tensor(12., grad_fn=&lt;SumBackward0&gt;)) 1a.grad tensor([[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]]) 123# 此处虽然没有指定c需要求导，但c依赖于a，而a需要求导# 因此c的requires_grad属性会自动设为Truea.requires_grad, b.requires_grad, c.requires_grad (True, False, True) 123# c.grad是None，c不是叶子节点，他的梯度是用来计算a的梯度# 虽然c.requires_grad = True，但其梯度计算完之后即被释放c.grad is None True 接下来看看autograd计算导数和我们手动推导的导数的区别。 y=x^2e^x它的导函数是： \frac{\partial{y}}{\partial{x}}=2xe^x+x^2e^x123456def f(x): y = x**2 * t.exp(x) return ydef gradf(x): dx = 2*x*t.exp(x) + x**2*t.exp(x) return dx 123x = V(t.randn(3, 4), requires_grad = True)y = f(x)y tensor([[1.3949e-01, 2.7201e-01, 4.9848e-01, 2.2968e+00], [3.2033e-01, 3.3618e-01, 2.3554e-02, 1.0507e+01], [3.9416e+01, 3.5322e+00, 9.6847e-02, 1.2743e+01]], grad_fn=&lt;MulBackward0&gt;) 12y.backward(t.ones(y.size())) # grad_variables形状与y一致x.grad tensor([[ 1.0154, -0.4398, -0.1755, 7.1583], [-0.4095, 1.7961, 0.3532, 24.3531], [76.1412, 10.0143, -0.4190, 28.6505]]) 12# autograd的计算结果与利用公式手动计算的结果一致gradf(x) tensor([[ 1.0154, -0.4398, -0.1755, 7.1583], [-0.4095, 1.7961, 0.3532, 24.3531], [76.1412, 10.0143, -0.4190, 28.6505]], grad_fn=&lt;AddBackward0&gt;) 计算图pytorch中autograd的底层采用了计算图，计算图是一种特殊的有向无环图（DAG），用于记录算子与变量之间的关系。一般用矩形表示算子，椭圆形表示变量。如表达式$\boldsymbol{z}=\boldsymbol{wx}+\boldsymbol{b}$可分解为$\boldsymbol{y}=\boldsymbol{wx}$和$\boldsymbol{z}=\boldsymbol{y}+\boldsymbol{b}$，其计算图如图所示，图中的MUL和ADD都是算子，$\boldsymbol{w}$、$\boldsymbol{x}$、$\boldsymbol{b}$为变量。 graph LR; A((W)) --> C[MUL]; B((X)) --> C[MUL]; C[MUL] --> D((y)); E((b)) --> F[Add]; D((y)) --> F[Add]; F[Add] --> G((z)); 如上有向无环图，$\boldsymbol{X}$和$\boldsymbol{b}$是叶子节点，这些节点通常有用户自己创建，不依赖于其他变量。$\boldsymbol{z}$称为根节点，是计算图的最终目标。利用链式法则很容易求各个叶子节点的梯度。 \frac{\partial{z}}{\partial{b}}=1,\frac{\partial{z}}{\partial{y}}=1\\\frac{\partial{y}}{\partial{w}}=x,\frac{\partial{y}}{\partial{x}}=w\\\frac{\partial{z}}{\partial{x}}=\frac{\partial{z}}{\partial{y}}\frac{\partial{y}}{\partial{x}}=1*w\\\frac{\partial{z}}{\partial{w}}=\frac{\partial{z}}{\partial{y}}\frac{\partial{y}}{\partial{w}}=1*x有了计算图上述链式求导可自动利用计算图的反向传播自动完成，其过程如图所示： graph LR; A((dz)) --> B[addBackward]; B[addBackward] --> C((dy)); B[addBackward] --> D((db)); C((dy)) --> E[mulBackward]; E[mulBackward] --> F((dX)); E[mulBackward] --> G((dW)); 图中记录了操作function，每个变量在图中的位置可通过其grad_fn属性在图中的位置推测得到。在反向传播的过程中，autograd沿着这个图从当前变量（根节点z）溯源，可以利用链式求导法则计算所有叶子节点的梯度。 每一个前向传播操作的函数都有与之对应的反向传播函数用来计算输入的各个variable的梯度，这些函数的函数名通常以Backward结尾。 12345x = V(t.ones(1))b = V(t.rand(1), requires_grad = True)w = V(t.rand(1), requires_grad = True)y = w * x # 等价于y = w.mul(x)z = y + b # 等价于z = y.add(b) 1x.requires_grad, b.requires_grad, w.requires_grad (False, True, True) 123# 虽然未指定y.requires_grad为True，但由于y依赖于需要求导的w# 故而y.requires_grad为Truey.requires_grad True 1x.is_leaf, w.is_leaf, b.is_leaf (True, True, True) 1y.is_leaf, z.is_leaf (False, False) 123# grad_fn可以查看这个variable的反向传播函数# z是add函数的输出，所以它的反向传播函数是AddBackwardz.grad_fn &lt;AddBackward0 at 0x7f073e390e80&gt; 1234#next_function保存grad_fn的输入，grad_fn的输入是一个tuple# 第一个是y，它是乘法（mul）的输出，所以对应的反向传播函数y.grad_fn是MulBackward# 第二个是b，它是叶子节点，由用户创建，grad_fn为None，但是有z.grad_fn.next_functions ((&lt;MulBackward0 at 0x7f073e390828&gt;, 0), (&lt;AccumulateGrad at 0x7f073e390f28&gt;, 0)) 12# variable的grad_fn对应着图中的functionz.grad_fn.next_functions[0][0] == y.grad_fn True 123# 第一个是w，叶子节点，需要求导，梯度是累加的# 第二个是x，叶子节点，不需要求导，所以为Noney.grad_fn.next_functions ((&lt;AccumulateGrad at 0x7f073e390470&gt;, 0), (None, 0)) 12# 叶子节点的grad_fn是Nonew.grad_fn, x.grad_fn (None, None) 计算$\boldsymbol{w}$的梯度时需要用到$\boldsymbol{x}$的数值（$\frac{\partial{y}}{\partial{w}}=x$），这些数值在前向过程中会保存成buffer，在计算完梯度之后会自动清空。为了能够多次反向传播需要指定retain_graph来保留这些buffer。 1y.grad_fn.saved_variables --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) &lt;ipython-input-30-284a59926bb7&gt; in &lt;module&gt; ----&gt; 1 y.grad_fn.saved_variables AttributeError: &#39;MulBackward0&#39; object has no attribute &#39;saved_variables&#39; 原因确实是版本问题，PyTorch0.3 中把许多python的操作转移到了C++中，saved_variables 现在是一个c++的对象，无法通过python访问。https://github.com/chenyuntc/pytorch-book/issues/7 可以查看这里进行学习https://github.com/chenyuntc/pytorch-book/blob/0.3/chapter3-Tensor和autograd/Autograd.ipynb,省掉上面的操作 123# 使用retain_graph保存bufferz.backward(retain_graph = True)w.grad tensor([1.]) 123# 多次反向传播，梯度累加，这也就是w中AccumulateGrad标识的含义z.backward()w.grad tensor([2.]) pytorch使用的是动态图，它的计算图在每次前向传播时都是从头开始构建的，所以它能够使用python的控制语句（如for、if等），根据需求创建计算图。这一点在自然语言处理领域中很有用，它意为你不需要事先构建所有可能用到的图的路径，图在运行时才构建。 1234567def abs(x): if x.data[0] &gt; 0: return x else: return -xx = V(t.ones(1), requires_grad = True)y = abs(x)y.backward()x.grad tensor([1.]) 1234x = V(-1 * t.ones(1), requires_grad = True)y = abs(x)y.backward()print(x.grad) tensor([-1.]) 123456789def f(x): result = 1 for ii in x: if ii.data &gt; 0: result = ii * result return resultx = V(t.arange(-2, 4).float(), requires_grad = True)y = f(x) # y = x[3] * x[4] * x[5]y.backward()x.grad tensor([0., 0., 0., 6., 3., 2.]) 变量的requires_grad属性默认为False，如果某一个节点requires_grad被设置为True，那么所有依赖它的节点requires_grad都是True。 with torch.no_grad()内的variable均为不会求导，其优先级高于requires_grad。函数可以用装饰器@torch.no_grad()。可实现一定程度的速度提升，并节省约一半显存，因为其不需要分配空间保存梯度。 详细内容可见：https://pytorch.org/docs/master/autograd.html#locally-disable-grad 1234x = t.tensor([1.], requires_grad = True)with t.no_grad(): y = x * 2y.requires_grad False 12345@t.no_grad()def doubler(x): return x * 2z = doubler(x)z.requires_grad False 在反向传播过程中非叶子节点的导数计算完之后即被清空。若想查看这些变量的梯度，有以下两种方法： 使用autograd.grad函数 使用hook 推荐使用hook方法，但在实际使用中应尽量避免修改grad的值。 123456x = V(t.ones(3), requires_grad = True)w = V(t.rand(3), requires_grad = True)y = x * w# y依赖于w，而w.requires_grad = Truez = y.sum()x.requires_grad, w.requires_grad, y.requires_grad (True, True, True) 123# 非叶子节点grad计算完之后自动清空，y.grad是Nonez.backward()x.grad, w.grad, y.grad (tensor([0.1283, 0.8326, 0.6539]), tensor([1., 1., 1.]), None) 12345678# 第一种方法：使用grad获取中间变量的梯度x = V(t.ones(3), requires_grad = True)w = V(t.rand(3), requires_grad = True)y = x * w# y依赖于w，而w.requires_grad = Truez = y.sum()# z对y的梯度，隐式调用backward()t.autograd.grad(z, y) (tensor([1., 1., 1.]),) 12345678910111213141516# 第二种方法：使用hook# hook是一个函数，输入是梯度，不应该有返回值def variable_hook(grad): print('y的梯度：\r\n',grad) x = V(t.ones(3), requires_grad = True)w = V(t.rand(3), requires_grad = True)y = x * w# 注册hookhook_handle = y.register_hook(variable_hook)z = y.sum()z.backward()# 除非你每次都要用hook，否则用完之后记得移除hookhook_handle.remove() y的梯度： tensor([1., 1., 1.]) 最后再来看看variable中grad属性和backward函数grad_variables参数的含义。 variables $\boldsymbol{x}$ 的梯度是目标函数$f(x)$对$\boldsymbol{x}$的梯度，形状与$\boldsymbol{x}$一致。 y.backward(grad_variables)中grad_variables相当于链式法则中的$\frac{\partial{z}}{\partial{x}}=\frac{\partial{z}}{\partial{y}}\frac{\partial{y}}{\partial{x}}$中的$\frac{\partial{z}}{\partial{y}}$。z是目标函数，一般是个标量，故而$\frac{\partial{z}}{\partial{y}}$的形状与$\boldsymbol{y}$的形状一致。z.backward()等价于y.backward(grad_y)。而z.backward()省略了grad_variables参数，是因为z是个标量，而$\frac{\partial{z}}{\partial{z}}=1$ 12345x = V(t.arange(0, 3).float(), requires_grad = True)y = x**2 + x*2z = y.sum()z.backward()x.grad tensor([2., 4., 6.]) 123456x = V(t.arange(0, 3).float(), requires_grad = True)y = x**2 + x*2z = y.sum()y_grad_variables = V(t.Tensor([1, 1, 1])) y.backward(y_grad_variables)x.grad tensor([2., 4., 6.]) 值得注意的是，只有对variable的操作才能使用autograd，如果variable的data直接进行操作，将无法使用反向传播。除了参数初始化，一般我们不会直接修改variable.data的值。 在pytorch中计算图的特点总结如下： autograd根据用户对variable的操作构建计算图。对variable的操作抽象为Function。 由用户创建的节点称为叶子节点，叶子节点的grad_fn为None。叶子节点中需要求导的variable，具有AccumulateGrad标识，因其梯度是累加的。 variable默认是不需要求导的，即requires_grad属性默认为False。如果某一个节点requires_grad被设置为True，那么所有依赖它的节点requires_grad都为True。 with torch.no_grad和@torch.no_grad()的作用下的节点都不会求导，优先级比requires_grad高。 多次反向传播时，梯度是累加的。反向传播的中间缓存会被清空，为进行多次反向传播需指定retain_graph=True来保存这些缓存。 非叶子节点的梯度计算完后即被清空，可以使用autograd.grad或hook技术获取非叶子节点梯度的值。 variable的grad与data形状一致，应避免直接修改variable.data，因为对data的直接操作无法利用autograd进行反向传播。 反向传播函数backward的参数grad_variables可以看成链式求导的中间结果，如果是标量，可以省略，默认为1。 pytorch采用动态图设计，可以很方便地查看中间层的输出，动态地设计计算图结构。 扩展autograd目前绝大多数函数都可以使用autograd实现反向求导，但如果需要自己写一个复杂的函数，不支持自动反向求导怎么办？答案是写一个Function，实现它的前向传播和反向传播代码，Function对应于计算图中的矩形，它接受参数，计算并返回结果。下面给出了一个例子。 123456789101112131415161718from torch.autograd import Functionclass MultiplyAdd(Function): @staticmethod def forward(ctx, w, x, b): print('type in forward', type(x)) ctx.save_for_backward(w, x)#存储用来反向传播的参数 output = w*x +b return output @staticmethod def backward(ctx, grad_output): w, x = ctx.saved_tensors # 老版本是saved_variables print('type in backward',type(x)) grad_w = grad_output * x grad_x = grad_output * w grad_b = grad_output * 1 return grad_w, grad_x, grad_b 分析如下： 自定义的Function需要继承autograd.Function，没有构造函数init，forward和backward函数都是静态方法 forward函数的输入和输出都是Tensor，backward函数的输入和输出都是Variable backward函数的输出和forward函数的输入一一对应，backward函数的输入和forward函数的输出一一对应 backward函数的grad_output参数即t.autograd.backward中的grad_variables 如果某一个输入不需要求导，直接返回None，如forward中的输入参数x_requires_grad显然无法对它求导，直接返回None即可 反向传播可能需要利用前向传播的某些中间结果，需要进行保存，否则前向传播结束后这些对象即被释放 12345678910x = V(t.ones(1))w = V(t.rand(1),requires_grad=True)b = V(t.rand(1),requires_grad=True)print('开始前向传播')z = MultiplyAdd.apply(w, x, b)print('开始反向传播')z.backward()# x不需要求导，中间过程还是会计算它的导数，但随后被清空x.grad, w.grad, b.grad 开始前向传播 type in forward &lt;class &#39;torch.Tensor&#39;&gt; 开始反向传播 type in backward &lt;class &#39;torch.Tensor&#39;&gt; (None, tensor([1.]), tensor([1.])) 12345678910x = V(t.ones(1))w = V(t.rand(1),requires_grad=True)b = V(t.rand(1),requires_grad=True)print('开始前向传播')z = MultiplyAdd.apply(w, x, b)print('开始反向传播')# 调用MultiplyAdd.backward# 会自动输出grad_w, grad_x, grad_bz.grad_fn.apply(V(t.ones(1))) 开始前向传播 type in forward &lt;class &#39;torch.Tensor&#39;&gt; 开始反向传播 type in backward &lt;class &#39;torch.Tensor&#39;&gt; (tensor([1.]), tensor([0.5986], grad_fn=&lt;MulBackward0&gt;), tensor([1.])) 在backward函数里之所以也要对variable进行操作是为了能计算梯度的梯度。 1234x = V(t.Tensor([5]), requires_grad = True)y = x ** 2grad_x = t.autograd.grad(y, x, create_graph = True)grad_x (tensor([10.], grad_fn=&lt;MulBackward0&gt;),) 1grad_grad_x = t.autograd.grad(grad_x[0],x);grad_grad_x (tensor([2.]),)]]></content>
      <categories>
        <category>笔记</category>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【notes】pytorch学习笔记1-Tensor部分]]></title>
    <url>%2F2019%2F08%2F19%2Fpytorch-1%2F</url>
    <content type="text"><![CDATA[Tensor和autograd 每个深度学习框架的设计核心是张量和计算图，在pytorch里体现为张量系统（Tensor）和自动微分系统（atutograd）。 Tensor 中文译为张量，可以简单看作一个数组。 与numpy里的ndarrays类似，但tensor支持GPU加速。 基础操作接口角度： torch.function tensor.function 存储角度： 不会修改自身数据，如a.add(b),返回一个值为加法结果的新的tensor。 会修改自身数据，如a.add_(b)，加法的值储存在a中了。 创建Tensor在pytorch中常见的新建tensor的方法： 类别 特点 函数 功能 第一类：基础方法 最灵活 Tensor(*sizes) 基础构造函数 第二类：根据sizes建立 常数型 ones(*sizes) 全1Tensor 常数型 zeros(*sizes) 全0Tensor 常数型 eyes(*sizes) 对角线为1，其他为0 概率分布型 rand/randn(*sizes) 均匀/标准分布 第三类：在一定范围内建立 等差数列型 arange(s,e,step) 从s到e，步长为step 等差数列型 linspace(s,e,steps) 从s到e，均匀切分成steps份 概率分布型 normal(mean,std)/uniform(from,to) 正态分布/均匀分布 概率分布型 randperm(m) 随机分布 其中使用Tensor函数新建tensor是最复杂多变的，它既可以接受一个list，并根据list的数据新建tensor，也可根据指定的形状新建tensor，还能传入其他的tensor。 123# 引入必要的包import torch as tfrom torch.autograd import Variable as V 12# 指定tensor的形状a = t.Tensor(2, 3);a tensor([[7.2443e+22, 4.2016e+30, 9.9708e+17], [7.2296e+31, 5.6015e-02, 4.4721e+21]]) 12# 用list的数据创建tensorb = t.Tensor([[1,2,3],[4,5,6]]);b tensor([[1., 2., 3.], [4., 5., 6.]]) 1b.tolist(),type(b.tolist()) # 把tensor转为list ([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], list) tensor.size()返回torch.Size()对象，它是tuple的子类，但其使用方式与tuple略有不同。 1b_size = b.size();b_size torch.Size([2, 3]) 1b.numel() # numelements前五个字母，b中元素总个数，等价于b.nelement() 6 12345# 创建一个和b形状一样的tensorc = t.Tensor(b_size)# 创建一个元素为2和3的tensord = t.Tensor((2, 3))c, d # 输出结果不同，明显看出torch.Size()对象和tuple的不同 (tensor([[5.8959e-35, 4.5636e-41, 1.0257e-36], [0.0000e+00, 5.0000e+00, 6.0000e+00]]), tensor([2., 3.])) tensor.shape等价于tensor.size() 1c.shape torch.Size([2, 3]) 需要注意：t.Tensor(\sizes)创建tensor时，系统不会马上分配空间，只会计算内存是否够用，使用到tensor时才会分配，而其他方法是创建后会立马分配空间。 * 1t.ones(2, 3) tensor([[1., 1., 1.], [1., 1., 1.]]) 1t.zeros(2, 3) tensor([[0., 0., 0.], [0., 0., 0.]]) 1t.linspace(1, 10 ,3) tensor([ 1.0000, 5.5000, 10.0000]) 1t.randn(2, 3) tensor([[-0.4864, 0.5022, -0.4059], [ 0.4138, 1.1588, -1.1650]]) 1t.randperm &lt;function _VariableFunctions.randperm&gt; 123# 0到n-1随机排列后的数列n = 10t.randperm(n) tensor([2, 5, 8, 3, 4, 1, 0, 7, 9, 6]) 1t.eye(2, 3) # 不要求行列数一致 tensor([[1., 0., 0.], [0., 1., 0.]]) 1t.normal(t.Tensor([0]),t.Tensor([1])) tensor([-0.5517]) 常用Tensor操作 tensor.view方法可以改变tensor的形状，但要保证前后元素总数一致。前后保持数据一致，返回的新tensor与源tensor共享内存。 在实际应用中可能经常需要增加或减少某个维度，这是squeeze和unsqueeze两个函数排上用场。 12a = t.arange(0, 6)a.view(2, 3) tensor([[0, 1, 2], [3, 4, 5]]) 12b = a.view(-1, 3) # 当某一维为-1时，会自动计算它的大小b tensor([[0, 1, 2], [3, 4, 5]]) 1b.shape, b.unsqueeze(1).shape # 注意形状，在第1维上增加“1” (torch.Size([2, 3]), torch.Size([2, 1, 3])) 1b.unsqueeze(-2) # -2表示倒数第二个维度 tensor([[[0, 1, 2]], [[3, 4, 5]]]) 12c = b.view(1, 1, 1, 2, 3)c, c.squeeze(0) # 压缩第0维的1 (tensor([[[[[0, 1, 2], [3, 4, 5]]]]]), tensor([[[[0, 1, 2], [3, 4, 5]]]])) 1c.squeeze() # 压缩所有的“1”的维度 tensor([[0, 1, 2], [3, 4, 5]]) 12a[1] = 100b # a和b共享内存，修改了a，b也变了 tensor([[ 0, 100, 2], [ 3, 4, 5]]) resize是另一种改变size的方法，和view不同的地方是resize可以改变尺寸，可以有不同数量的元素。如果新尺寸超过了旧尺寸，会自动分配空间，如果新尺寸小于旧尺寸，之前的数据依旧会保存。 12b.resize_(1, 3)b tensor([[ 0, 100, 2]]) 12b.resize_(3, 3) # 旧的数据依旧被保存，多出的数据会分配新空间。b tensor([[ 0, 100, 2], [ 3, 4, 5], [7881702260482471202, 8319104481852400229, 7075192647680159593]]) 索引操作Tensor支持和numpy.ndarray类似的索引操作，语法上也类似。 如无特殊说明，索引出来的结果与原tensor共享内存 1a = t.randn(3,4);a tensor([[ 0.8865, -0.8832, -1.0883, -0.2804], [-0.9056, 0.0635, 0.5528, -0.0222], [ 1.4919, -1.0480, -1.7623, 0.8558]]) 1a[0] # 第0行 tensor([ 0.8865, -0.8832, -1.0883, -0.2804]) 1a[:, 0] # 第0列 tensor([ 0.8865, -0.9056, 1.4919]) 1a[0][2] # 第0行第2个元素，等价于a[0,2] tensor(-1.0883) 1a[0, -1] # 第0行最后一个元素 tensor(-0.2804) 1a[:2] # 前两行 tensor([[ 0.8865, -0.8832, -1.0883, -0.2804], [-0.9056, 0.0635, 0.5528, -0.0222]]) 1a[:2, 0:2] # 前两行，第0,1列 tensor([[ 0.8865, -0.8832], [-0.9056, 0.0635]]) 1a[0:1, :2].shape, a[0, :2].shape # 注意两者的区别是形状不同，但是值是一样的 (torch.Size([1, 2]), torch.Size([2])) 123a[a &gt; 1] # 等价于a.masked_select(a&gt;1)# 选择结果与原tensor不共享内存空间 tensor([1.4919]) 1a[t.LongTensor([0,1])] # 第0行和第1行 tensor([[ 0.8865, -0.8832, -1.0883, -0.2804], [-0.9056, 0.0635, 0.5528, -0.0222]]) 常用的选择函数： 函数 功能 index_select(input, dim, index) 在指定维度dim上选取，例如选取某行某列 masked_select(input, mask) 例子如上，a[a &gt; 0],使用ByteTensor进行选取 non_zero(input) 非0元素的下标 gather(input, dim, index) 根据index，在dim维度上选取数据，输出的size与index一样 gather是一个比较复杂的操作，对于一个二维的tensor，输出的每个元素如下：12out[i][j] = input[index[i][j]][j] # dim = 0out[i][j] = input[i][index[i][j]] # dim = 1 1a = t.arange(0, 16).view(4, 4);a tensor([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14, 15]]) 123# 选取对角线上的元素index = t.LongTensor([[0,1,2,3]])a.gather(0, index) tensor([[ 0, 5, 10, 15]]) 123# 选取反对角线上的元素index = t.LongTensor([[3, 2, 1, 0]]).t()a.gather(1,index) tensor([[ 3], [ 6], [ 9], [12]]) 123# 选取反对角线上的元素，注意与上面不同index = t.LongTensor([[3, 2, 1, 0]])a.gather(0, index) tensor([[12, 9, 6, 3]]) 123# 选取两个对角线上的元素index = t.LongTensor([[0, 1, 2, 3], [3, 2, 1, 0]]).t()b = a.gather(1, index);b tensor([[ 0, 3], [ 5, 6], [10, 9], [15, 12]]) gather的逆操作是scatter_, gather把数据从input中按index取出，而scatter_是把取出的数据再放回去。注意scatter_函数是inplace操作。 1234out = input.gather(dim, index)--&gt;近似逆操作out = Tensor()out.scatter_(dim, index) 1234# 把两个对角线元素放回到指定位置c = t.zeros(4, 4)c.scatter_(1, index, b.float()) tensor([[ 0., 0., 0., 3.], [ 0., 5., 6., 0.], [ 0., 9., 10., 0.], [12., 0., 0., 15.]]) 高级索引高级索引可以看成是普通索引的扩展，但是高级索引操作的结果一般不和原Tensor共享内存。 1x = t.arange(0, 27).view(3, 3, 3);x tensor([[[ 0, 1, 2], [ 3, 4, 5], [ 6, 7, 8]], [[ 9, 10, 11], [12, 13, 14], [15, 16, 17]], [[18, 19, 20], [21, 22, 23], [24, 25, 26]]]) 1x[[1, 2], [1, 2], [2, 0]] # 元素的个数是列表的长度 元素为x[1,1,2]和x[2,2,0] tensor([14, 24]) 1x[[2,1,0],[0],[1]] # 元素为最长列表的长度 x[2,0,1] x[1,0,1] x[0,0,1] tensor([19, 10, 1]) 1x[[0,2],...] # x[0] x[2] tensor([[[ 0, 1, 2], [ 3, 4, 5], [ 6, 7, 8]], [[18, 19, 20], [21, 22, 23], [24, 25, 26]]]) Tensor类型默认的Tensor类型为FloatTensor，可通过t.get_default_tensor_type修改默认类型（如果默认类型为GPU tensor，在所有操作都在GPU上进行）。 HalfTensor是专门为GPU版本设计的，同样的元素个数，显存占用只有FloatTensor的一半，所以可以极大地缓解GPU显存不足的问题，但是由于HalfTensor所能表示的数值大小和精度有限，所以可能出现溢出等问题。 数据类型 CPU tensor GPU tensor 32bit浮点 torch.FloatTensor torch.cuda.FloatTensor 64bit浮点 torch.DoubleTensor torch.cuda.DoubleTensor 16半精度浮点 N/A torch.cuda.HalfTensor 8bit无符号整型（0~255） torch.ByteTensor torch.cuda.ByteTensor 8bit有符号整型（-128~127） torch.CharTensor torch.cuda.CharTensor 16bit有符号整型 torch.ShortTensor torch.cuda.ShortTensor 32bit有符号整型 torch.IntTensor torch.cuda.IntTensor 64bit有符号整型 torch.LongTensor torch.cuda.LongTensor 各数据类型之间可以互相转换，type(new_type)是通用的做法，同时还有float、long、half等快捷方法。CPU tensor与GPUtensor之间的互相装换通过tensor.cuda和tensor.cpu的方法实现。Tensor还有一个new方法，用法与t.Tensor一样，会调用该tensor对应类型的构造函数，生成与当前tensor类型一致的tensor。 123456# 设置默认tensor类型, 注意参数是字符串# t.set_default_tensor_type('torch.IntTensor') 会报错# TypeError: only floating-point types are supported as the default type# t.get_default_dtype() 返回 torch.float32# t.set_default_dtype(t.int) 报错 TypeError: only floating-point types are supported as the default type 1a = t.Tensor(2, 3);a tensor([[1.8609e+34, 1.8179e+31, 1.8524e+28], [9.6647e+35, 2.0076e+29, 7.3185e+28]]) 1b = a.int();b tensor([[-2147483648, -2147483648, -2147483648], [-2147483648, -2147483648, -2147483648]], dtype=torch.int32) 1c = a.type_as(b);c tensor([[-2147483648, -2147483648, -2147483648], [-2147483648, -2147483648, -2147483648]], dtype=torch.int32) 1d = b.new(2, 3);d tensor([[ 0, 775041082, 960062260], [1697986359, 926101553, 895706424]], dtype=torch.int32) 123# 查看函数new的源码a.new?? 逐元素操作这部分操作会对tensor的每个元素进行操作，输入和输出的形状相同。 函数 功能 abs/sqrt/div/exp/fmod/log/pow.. 绝对值/平方根/除法/指数/求余/对数/求幂 cos/sin/asin/atan2/cosh 三角函数 ceil/round/floor/trunc 上取整/四舍五入/下取整/只保留整数部分 clamp(input,min,max) 超过min和max部分截断 sigmod/tanh… 激活函数 对于很多基本的运算，比如加减乘除求余等运算pytorch都实现了运算符重载，可以直接使用运算符。其中camp(x, min, max)的输出满足一个分段函数： y_i= \begin{cases} min, & {x_i < min}\\\\ x_i, & {min \leq x_i \leq max}\\\\ max, & {x_i > max} \end{cases}12a = t.arange(0, 6).view(2, 3).float() # 注意要转换一下类型，否则会报错t.cos(a) tensor([[ 1.0000, 0.5403, -0.4161], [-0.9900, -0.6536, 0.2837]]) 1a % 3 # 等价于t.fmod(a, 5) tensor([[0., 1., 2.], [0., 1., 2.]]) 1a ** 2# 等价于t.power(a, 2) tensor([[ 0., 1., 4.], [ 9., 16., 25.]]) 1234# a中每个元素与3相比取较大的那一个print(a)t.clamp(a, min = 3) tensor([[0., 1., 2.], [3., 4., 5.]]) tensor([[3., 3., 3.], [3., 4., 5.]]) 归并操作这类操作会使输入形状小于输出形状，并可以沿着某一维度进行制定操作。 函数 功能 mean/sum/median/mode 均值/和/中位数/众数 norm/dist 范数/距离 std/var 标准差/方差 cumsum/cumprod 累加/累乘 几乎每个函数都有一个dim参数，用来制定在那个维度上执行。假设输入的形状是(m, n, k): 如果指定dim = 0，输出的形状为(1, n, k)或者(n, k) 如果指定dim = 1，输出的形状为(m, 1, k)或者(m, k) 如果指定dim = 2，输出的形状为(m, n, 1)或者(m, n) 也就是dim指定哪个维度，那个维度就会变成1，size中是否有1取决于keepdim，keepdim=True会保留1，keepdim默认为False。但是并非总是这样，比如cumsum。 归并运算就是对其他维度取值相同且该维度取值不同元素进行操作。 12b = t.ones(2, 3)b.sum(dim = 0, keepdim = True) tensor([[2., 2., 2.]]) 1b.sum(dim = 0) #keepdim = False tensor([2., 2., 2.]) 1b.sum(dim = 1) tensor([3., 3.]) 123a = t.arange(0, 6).view(2, 3)print(a)a.cumsum(dim = 1) #沿着行累加 tensor([[0, 1, 2], [3, 4, 5]]) tensor([[ 0, 1, 3], [ 3, 7, 12]]) cumsum可以理解为以dim这个维度上索引取值相同的看作一个整体，比如dim=0每一行就是一个整体，cumsum运算相当于dim这个维度上取值为n的值加上取值为n-1的值（这个n-1已经进行过前面的运算，不是初始的值）。 比较比较函数有的是逐元素操作，有的是归并操作。 函数 功能 gt/lt/ge/le/eq/ne 大于/小于/大于等于/小于等于/等于/不等 topk 最大的k个数 sort 排序 max/min 比较两个tensor的最大值或最小值 表中第一行的比较操作已经重载，已经可以使用a&gt;=b, a&gt;b, a!=b和a==b，其返回结果为一个ByteTensor,可以用来选取元素(高级索引)。 max和min两个操作比较特殊，以max为例： t.max(tensor):返回tensor中最大的一个数。 t.max(tensor,dim)：指定维上最大的一个数，返回tensor和下标。 t.max(tensor1,tensor2)：比较两个tensor中较大的元素。 tensor和一个数的比较可以用clamp函数。 1a = t.linspace(0, 15, 6).view(2, 3);a tensor([[ 0., 3., 6.], [ 9., 12., 15.]]) 1b = t.linspace(15, 0, 6).view(2, 3);b tensor([[15., 12., 9.], [ 6., 3., 0.]]) 1a &gt; b tensor([[False, False, False], [ True, True, True]]) 1a[a &gt; b] tensor([ 9., 12., 15.]) 1t.max(a) tensor(15.) 1t.max(a, dim = 1) torch.return_types.max( values=tensor([ 6., 15.]), indices=tensor([2, 2])) 1t.max(a, b) tensor([[15., 12., 9.], [ 9., 12., 15.]]) 123# 比较a和10较大的元素t.clamp(a, min=10) tensor([[10., 10., 10.], [10., 12., 15.]]) 线性代数pytorch的线性函数封装了Blas和Lapack。 函数 功能 trace 对角线元素（矩阵的迹） diag 对角线元素 triu/tril 矩阵的上三角/下三角，可以指定偏移量 mm/bmm 矩阵乘法，batch的矩阵乘法 addmm/addbmm/addmv 矩阵运算 t 转置 dot/cross 内积/外积 inverse 求逆矩阵 svd 奇异值分解 需要注意矩阵装置会导致储存空间不连续，需调用它的.contiguous方法将其转为连续。 12b = a.t()b.is_contiguous(),b (False, tensor([[ 0., 9.], [ 3., 12.], [ 6., 15.]])) 1b.contiguous() tensor([[ 0., 9.], [ 3., 12.], [ 6., 15.]]) Tensor和Numpytensor和numpy数组之间具有很高的相似性，彼此之间相互操作也十分高效。需要注意，numpy和tensor共享内存。当遇到tensor不支持的操作时，可先转成Numpy数组，处理后再装回tensor，其转换开销很小。 广播法则是科学运算中经常使用的一个技巧，它在快速执行向量化的同时不会占用额外的内存、显存。Numpy的广播法则定义如下： 让所有输入数组都向shape最长的数组看齐，shape中不足的部分通过在前面加1补齐。 两个数组要么在某一个维度的长度一致，要么其中一个为1，否则不能计算。 当输入数组的某个维度的长度为1时，计算时沿着此维度复制扩充成一样的形状。 pytorch当前已经支持了自动广播法则，但建议可以手动通过函数实现广播法则，更直观不易出错。 unsqueeze或者view：为数据的某一维的形状补1，实现法则1。 expand或者expand_as，重复数组，实现法则3；该操作不会复制数组，所以不会占用额外的空间。 注意:repeat实现有expand类似，但是repeat会把相同数据复制多份，因此会占用额外空间。 12a = t.ones(3, 2)b = t.zeros(2, 3, 1) 1234567# 自动广播法则# 第一步：a是二维，b是三维，所以先在较小的a前面补1，# 即：a.unsqueeze(0), a的形状变成(1, 3, 2), b的形状是(2, 3, 1),# 第二步：a和b在第一维和第三维的形状不一样，其中一个为1# 可以利用广播法则扩展，两个形状都变成了(2, 3, 2)(a + b).shape,a + b (torch.Size([2, 3, 2]), tensor([[[1., 1.], [1., 1.], [1., 1.]], [[1., 1.], [1., 1.], [1., 1.]]])) 1a.unsqueeze(0).expand(2, 3, 2) + b.expand(2, 3, 2) tensor([[[1., 1.], [1., 1.], [1., 1.]], [[1., 1.], [1., 1.], [1., 1.]]]) 123import numpy as npa = np.ones([2, 3], dtype = np.float32)a array([[1., 1., 1.], [1., 1., 1.]], dtype=float32) 1b = t.from_numpy(a);b tensor([[1., 1., 1.], [1., 1., 1.]]) 12b = t.Tensor(a) # 也可以直接讲numpy对象传入Tensor，这种情况下若numpy类型不是Float32会新建。b tensor([[1., 1., 1.], [1., 1., 1.]]) 12c = b.numpy() # a, b, c三个对象共享内存c array([[1., 1., 1.], [1., 1., 1.]], dtype=float32) 12# expand不会占用额外空间，只会在需要时才扩充，可极大地节省内存。e = t.Tensor(a).unsqueeze(0).expand(1000000000000, 2, 3) 内部结构tensor分为头信息区（Tensor）和存储区（Storage），信息区主要保存着tensor的形状（size），步长（stride）、数据类型（type）等信息，而真正的数据则保存成连续的数组。 graph LR; A[Tensor A: *size *stride * dimention...] --> C[Storage:*data *size ...]; B[Tensor B: *size *stride * dimention....] --> C[Storage:*data *size ...]; 一般来说，一个tensor有着与之对应的storage，storage是在data之上封装的接口，便于使用。不同的tensor的头信息一般不同，但却可能使用相同的storage。下面我们来看两个例子。 12a = t.arange(0, 6)a.storage() 0 1 2 3 4 5 [torch.LongStorage of size 6] 12b = a.view(2, 3)b.storage() 0 1 2 3 4 5 [torch.LongStorage of size 6] 123# 一个对象的id值可以看作它在内存中的地址# a和b storage的内存地址一样，即是同一个storageid(b.storage()) == id(a.storage()) True 1234# a改变，b也随之改变，因为它们共享storagea[1] = 100b tensor([[ 0, 100, 2], [ 3, 4, 5]]) 12c = a[2:]c.storage() 0 100 2 3 4 5 [torch.LongStorage of size 6] 12c.data_ptr(), a.data_ptr(), c.dtype # data_ptr返回tensor的首元素的内存地址# 可以看出相差16，这是因为2x8相差两个元素，每个元素占8个字节 (61509136, 61509120, torch.int64) 12c[0] = -100 # c[0]的内存地址对应a[2]内存地址a tensor([ 0, 100, -100, 3, 4, 5]) 123d = t.Tensor(c.float().storage())d[0] = 6666b tensor([[ 0, 100, -100], [ 3, 4, 5]]) 123# 下面4个共享storageid(a.storage()) == id(b.storage()) == id(c.storage()) == id(d.storage()) True 1a.storage_offset(), c.storage_offset(), a[3:].storage_offset() (0, 2, 3) 12e = b[::2, ::2] # 隔2行/列取一个元素id(e.storage()) == id(a.storage()) True 1b.stride(), e.stride() ((3, 1), (6, 2)) 1e.is_contiguous() False 可见绝大多数操作并不修改tensor的数据，只是修改头信息。这样更节省内存，同时提升了处理的速度。但是，有些操作会导致tensor不连续，这时需调用tensor.contiguous方法将他们变成连续数据，该方法复制数据到新的内存，不再与原来的数据共享storage。 另外高级索引一般不共享内存，而普通索引共享storage。 其他有关Tensor的话题持久化tensor的保存和加载十分简单，使用t.save和t.load即可完成相应功能。在save/load时可以指定使用的pickle模块，在load时还可以将GPU tensor映射到CPU或其他GPU上。 123456789if t.cuda.is_available(): a = a.cuda(1) t.save(a, 'a.pth') # 加载为b，储存于GPU1上（因为保存时就在GPU1上） b = t.load('a.pth') # 加载为c，储存在CPU c = t.load('a.pth', map_location = lambda storage,loc:storage) # 加载为d，储存于GPU0上 d = t.load('a.pth', map_location = &#123;'cuda:1':'cuda:0'&#125;) 向量化向量化计算是一种特殊的并行计算方式，通常是对不同的数据执行同样的一个或一批指令。向量化可极大第提高科学运算的效率。Python有许多操作很低效，尤其是for循环。在科学计算中要极力避免使用Python原生的for循环，尽量使用向量化的数值计算。 12345def for_loop_add(x, y): result = [] for i, j in zip(x, y): result.append(i + j) return t.Tensor(result) 12x = t.zeros(100)y = t.ones(100) 12%timeit -n 10 for_loop_add(x, y)%timeit -n 10 x + y 729 µs ± 414 µs per loop (mean ± std. dev. of 7 runs, 10 loops each) The slowest run took 4.81 times longer than the fastest. This could mean that an intermediate result is being cached. 3.5 µs ± 2.69 µs per loop (mean ± std. dev. of 7 runs, 10 loops each) 可见有好几百倍的速度差距，因此在实际使用中应尽量调用内建函数，这些函数底层由C/C++实现，能通过执行底层优化实现高效计算。 此为还需要注意几点： 大多数t.function都有一个参数out，这时产生的结果将保存在out指定的tensor之中 t.set_num_threads可以设置pytorch进行CPU多线程并行计算时所占用的线程数，来限制pytorch所占用的CPU数目。 t.set_printoptions可以用来设置打印tensor时的数值精度和格式。 1a = t.randn(2, 3); a tensor([[-0.1227, -0.0569, -0.6876], [ 1.6025, 0.6995, 0.1694]]) 1t.set_printoptions(precision = 10);a tensor([[-0.1226951405, -0.0568769276, -0.6875813603], [ 1.6024936438, 0.6995284557, 0.1693879962]])]]></content>
      <categories>
        <category>笔记</category>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[thinkload]读书-疑问]]></title>
    <url>%2F2019%2F08%2F13%2Fdeeplearning-book%2F</url>
    <content type="text"><![CDATA[2019-8-13 为什么要求算法最小化cost function却要以准确率错误率来衡量它？从cost到准确率丢失了衡量差距到底有多大的信息，这两个并不等价啊！ 如何衡量一个被选择的函数在某一目的上达到了多少效果？或者起到了多少作用？ 把输入端之前看成也连接了一个网络，这个网络是一种输出数据的网络，两个网络连接在一起看成一个大网络，而这种网络的生成目的应当是不同于后面我们人类设计网络的目的。但是输出的结果仍能一定程度上的符合人类需求，是不是说网络的每个部分可以存在不同的目标？ 有一个迷宫一样的屋子，同样也是一个挑战，找出从一楼走到天台的最短路径，有一万个人来接受挑战每个走到屋顶的人都被告知自己是否是走的最短路径，每个人都是独立的，有自己的判断标准，当他再次上楼时如何分析他们行为的变化？]]></content>
      <categories>
        <category>thinkload</category>
        <category>花书</category>
      </categories>
      <tags>
        <tag>thinkload</tag>
        <tag>花书</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【notes】docker学习笔记6-docker容器数据管理]]></title>
    <url>%2F2019%2F08%2F09%2Fdocker-6%2F</url>
    <content type="text"><![CDATA[docker容器的数据管理简介 docker容器的数据卷 docker的数据卷容器 docker数据卷的备份和还原 docker容器的数据卷什么是数据卷(Data Volume)docker的生存周期是与运行的程序相一致的，而我们需要数据持久化，docker容器之间也需要共享一些数据 数据卷是经过特殊设计的目录，可以绕过联合文件系统（UFS)，为一个或多个容器提供访问。 数据卷设计的目的，在于数据持久化，它完全对独立于容器的生存周期，因此docker不会在容器删除时删除其挂载的数据卷，也不会存在类似的垃圾收集机制，对容器引用的数据卷进行处理。 数据卷架构： docker数据卷独立于docker，独立运docker的生存周期。 docker数据卷位于docker的宿主机中文件系统。 docker数据卷既可以是目录也可是文件。 docker数据卷与宿主机进行数据共享。 同一目录或文件可以支持多个容器 数据卷的特点 数据卷在容器启动时初始化，如果容器使用的镜像在挂载点包含了数据，这些数据会拷贝到新初始化的数据卷中。 数据卷可以在容器之间共享和重用 可以对数据卷里的内容直接进行修改 数据卷的变化不会影响镜像的更新 卷会一直存在，即使挂载数据卷的容器已经被删除 数据卷的使用 为容器添加数据卷 1sudo docker run -v ~/container_data:/data -it ubuntu /bin/bash 在本机系统的目录:在容器中映射的目录名 注：这种方式（bind mount)已不推荐使用，应使用volume方式 123456docker volume create my_volume # 创建卷docker volume ls # 卷列表docker volume inspect my_volume #卷信息docker volume rm my_volume # 删除卷docker run -v [卷名]:[容器目录]:[选项列表] -it ubuntu /bin/bash 详情：https://deepzz.com/post/the-docker-volumes-basic.html 为数据卷添加访问权限 1sudo docker run -v [卷名]:[容器目录]:ro(访问权限) -it ubuntu /bin/bash 使用dockerfile构建包含数据卷的镜像dockerfile指令： VOLUME [“/data1”, “/data2”] 不能映射到本地目录，并且运行同一镜像的不同容器所创建的数据卷也是不一样的。 docker的数据卷容器什么是数据卷容器： 命名的容器挂载数据卷，其他容器通过挂载这个容器实现数据共享，挂载数据卷的容器，就叫做数据卷容器 图示： 挂载数据卷容器的方法 1docker run --volumes-from [CONTAINER NAME] 如果数据卷容器删除（即使同时删除挂载的数据卷）后，挂载该数据卷容器的容器的数据目录仍存在且有效。 数据卷容器的作用仅仅是将数据卷挂载的配置传递到挂载了数据卷容器的新容器中。 docker数据卷的备份与还原 数据备份方法 1docker run --volumes-from [container name] -v $(pwd):/backup ubuntu tar cvf /backup/backup.tar [container data volume] 数据还原方法 1docker run --volumes-from [container name] -v $(pwd):/backup ubuntu tar xvf /backup/backup.tar [container data volume]]]></content>
      <categories>
        <category>笔记</category>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【notes】docker学习笔记5-dockerfile]]></title>
    <url>%2F2019%2F08%2F08%2Fdocker-5%2F</url>
    <content type="text"><![CDATA[dockerfile指令指令格式注释： # Comment指令： INSTRUCTION argument FROM FROM \ FROM \:\ 必须已经存在的镜像，也就是基础镜像 必须是第一条非注释指令 MAINTAINER MAINTAINER \ 指定镜像的作者信息，包含镜像的所有者和练习方式 RUN构建构成运行的 RUN \ (shell模式) /bin/sh -c command RUN [“executable”, “param1”, “param2”] (exec模式) RUN[“/bin/bash”, “-c”, “echo hello”] EXPOSE EXPOSE \ [\…] 指定运行该镜像的容器使用的端口，但只是告诉docker会使用特定的端口号，出于安全考虑不会自动打开，在容器运行时仍需要手动指定端口映射。CMD ENTRYPOINT 指定容器启动时运行的命令 CMD [“executable”, “param1”, “param2”] (exec模式) CMD command param1 param2 (shell模式) CMD [“params1”, “params2”] (作为ENTRYPOINT指令的默认参数) 在docker run时如果指定命令的话dockerfile里的cmd命令会被覆盖掉。 ENTRYPOINT [“executable”, “param1”, “param2”] (exec模式) ENTRYPOINT command param1 param2 (shell模式) 默认不会被覆盖，如果需要覆盖需要指定docker run —entrypoint 覆盖 ADD COPY VOLUME设置镜像的目录和文件 ADD \…\ ADD [“]]></content>
      <categories>
        <category>笔记</category>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【notes】docker学习笔记4-docker客户端与守护进程]]></title>
    <url>%2F2019%2F08%2F07%2Fdocker-4%2F</url>
    <content type="text"><![CDATA[docker的C/S模式 客户端与守护进程通信的接口 命令行接口 remote API： RESTful风格API STDIN STDOUT STDERR 语言参考：https://docs.docker.com/reference/api/docker_remote_api 连接方式 unix:///var/run/docker.sock 默认方式 tcp://host:port fd://socketfd 利用socket进行通信 查看正在运行的守护进程 1ps -ef | grep docker 连接socket进行通信 12nc -U /var/run/docker.sockGET /info HTTP/1.1 以上都是在本地的访问，docker也支持远程访问。 docker守护进程的配置和操作 查看守护进程 12ps -ef | grep dockersudo status docker 守护进程的启动、停止和重启 123sudo service docker startsudo service docker stopsudo service docker restart docker的启动选项 1docker -d [OPTIONS] #所以守护形式运行 运行相关: -D, —debug = false -e, —exec-driver = “native” -g, —graph = “/var/lib/docker” —icc = true -l, —log-level = “info” —label = [] -p, —pidfile = “/var/run/docker.pid” docker服务器连接相关： -G, —group = “docker” -H, —host = [] —tls = false —tlscacert = “/home/sven/.docker/ca.pem” —tlscert = “/home/sven/.docker/cert.pem” —tlskey = “/home/sven/.docker/key.pem” —tlsverify = false RemotAPI相关： —api-enable-cors = false Registry相关： —insecure-registry = [] —registry-mirror = [] 网络设置相关： -b, —bridge = “” —bip = “” —fixed-cidr = “” —fixed-cidr-v6 = “” —dns = [] —dns-search = [] —ip = 0.0.0.0 —ip-forward = true —ip-masq = true —iptables = true —ipv6 = false —mtu = 0 启动配置文件 /etc/default/docker 注：ubuntu 16.04及以上版本使用： 修改/lib/systemd/system/docker.service中的ExecStart 加载配置： 123systemctl daemon-reloadservice docker restartdocker info docker的远程访问 第二台安装docker的服务器 保证Client API与Server API版本一致 修改docker守护进程启动选项 修改服务器端配置 -H tcp://host:post unix:///path/to/socket fd://* or fd//socketfd 守护进程默认配置： -H unix:///var/run/docker.sock 注：我的默认的是 fd:// 改为 tcp: tcp://0.0.0.0:2375 1curl http://ip:2375/info 修改客户端配置 -H tcp://host:post unix:///path/to/socket fd://* or fd//socketfd 默认配置： -H unix:///var/run/docker.sock docker -H tcp//ip:2375 # 太麻烦 export DOCKET_HOST=”tcp://ip:2357” # 使用环境变量 export DOCKET_HOST=”tcp://ip:2357” # 使用本地 怎样在设置了远程连接的服务器也支持本机连接？答：给-H再增加一个方式，-H可以设置多个值。]]></content>
      <categories>
        <category>笔记</category>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【notes】docker学习笔记3-docker镜像]]></title>
    <url>%2F2019%2F08%2F06%2Fdocker-3%2F</url>
    <content type="text"><![CDATA[查看和删除镜像 镜像的存储位置：/var/lib/docker 列出镜像1docker images [OPSIONS] [REPOSITORY] -a, —all = false # 显示所有镜像，默认并不显示中间层镜像（没有标签名的镜像） -f, —filter = [] # 过滤条件 —no-trunc = false # 不使用截断的形式来显示数据(默认使用截断显示EID，比文件名短) -q, —quiet = false # 只显示EID 镜像标签和仓库 镜像仓库 区分： REPOSITORY 仓库 REGISTRY 仓库 REGISTRY里会有很多REPOSITORY仓库，每个REPOSITORY里会有一个个独立的镜像。 标签 TAG 镜像的名字 = 镜像仓库名 : 镜像标签 —对应—&gt; 镜像ID ubuntu:latest, ubuntu:14.04, ….. 如果没有指定标签，默认为latest。 同一仓库的不同标签可以对应同一镜像ID，也就是说可以根据需求给同一镜像文件打上不同的标签。 没有标签名的镜像称作中间层镜像。 查看镜像1docker inspect [OPTIONS] CONTIANER|IMAGE [CONTAINER|IMAGE...] -f, —format=”” 删除镜像1docker rmi [OPTIONS] IMAGE [IMAGE...] -f, —force = false 强制删除 —no-prune = false 不删除未打标签的父镜像 对应多个标签的镜像文件可以直接用ID选定所有标签 1docker rmi ID 获取和推送镜像查找镜像 Docker Hub https://registry.hub.docker.com docker search 1docker search [OPTIONS] TERM —automated = false —no-trunc = false -s, stars = 0 只显示最少多少stars的 最多返回25个结果 拉取镜像1docker pull [OPTIONS] NAME [:TAG] -a, —all-tags = false 下载仓库中所有被标记的镜像 推送镜像1docker push username/IMAGE 构建镜像 保存对容器的修改，并再次使用 自定义镜像的能力 以软件的形式打包并分发服务及其运行环境 docker commit通过容器构建 1docker commit [OPTIONS] CONTAINER [REPOSITORY[:TAG]] -a, —author=”” Author e.g., “John Hannibal Smith hannibal@a-team.com” -m, —message=”” 记录构建的信息 -p, —pause = true 不暂停容器的运行 docker build通过Dockerfile文件构建 dockerfile: #First DockerfileFROM ubuntu:14.04MAINTAINER dormancypress “dormancypress@outlook.comRUN apt-get updateRUN apt-get install -y nginxEXPOSE 80 1docker build [OPTIONS] PATH|URL|- —force-rm = false —no-cache = false —pull=false -q,—quiet = false —rm = true -t, —tag=””]]></content>
      <categories>
        <category>笔记</category>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【notes】docker学习笔记2-docker容器]]></title>
    <url>%2F2019%2F08%2F06%2Fdocker-2%2F</url>
    <content type="text"><![CDATA[容器的基本操作启动容器 仅一次命令 1docker run IMAGE [COMMAND] [ARG] 启动交互式容器 1docker run -i -t IMAGE /bin/bash -i —interactive=ture | false 默认是false，为容器始终打开标准输入-t —tty=true | false 默认是false，分配一个终端 自定义容器名字 1docker run --name=自定义名 -i -t IMAGE /bin/bash 重新启动已建立的容器docker start [-i] 容器名 查看容器 不加参数是正在运行的容器，-a是所有容器，-l是最新创建的一个容器。 1docker ps [-a] [-l] 查看容器参数 1docker inspect [ID] or [name] 删除容器1docker rm 容器名 守护式容器什么是守护式容器： 能够长期运行 没有交互式会话 以守护形式运行容器：12docker run -i -t IMAGE /bin/bashCtrl + P Ctrl + Q 附加到运行中的容器1docker attach 容器名 启动守护式容器1docker run -d 镜像名 [COMMAND] [ARG...] 得知容器运行情况1docker logs [-f] [-t] [--tail] 容器名 -f —follows=true | false 默认为false 一直跟踪日志变化并返回结果-t —timestamps=true | false 默认为false 结果加上时间戳—tail= “all” 多少数量的日志 查看运行中容器进程1docker top 容器名 在运行中的容器内启动新进程1docker exec [-d] [-i] [-t] 容器名 [COMMAND] [ARG...] 停止守护式容器 发送指令等待停止 1docker stop 容器名 直接停止容器 1docker kill 容器名 在容器中部署静态网站设置容器的端口映射run [-P] -P , —publish-all = true | false 默认为false 为容器暴露的所有端口设置映射 1docker run -P -t -i ubuntu /bin/bash -p , 指定端口 容器端口 1docker run -p 80 -i -t ubuntu /bin/bash 宿主机端口:容器端口 1docker run -p 8080:80 -i -t ubuntu /bin/bash ip::容器端口 1docker run -p 0.0.0.0:80 -i -t ubuntu /bin/bash ip:宿主机端口:容器端口 1docker run -p 0.0.0.0:8080:80 -i -t ubuntu /bin/bash Nginx部署 创建映射80端口的交互式容器 1docker run -p 80 --name web -it ubuntu /bin/bash 安装Nginx 安装文本编辑器vim 1234apt-get updateapt-get upgradeapt-get install nginx -yapt-get install vim -y 创建静态页面 12mkdir -p /var/www/htmlvim index.html 修改Nginx配置文件 1vim /etc/nginx/sites-enabled/default 运行Nginx 123nginxps -efCtrl P Ctrl Q 验证网站访问 1234docker port web # 查看端口映射情况docker top web # 查看进程运行情况docker inspect web #查看ipcurl http://172.17.0.2]]></content>
      <categories>
        <category>笔记</category>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【notes】docker学习笔记1-docker基本组成]]></title>
    <url>%2F2019%2F08%2F06%2Fdocker-1%2F</url>
    <content type="text"><![CDATA[Docker的基本组成 Docker Client 客户端 Docker Daemon 守护进程 Docker Image 镜像 Docker Container 容器 Docker Registry 仓库 Docker客户端/守护进程 C/S架构 docker客户端对服务器的访问： 本地/远程 docker客户端向发送给守护进程请求，守护进程的执行结果还会传回给客户端。 Docker Image镜像 构建和打包阶段。 容器的基石，相当于保存了容器运行需要的源代码。 层叠的层叠文件系统。 bootfs（引导文件系统）-&gt; rootfs(Ubuntu) -&gt; add emacs -&gt; add Apache 联合加载（union mount）:一次加载多个文件系统（add Apache，add emacs），将所有文件系统叠加在一切。镜像可以叠加在一起，位于底部的成为基础镜像（rootfs），add emacs（副镜像）。 Docker Container容器 通过镜像启动。 启动执行阶段。 配置数据和镜像层（bootfs -&gt; ······ -&gt; add emacs) -&gt; 可写层。 写时复制：docker出现变化时都会应用到可写层，先从只读镜像层复制到可写层然后只读层的文件被隐藏。 Docker Registry仓库 保存docker镜像。 分为公有和私有。公有：Docker Hub 图示结构Docker: Docker Image: Docker Container: docker基本指令 查找镜像 1docker search tutorial 下载镜像 1docker pull learn/tutorial 启动一个容器，使用echo命令输出hello world 1docker run learn/tutorial echo 'hello world' 启动一个容器下载ping 1docker run learn/tutorial apt-get install -y ping 查看有哪些容器 1docker ps -l 提交容器，即创建一个新的镜像 1docker commit [docker ID] learn/ping 用新镜像建立一个容器 1docker run learn/ping ping www.baidu.com 查看容器信息 1docker inspect [docker ID] 查看有哪些镜像 1docker image 将镜像保存到docker hub上 1docker push /learn/ping Docker容器相关技术简介Docker依赖的Linux内核特性 Namespaces 命名空间 提供了系统资源的隔离，for轻量级虚拟化服务 五种命名空间： PID 进程隔离 NET 管理网络接口 IPC 管理跨进程通信的访问 MNT 管理挂载点 UTS 隔离内核和版本标识 Control groups 控制组 资源限制（内存上限等） 优先级设定（设定哪些进程组使用哪些资源） 资源计量 资源控制（挂起恢复） Docker容器的能力 文件系统隔离：每个容器都有自己的root文件系统 进程隔离： 每个容器都运行在自己的进程环境中 网络隔离： 容器间的虚拟网络接口和IP地址都是分开的 资源隔离和分组：使用cgroups将CPU和内存之类的资源独立分配给每个Docker容器]]></content>
      <categories>
        <category>笔记</category>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用Xshell管理虚拟机Ubuntu]]></title>
    <url>%2F2019%2F08%2F03%2Fxshell-vmware%2F</url>
    <content type="text"><![CDATA[因为使用VM虚拟机太过占用资源，所以我们可以用Xshell连接到虚拟机，来达到节省本机资源的目的。 安装SSH： 123sudo apt-get install openssh-serverservice iptables stop #关闭防火墙service ssh start #开启ssh服务 获得登录需要的ip ,在虚拟机输入： 1ifconfig ens*后面的inet后面的值就是ip。 按照我之前写过的xshell连接的教程 windows系统：Xshell下载安装+连接服务器 建立会话就ok，主机就是刚才你获得的ip，登录用的用户名和密码就是你安装时填的用户名(非root账户)和密码。 之后只需要打开虚拟机后最小化界面，从xshell登入后reboot一下虚拟机，这样从内存角度就能节省将近90多MB。 注意： reboot后就不要在打开VMware了，一直让它最小化直到关闭。]]></content>
      <categories>
        <category>Xshell</category>
      </categories>
      <tags>
        <tag>Xshell</tag>
        <tag>虚拟机</tag>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[photoshop cc 2019安装破解]]></title>
    <url>%2F2019%2F08%2F02%2Fphotoshop-cc-2019-download%2F</url>
    <content type="text"><![CDATA[Photoshop如今已经非常常用的处理图片的软件，本文就是介绍一下photoshop cc 2019安装破解的完整过程。 注：本文参考了http://www.3322.cc/soft/48343.html 下载creative cloud什么是creative cloud？creative cloud相当于adobe系列的一个应用商城，我们可以在里面安装各种adobe系列的软件。下载链接： 官网链接 网盘链接 下载完成直接按提示安装，然后注册adobe账号并登陆。 下载安装photoshop-cc-2019默认的下载位置在c盘，如果想改到其他盘可以点击右上角的三个点，出来菜单再点首选项。 然后点击creative Cloud界面，在安装位置条目处更改到你想安装到的位置。 打开creative cloud，找到photoshop的条目点击试用，photoshop自动下载安装成功。 利用补丁破解安装完成后安全起见先不要打开ps，我们先下载补丁工具。下载链接：网盘链接 其他链接 将压缩包里的adobe.photoshop.cc.2019.20.0.0-patch.exe文件解压到ps安装目录下，就是你刚才修改的安装位置，保证那个位置下有photoshop.exe文件。 然后点击运行补丁（你可能会听到一段诡异的音乐。。。）。 点击应用，等待出现文件补丁已成功完成的提示。 这样就破解完成了，这时再打开ps发现没有试用还有多少天的提醒了。 按照补丁制作者的建议，在 编辑 ==&gt; 首选项 ==&gt; 常规 ==&gt; 停用”主页”屏幕 打钩。 最后做好重启一下ps再试用。 注：这篇文章是我安装后就写了，我在安装完的七天后再次检验是否失效，如果失效我会更新补丁，如果补丁失效可以回来看我是否有更新方法。]]></content>
      <categories>
        <category>photoshop</category>
      </categories>
      <tags>
        <tag>破解</tag>
        <tag>photoshop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Xshell：在本地浏览器使用服务器的jupyter notebook]]></title>
    <url>%2F2019%2F08%2F02%2Fhexo-jupyter%2F</url>
    <content type="text"><![CDATA[有的服务器里只是命令行，无法可视化，可能就无法使用jupyter notebook。其实需要稍微修改一下连接的属性就能在本地浏览器里打开在服务器里启动的jupyter notebook，具体操作如下： 首先右击会话管理器里的服务器标签，在菜单点击属性。 然后点击左侧的隧道，然后再点击添加。 输入两个端口号，我这输入的是jupyter notebook默认的8888端口，然后点确定 然后再取消右下方转发X11连接到的选项，然后点确定。 之后双击会话管理器里的服务器进行连接，在命令行里输入jupyter notebook，启动后在浏览器里访问就会看到jupyter notebook的界面了。]]></content>
      <categories>
        <category>Xshell</category>
      </categories>
      <tags>
        <tag>Xshell</tag>
        <tag>jupyter notebook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[windows系统：Xshell下载安装+连接服务器]]></title>
    <url>%2F2019%2F08%2F01%2FXshellDownload%2F</url>
    <content type="text"><![CDATA[学习深度学习需要足够的计算资源，往往需要连接远程服务器用来计算。本篇文章就介绍一下如何在windows系统里利用xshell连接服务器。 xshell下载安装首先要下载安装包:百度网盘资源。当然也可以去官网下载安装包，选择家庭学校免费版，下载前要填一下姓名邮箱，提交后你会收到带有下载链接的邮件。 点击安装包，然后一路默认下一步就ok。如果不想安装在c盘也可以，在其他盘里专门存xshell的各种文件，安装过程中只需把主文件夹换成你刚才的文件夹就ok。 建立连接 打开xshell后，点击左上角的文件然后点新建。 然后填入服务器名称、主机、端口号,然后点确定。 双击左侧会话管理器里刚建的服务器，在弹出的窗口里填入登录用的用户名，选上记住用户名。 然后输入密码，并选上记住密码。 点击确定以后就能在黑色的shell看到已经登录成功的提示了，然后就可以在shell里进行操作了。 之后登录只需要双击左侧会话管理器里的对应标签即可。 上传下载文件 在Linux主机上，安装上传下载工具包rz及sz，使用sudo apt install lrzsz 进行安装。 从Windows上传文件，上传命令为rz；输入命令后会弹出选择要上传的本地文件的窗口。 从Linux主机下载文件，下载命令为sz ，后面跟要下载的文件名。例如： sz helloworld.py。 然后就会弹出选择要保存到本机位置的窗口。 xshell的基本操作就说这些了，这些的操作已经基本够用了]]></content>
      <categories>
        <category>Xshell</category>
      </categories>
      <tags>
        <tag>Xshell</tag>
        <tag>linux服务器</tag>
        <tag>windows</tag>
      </tags>
  </entry>
</search>
