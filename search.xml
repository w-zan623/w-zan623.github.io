<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[【notes】pytorch学习笔记3-神经网络工具箱nn]]></title>
    <url>%2F2019%2F08%2F22%2Fnn.Module%2F</url>
    <content type="text"><![CDATA[神经网络工具箱nnautograd实现了自动微分系统，然而对于深度学习来说过于底层。nn模块是构建与autograd之上的神经网络模块。除了nn之外，我们还会介绍神经网络中常用的工具，比如优化器optim,初始化init等。 nn.Moduletorch.nn的核心数据结构是Module，它是一个抽象的概念，既可以表示神经网络中的某个层（layer），也可以表示一个包含很多层的神经网络。在实际使用中，最常见的做法是继承nn.Module，撰写自己的网络、层。 全连接层，又名仿射层，输出$\boldsymbol{x}$和输入$\boldsymbol{x}$满足$\boldsymbol{y=Wx+b}$，$\boldsymbol{W}$ 和 $\boldsymbol{b}$ 是可学习函数。 123import torch as tfrom torch import nnfrom torch.autograd import Variable as V 12345678910class Linear(nn.Module): def __init__(self, in_features, out_features): super(Linear, self).__init__() #等价于nn.Module.__init__(self) self.w = nn.Parameter(t.randn(in_features, out_features)) self.b = nn.Parameter(t.randn(out_features)) def forward(self, x): x = x.mm(self.w) return x + self.b.expand_as(x) 1234layer = Linear(4, 3)input = V(t.randn(2, 4))output = layer(input)output tensor([[ 2.0269, 5.1465, 1.5603], [-0.6868, -0.8096, -0.6427]], grad_fn=&lt;AddBackward0&gt;) 123for name, parameter in layer.named_parameters(): print(name, parameter) w Parameter containing: tensor([[-0.0121, -0.2593, -0.5310], [ 0.2982, -0.2846, -0.0437], [ 0.6220, 1.7351, 0.8025], [ 1.0544, 2.3325, 0.6561]], requires_grad=True) b Parameter containing: tensor([0.2586, 2.3734, 0.5372], requires_grad=True) 但需要注意一下几点： 自定义层Linear必须继承nn.Module，并且在其构造函数中需调用nn.Module的构造函数，即super(Linear, self).init()或nn.Module.__init__(self)。 在构造函数__init__中必须自己定义可学习的参数，并封装成Parameter，如在本例中我们把w和b封装成Parameter。Parameter是一种特殊的Variable，但其默认需要求导（requires_grad=True）。 forward函数实现前向传播过程，其输入可以是一个或多个variable，对x的任何操作也必须是variable支持的操作。 无需写反向传播函数，因其前向传播都是对variable进行操作，nn.Module能够利用autograd自动实现反向传播，这一点比Function简单许多。 使用时，直观上可将layer看成数学概念中的函数，调用layer(input)即可得到input对应的结果。它等价于layers.__call__(input)，在__call__函数中，主要调用的是layer.forward(X)，另外还对钩子做了一些处理。所以在实际使用中应尽量使用layer(x)而不是使用layer.forward(x)。 Module能够自动检测到自己的parameter，并将其作为学习参数。除了parameter，Module还包含子Module，主Module能够递归查找子Module中的parameter。 下面看一个稍微复杂的网络：多层感知机。 123456789class Perceptron(nn.Module): def __init__(self, in_features, hidden_features, out_features): nn.Module.__init__(self) self.layer1 = Linear(in_features, hidden_features) self.layer2 = Linear(hidden_features, out_features) def forward(self, x): x = self.layer1(x) x = t.sigmoid(x) return self.layer2(x) 123perceptron = Perceptron(3, 4, 1)for name, param in perceptron.named_parameters(): print(name, param.size()) layer1.w torch.Size([3, 4]) layer1.b torch.Size([4]) layer2.w torch.Size([4, 1]) layer2.b torch.Size([1]) 注意一下两个知识点。 构造函数__init__中，可利用前面自定义的Linear层（module）作为当前module对象的一个字module，它的可学习参数，也会成为当前module的可学习参数。 在前向传播函数中，我们有意识地将输出变量都命名为x，是为了能让python回收一些中间层的输出，从而节省内存。但并不是所有的中间结果都会被回收，有些variable虽然名字被覆盖，但其在反向传播时仍需要用到，此时python的内存回收模块将通过检查引用计数，不会回收这一部分内存。 module中parameter的全局命名规范如下。 parameter直接命名。例如self.param_name = nn.Parameter(t.randn(3,4))，命名为param_name。 子module中的parameter，会在其名字之前加上当前module的名字，就是sub_module.param_name。 为了方便用户使用，pytorch实现了神经网络中绝大多数的layer，这些layer都继承了nn.Module，封装了可学习参数parameter，并实现了forward函数，且专门针对GPU运算进行了CuDNN优化。具体内容可参考官方文档或在IPython/Jupyter中使用nn.layer。 阅读文档注意： 构造函数的参数，如nn.Linear(in_features, out_features, bias)，需关注着三个参数的作用 属性、可学习参数和子module。如nn.Linear中有weight和bias两个可学习参数，不包含子module 输入输出的形状，如nn.Linear的输入形状是(N, input_features)，输出为(N, output_features)，N是batch_size。若想输入一个数据需要调用unsqueeze(0)函数将数据伪装成batch_size = 1的batch 常用的神经网络层图像相关层图像相关层主要包括卷积层（Conv）、池化层（Pool）等，这些层在实际使用中可分为一维、二维和三维，池化层又分为平均池化（AvgPool）、最大值池化（MaxPool）、自适应池化（AdaptiveAvgPool）等。卷积层除了常用的前向卷积外，还有逆卷积（TransposeConv）。 123456789import torch as tfrom torch import nnfrom torch.autograd import Variable as Vfrom PIL import Imagefrom torchvision.transforms import ToTensor, ToPILImageto_tensor = ToTensor()to_pil = ToPILImage()curry = Image.open('curry')curry 1234567891011# 输入一个batch，batch_size = 1input = to_tensor(curry).unsqueeze(0)# 锐化卷积核kernel = t.ones(3, 3) / -9kernel[1][1] = 1conv = nn.Conv2d(1, 1, (3, 3), 1, bias = False)conv.weight.data = kernel.view(1, 1, 3, 3)out = conv(V(input))to_pil(out.data.squeeze(0)) Shape: Input: $(N, C_{in}, H_{in}, W_{in})$ Output: $(N, C_{out}, H_{out}, W_{out})$ where$H_{out} = \left\lfloor\frac{H_{in} + 2 \times \text{padding}[0] - \text{dilation}[0]\times (\text{kernel_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor$ $W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[1] - \text{dilation}[1]\times (\text{kernel_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor$ 图像的卷积操作还有各种变体，有关各种变体的介绍可以参照此处的介绍https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md 池化层可以看作是一种特殊的卷积层，用来下采样。但池化层没有可学习参数，其weight是固定的。 12pool = nn.AvgPool2d(2, 2)list(pool.parameters()) [] 12out = pool(V(input))to_pil(out.data.squeeze(0)) 除了卷积层和池化层，深度学习中还将常用到一下几个层 Linear：全连接层 BatchNorm：批规范化层，分为1D、2D和3D。除了标准的BatchNorm之外，还有在风格迁移中常用到的InstanceNorm层。 Dropout：dropout层，用来防止过拟合，同样分为1D、2D和3D。 1234# 输入batch_size=2，维度3input = V(t.randn(2, 3))linear = nn.Linear(3, 4)h = linear(input);h tensor([[-0.4360, 0.3433, -0.1978, -0.3128], [-0.9655, 0.6278, 0.2510, 0.1256]], grad_fn=&lt;AddmmBackward&gt;) 12345678910# 4 channel,初始化标准差为4，均值为0bn = nn.BatchNorm1d(4)bn.weight.data = t.ones(4) * 4bn.bias.data = t.zeros(4)bn_out = bn(h)# 注意输出的均值和方差# 方差是标准差的平方，计算无偏方差分母会减1# 使用unbiased=False，分母不减1bn_out.mean(0), bn_out.var(0, unbiased = False) (tensor([-1.1921e-07, 0.0000e+00, 0.0000e+00, 0.0000e+00], grad_fn=&lt;MeanBackward1&gt;), tensor([15.9977, 15.9921, 15.9968, 15.9967], grad_fn=&lt;VarBackward1&gt;)) 123456# 每个元素以0.5的概率舍弃dropout = nn.Dropout(0.5)o = dropout(bn_out)# 有一半的概率会变成0o tensor([[7.9994, -0.0000, -0.0000, -0.0000], [-0.0000, 7.9980, 7.9992, 7.9992]], grad_fn=&lt;MulBackward0&gt;) 以上很多例子中都对module的属性直接操作，其大多数是可学习参数，一般会随着学习的进行而不断改变。实际使用中除非需要使用特殊的初始化，否则尽量不要直接改变参数。 激活函数pytorch实现了常见的激活函数。其他具体的接口信息可参见官方文档，这些激活函数可以作为独立的layer使用。这里介绍最常用的激活函数ReLU，其数学表达式为： ReLU(x)=max(0,x)123456relu = nn.ReLU(inplace = True)input = V(t.randn(2, 3))print(input)output = relu(input)print(output) # 小于0的都被截断为0id(input) == id(output) tensor([[ 0.5049, 0.6093, -0.1565], [-0.9114, -0.9594, 1.0539]]) tensor([[0.5049, 0.6093, 0.0000], [0.0000, 0.0000, 1.0539]]) True ReLU函数有个inplace参数，如果设为True，如果设为True，它会把输出直接覆盖到输入中，这样可以节省内存/显存。之所以可以覆盖是因为在计算ReLU的反向传播时，只需根据输出就能够推算出反向传播的梯度。但是只有少数的autograd操作支持inplace操作（如variable.sigmoid_()），除非你明确知道自己在做什么，否则一般不要使用inplace操作。 在以上例子里，都是将每一层的输出直接作为下一层的输入，这种网络成为前馈传播网络。对于此种网络，如果每次都写复杂的forward函数会有些麻烦，在此就有两种简化方式，ModuleList和Sequential。其中Sequential是一个特殊的Module，它包含几个子module，前向传播时会将输入一层接一层第传递下去。ModuleList也是一个特殊的Module，可以包含几个子Module，可以像用list一样使用它，但不能直接把输入传给ModuleList。 12345678910111213141516171819202122# Sequential的三种写法net1 = nn.Sequential()net1.add_module('conv', nn.Conv2d(3, 3, 3)) # 输入为(N, C_&#123;in&#125;, H_&#123;in&#125;, W_&#123;in&#125;)，参数为net1.add_module('batchnorm', nn.BatchNorm2d(3)) # 3为(N, C, H, W)中的Cnet1.add_module('activation_layer', nn.ReLU())net2 = nn.Sequential( nn.Conv2d(3, 3, 3), nn.BatchNorm2d(3), nn.ReLU() )from collections import OrderedDictnet3 = nn.Sequential(OrderedDict([ ('conv1', nn.Conv2d(3, 3, 3)), ('bn1', nn.BatchNorm2d(3)), ('relu1', nn.ReLU()),]))print('net1', net1)print('net2', net2)print('net3', net3) net1 Sequential( (conv): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1)) (batchnorm): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (activation_layer): ReLU() ) net2 Sequential( (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1)) (1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) net3 Sequential( (conv1): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1)) (bn1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU() ) 12# 可根据名字或序号取出子modulenet1.conv, net2[0], net3.conv1 (Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1)), Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1)), Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))) 12input = V(t.rand(1, 3, 4, 4))net1(input), net2(input), net3(input), net3.relu1(net2[1](net1.conv(input))) (tensor([[[[0.0000, 1.4727], [0.1600, 0.0000]], [[0.0000, 0.0000], [0.7015, 1.1069]], [[1.7189, 0.0000], [0.0000, 0.0000]]]], grad_fn=&lt;ReluBackward0&gt;), tensor([[[[0.0000, 1.6957], [0.0000, 0.0000]], [[1.2454, 0.6350], [0.0000, 0.0000]], [[1.0204, 0.4811], [0.1430, 0.0000]]]], grad_fn=&lt;ReluBackward0&gt;), tensor([[[[0.0000, 1.5585], [0.1751, 0.0000]], [[0.0000, 1.4177], [0.1846, 0.0000]], [[0.0000, 0.0000], [1.3537, 0.2417]]]], grad_fn=&lt;ReluBackward0&gt;), tensor([[[[0.0000, 1.4727], [0.1600, 0.0000]], [[0.0000, 0.0000], [0.7015, 1.1069]], [[1.7189, 0.0000], [0.0000, 0.0000]]]], grad_fn=&lt;ReluBackward0&gt;)) 1234567modulelist = nn.ModuleList([nn.Linear(3, 4), nn.ReLU(), nn.Linear(4, 2)])input = V(t.rand(1, 3))for model in modulelist: input = model(input)# 下面会报错，因为modellist没有实现forward方法# output = modellist(input) 为何不直接使用python中自带的list，而非要多次一举呢？这是因为ModuleList是Module的子类，当在Module中使用它时，就能自动识别为子module。 1234567891011class MyModule(nn.Module): def __init__(self): super(MyModule, self).__init__() self.list = [nn.Linear(3,4), nn.ReLU()] # 直接用list self.module_list = nn.ModuleList([nn.Conv2d(3, 3, 3), nn.ReLU()]) # 用nn.ModuleList def forward(self): pass model = MyModule()model MyModule( (module_list): ModuleList( (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1)) (1): ReLU() ) ) 123for name, param in model.named_parameters(): print(name, param.size()) module_list.0.weight torch.Size([3, 3, 3, 3]) module_list.0.bias torch.Size([3]) 可见，list中的子module并不能被主module识别，而ModuleList中的子module能够被主module识别。 除ModuleList之外还有ParameterList，它是一个可以包含多个parameter的类list对象。在实际应用中，使用方式和ModuleList类似。在构造函数__init__中用到list、tuple、dict等对象，一定要思考是否应该用ModuleList或ParameterList代替。 损失函数在深度学习中会用到各种各样的损失函数，这些损失函数可看作是一种特殊的layer，pytorch也将这些损失函数实现为nn.Module的子类。然而在实际使用中通常将这些损失函数专门提取出来，作为独立的一部分。 123456789#batch_size = 3, 计算对应每个类别的分数score = V(t.randn(3, 10)) # (N, C) N是batch_size，C是class的个数# 三个样本分别属于1， 0， 1类，label必须是LongTensorlabel = V(t.Tensor([1, 0, 9])).long()# loss与普通的layer无差异criterion = nn.CrossEntropyLoss()loss = criterion(score, label)loss tensor(2.8392) 优化器pytorch将深度学习中常用的优化方法全部封装到torch.optim中，其设计十分灵活，能够很方便地扩展自定义的优化方法。所有的优化方法都是继承基类optim.Optimizer，并实现了自己的优化步骤。下面就以最基本的优化方法————随机梯度下降法（SGD）举例说明。这里需要重点掌握： 优化方法的基本使用方法 如何对模型的不同部分设置不同的学习率 如何调整学习率 12345678910111213141516171819202122232425262728# 首先定义一个LeNet网络class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.features = nn.Sequential( nn.Conv2d(3, 6, 5), nn.ReLU(), nn.MaxPool2d(2, 2), nn.Conv2d(6, 16, 5), nn.ReLU(), nn.MaxPool2d(2, 2) ) self.classifier = nn.Sequential( nn.Linear(16 * 5 * 5, 120), nn.ReLU(), nn.Linear(120, 84), nn.ReLU(), nn.Linear(84, 10), ) def forward(self, x): x = self.features(x) x = x.view(-1, 16 * 5 * 5) x = self.classifier(x) return xnet = Net() 123456789from torch import optimoptimizer = optim.SGD(params = net.parameters(), lr = 1)optimizer.zero_grad() # 梯度清零，等价于net.zero_grad()input = V(t.randn(1, 3, 32, 32))output = net(input)output.backward(output) # fake backwardoptimizer.step() # 执行优化 123# 为不同子网络参数设置不同的学习率，在finetune中经常用到# 如果对某个参数不指定学习率，就是用默认学习率optimizer = optim.SGD([&#123;'params':net.features.parameters()&#125;,&#123;'params':net.classifier.parameters(), 'lr':1e-2&#125;], lr = 1e-5) 123456789# 只为两个全连接层设置较大的学习率，其余层的学习率较小special_layers = nn.ModuleList([net.classifier[0],net.classifier[2]])special_layers_params = list(map(id, special_layers.parameters()))base_params = filter(lambda p: id(p) not in special_layers_params, net.parameters())optimizer = t.optim.SGD([ &#123;'params':base_params&#125;, &#123;'params':special_layers.parameters(), 'lr': 0.01&#125;], lr = 0.001) 调整学习率主要有两种做法。一种是修改optimmizer.param_groups中对应的学习率。另一种是新建优化器（更简单也是更推荐的做法），由于optimizer十分轻量级，构建开销很小，故可以构建新的optimizer。但是新建优化器会重新初始化动量等状态信息，这对使用动量的优化器来说（如带momentum的sgd），可能会造成损失函数在收敛过程中出震荡。 123456# 调整学习率，新建一个optimizerold_lr = 0.1optimizer = optim.SGD([ &#123;'params': net.features.parameters()&#125;, &#123;'params': net.classifier.parameters(), 'lr':old_lr *0.1&#125;],lr = 1e-5) nn.functionalnn中还有一个常用的模块：nn.functional。nn中的大多数layer在functional中都有一个与之相对应的函数。nn.functional中的函数和nn.Module的主要区别在于，用nn.Module实现的layers是一个特殊的类，都是由class Layer(nn.Module)定义，会自动提取可学习的参数；而nn.functional中的函数更像是纯函数，由def function(input)定义。 12345input = V(t.randn(2, 3))model = nn.Linear(3, 4)output1 = model(input)output2 = nn.functional.linear(input, model.weight, model.bias)output1 == output2 tensor([[True, True, True, True], [True, True, True, True]]) 123b = nn.functional.relu(input)b2 = nn.ReLU()(input)b == b2 tensor([[True, True, True], [True, True, True]]) 应该什么时候使用nn.Module，什么时候使用nn.functional？如果模型有可学习的参数，最好用nn.Module，否则既可以使用nn.functional也可以使用nn.Module，二者在性能上没有太大差异，具体的使用方式取决于个人喜好。但建议还是使用nn.Dropout而不是nn.functional.dropout，因为dropout在训练和测试两个阶段的行为有所差别，使用nn.Module对象能够通过model.eval操作以一区分。 1234567891011121314151617from torch.nn import functional as Fclass Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.conv2 = nn.Conv2d(6, 16, 5) self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): x = F.max_pool2d(F.relu(self.conv1(x)), 2) x = F.max_pool2d(F.relu(self.conv2(x)), 2) x = x.view(-1, 16 * 5 * 5) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x 123net = Net()x = t.randn(1, 3, 32, 32)net(x) tensor([[-0.0539, 0.0145, 0.0214, 0.0474, -0.0516, 0.0890, 0.0539, 0.0805, 0.0785, -0.1043]], grad_fn=&lt;AddmmBackward&gt;) 不具备可学习参数的层（激活层、池化层等），将它们用函数代替，这样可以不用放置在构造函数__init__中。有可学习参数的模块，也可以用functional代替，只不过实现起来较繁琐，需要手动定义参数parameter。 123456789class MyLinear(nn.Module): def __init__(self): super(MyLinear, self).__init__() self.weight = nn.Parameter(t.randn(3, 4)) self.bias = nn.Parameter(t.zeros(3)) def forward(self,input): return F.linear(input, self.weight, self.bias) 123x = t.randn(1, 4)linear = MyLinear()linear(x) tensor([[-0.0678, 2.5530, 0.8512]], grad_fn=&lt;AddmmBackward&gt;) 初始化策略在深度学习中参数的初始化十分重要，良好的初始化能使模型收敛更快，并达到更高水平，而糟糕的初始化可能使模型迅速崩溃。pytorch中nn.Module的模块参数都采取了较合理的初始化策略，因此一般不用我们考虑。当然我们可以用自定义的初始化代替系统的默认初始化。自定义初始化尤为重要，因为t.Tensor()返回的是内存中的随机数，很可能会有极大值，这在实际训练网络中会造成溢出或者梯度消失。pytorch中的nn.init模块专门为初始化设计，实现了常用的初始化策略。如果某种初始化策略nn.init不提供，用户也可以自己直接初始化。 123456# 利用nn.init初始化from torch.nn import initlinear = nn.Linear(3, 4)t.manual_seed(1)# 等价于linear.weight.data.normal_(0, std)init.xavier_normal_(linear.weight) Parameter containing: tensor([[ 0.3535, 0.1427, 0.0330], [ 0.3321, -0.2416, -0.0888], [-0.8140, 0.2040, -0.5493], [-0.3010, -0.4769, -0.0311]], requires_grad=True) 12345import matht.manual_seed(1)std = math.sqrt(2)/math.sqrt(7.)linear.weight.data.normal_(0, std) tensor([[ 0.3535, 0.1427, 0.0330], [ 0.3321, -0.2416, -0.0888], [-0.8140, 0.2040, -0.5493], [-0.3010, -0.4769, -0.0311]]) 12345678910# 对模型的所有参数进行初始化for name, params in net.named_parameters(): if name.find('linear') != -1: print(params[0]) # weight print(params[1]) # bias elif name.find('conv') != -1: pass elif name.find('norm') != -1: pass nn.Module深入分析如果想深入地理解nn.Module，研究其原理是很有必要的。首先来看看nn.Module基类的构造函数的源代码： 1234567def __init__(self): self._parameters = OrderedDict() self._modules = OrderedDict() self._buffers = OrderedDict() self._backward_hooks = OrderedDict() self._forward_hooks = OrderedDict() self.training = True 其中每个属性的解释如下： _parameters：字典。保存用户直接设置的parameter，self.param1 = nn.Parameter(t.randn(3, 3))会被检测到，在字典中加入一个key为param，value为对应parameter的item，而self.submodule = nn.Linear(3, 4)中的parameter则不会存于此。 _modules：子module。通过self.submodule = nn.Linear(3, 4)指定的子module会保存于此。 _buffers：缓存。如batchnorm使用momentum机制，每次前向传播需用到上一次前向传播的结果。 _backward_hooks与_forward_hooks：钩子技术，用来提取中间变量，类似variable的hook training：BatchNorm与Dropout层在训练阶段和测试阶段中采取的策略不同，通过判断training值决定前向传播策略。 上述几个属性中，_parameters、_modules和_buffers这三个字典中的键值，都可以通过self.key方式获得，效果等价于self._parameters[‘key’] 123456789101112class Net(nn.Module): def __init__(self): super(Net, self).__init__() # 等价于self.register_parameter('param1', nn.Parameter(t.randn(3, 3))) self.param1 = nn.Parameter(t.rand(3, 3)) self.submodel1 = nn.Linear(3, 4) def foward(self, input): x = self.param1 * input x = self.submodel1(x) return xnet = Net()net Net( (submodel1): Linear(in_features=3, out_features=4, bias=True) ) 1net._modules OrderedDict([(&#39;submodel1&#39;, Linear(in_features=3, out_features=4, bias=True))]) 1net._parameters OrderedDict([(&#39;param1&#39;, Parameter containing: tensor([[0.3398, 0.5239, 0.7981], [0.7718, 0.0112, 0.8100], [0.6397, 0.9743, 0.8300]], requires_grad=True))]) 1net.param1 == net._parameters['param1'] tensor([[True, True, True], [True, True, True], [True, True, True]]) 12for name, param in net.named_parameters(): print(name, param.size()) param1 torch.Size([3, 3]) submodel1.weight torch.Size([4, 3]) submodel1.bias torch.Size([4]) 12for name, submodel in net.named_modules(): print(name, submodel) Net( (submodel1): Linear(in_features=3, out_features=4, bias=True) ) submodel1 Linear(in_features=3, out_features=4, bias=True) 1234bn = nn.BatchNorm1d(2)input = V(t.rand(3,2), requires_grad = True)output = bn(input)bn._buffers OrderedDict([(&#39;running_mean&#39;, tensor([0.0362, 0.0596])), (&#39;running_var&#39;, tensor([0.9009, 0.9262])), (&#39;num_batches_tracked&#39;, tensor(1))]) nn.Module在实际使用中可能层层嵌套，一个module包含若干个子module，每一个子module又包含了更多的子module。为了方便用户访问各个子module，nn.Module实现了很多方法，如函数children可以查看直接子module，函数modules可以查看所有的子module（包括当前module）。与之相对应的还有函数named_children和named_modules，其能够在返回module列表的同时返回它们的名字。 123input = V(t.arange(0, 12).view(3, 4).float())model = nn.Dropout()model(input) tensor([[ 0., 0., 0., 0.], [ 0., 0., 12., 0.], [ 0., 18., 0., 0.]]) 12model.training = Falsemodel(input) tensor([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.]]) 对batchnorm、dropout、instancenorm等在训练和测试阶段行为差距较大的层，如果在测试时不将其training值设为False，则可能会有很大影响，这在实际使用中千万注意。虽然可通过直接设置training属性将子module设为train和eval模式，但是这种方式比较繁琐。推荐的做法是调用model.train()函数，它会将当前module及其子module中的所有training属性都设为True。model.eval()函数会把training属性都设为False。 123print(net.training, net.submodel1.training)net.eval()net.training, net.submodel1.training True True (False, False) 1list(net.named_modules()) [(&#39;&#39;, Net( (submodel1): Linear(in_features=3, out_features=4, bias=True) )), (&#39;submodel1&#39;, Linear(in_features=3, out_features=4, bias=True))] register_forward_hook和register_backward_hook函数的功能类似于variable的register_hook，可在module前向传播或反向传播时注册钩子。每次前向传播执行结束后会执行钩子函数（hook）。前向传播的钩子函数具有如下形式：hook(module, input, output) -&gt; None，而反向传播则具有如下形式：hook(module, grad_input, grad_ouput) -&gt; Tensor or None。钩子函数不应修改输入和输出，并且在使用后应及时删除，以避免每次都运行钩子增加运行负载。钩子函数主要用在获取某些中间结果的情景，如中间某一层的输出或某一层的梯度。这些结果本应写在forward函数中，但如果在forward函数中加上这些处理，可能会使处理逻辑比较复杂，这时使用钩子技术就更合适。下面考虑一种场景：有一个预训练的模型，需要提取模型的某一层（不是最后一层）的输出作为特征进行分类，希望不修改其原有的模型定义文件，这时就可以利用钩子函数。 123456789model = VGG()features = t.Tensor()def hook(module, input, output): features.copy_(output.data) handle = model.layer8.register_forward_hook(hook)_ = model(input)# 用完hook后删除handle.remove() nn.Module对象在构造函数中的行为看起来有些诡异，想要理解就需要看两个魔法方法__getattr__和__setattr__。在python中有两个常用的builtin方法：getattr和setattr。getattr(obj, ‘attr1’)等价于obj.attr，setattr(obj, ‘name’, value)等价于obj.name = value。 result = obj.name会调用builtin函数getattr(obj, ‘name’)，如果该属性找不到，会调用obj.__getattr__(‘name’) obj.name = value会调用builtin函数setattr(obj, ‘name’, value)，如果obj对象实现了__setattr__方法，setattr会直接调用obj.__setattr__(‘name’, value)。 1234567891011class person(): dict = &#123;'name':'xxx','sex':'boy','age':18&#125; def __getattr__(self,name): return self.dict[name] def __setattr__(self, name , value): self.dict[name] = value return value 12one = person()one.name, one.sex, one.age (&#39;xxx&#39;, &#39;boy&#39;, 18) 1one.name = '吴彦祖';one.name &#39;吴彦祖&#39; nn.Module实现了自定义的__setattr__函数，当执行module.name=value时，会在__setattr__中判断value是否为Parameter或nn.Module对象，如果是则将这些对象加到_parameters和_modules两个字典中；如果是其他类型的对象，如Variable、list、dict等，则调用默认的操作，将这个值保存在__dict__中。 123module = nn.Module()module.param = nn.Parameter(t.ones(2, 2))module._parameters,module.param (OrderedDict([(&#39;param&#39;, Parameter containing: tensor([[1., 1.], [1., 1.]], requires_grad=True))]), Parameter containing: tensor([[1., 1.], [1., 1.]], requires_grad=True)) 1234567submodule1 = nn.Linear(2, 2)submodule2 = nn.Linear(2, 2)module_list = [submodule1, submodule2]# 对于list对象，调用builtin函数，保存在__dict__中module.submodules = module_listprint('_modules:',module._modules)print("__dict__['submodules']:",module.__dict__.get('submodules')) _modules: OrderedDict() __dict__[&#39;submodules&#39;]: [Linear(in_features=2, out_features=2, bias=True), Linear(in_features=2, out_features=2, bias=True)] 123module.submodules = nn.ModuleList(module_list)print('_modules:',module._modules)print("__dict__['submodules']:",module.__dict__.get('submodules')) _modules: OrderedDict([(&#39;submodules&#39;, ModuleList( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ))]) __dict__[&#39;submodules&#39;]: None 因_modules和_parameters中的item未保存在__dict__中，所以默认的getattr方法无法获取它，因而nn.Module实现了自定义的__getattr__方法。如果默认的getattr无法处理，就调用自定义的__getattr__方法，尝试从_modules、_parameters和_buffers三个字典中获取。 123getattr(module, 'training') # 等价于module.training#error#module.__getattr__('training') True 1234module.attr1 = 2getattr(module, 'attr1')# 报错# module.__getattr__('attr1') 2 1getattr(module, 'param') Parameter containing: tensor([[1., 1.], [1., 1.]], requires_grad=True) 在pytorch中保存模型十分简单，所有的module对象都具有state_dict()函数，返回当前Module所有的状态数据。将这些状态数据保存后，下次使用模型时即可利用model.load_state_dict()函数将状态加载进来。优化器（optimizer）也有类似机制，不过一般并不需要保存优化器的运行状态。 123456# 保存模型t.save(net.state_dict(), 'net.pth')# 加载已保存的模型net2 = Net()net2.load_state_dict(t.load('net.pth')) &lt;All keys matched successfully&gt; 123t.save(net, 'net_all.pth')net2 = t.load('net_all.pth')net2 /usr/local/lib/python3.6/dist-packages/torch/serialization.py:256: UserWarning: Couldn&#39;t retrieve source code for container of type Net. It won&#39;t be checked for correctness upon loading. &quot;type &quot; + obj.__name__ + &quot;. It won&#39;t be checked &quot; Net( (submodel1): Linear(in_features=3, out_features=4, bias=True) ) 将Module放在GPU上运行也十分简单，只需一下两步。 model = model.cuda():将模型的所有参数转存到GPU input.cuda():将输入数据放置到GPU上。 至于如何在多个GPU上并行计算，pytorch也提供了两个函数，可实现简单高效的并行GPU计算。 nn.parallel.data_parallel(module, inputs, device_ids = None, output_device = None, dim = 0, module_kwargs = None) class torch.nn.DataParallel(module, device_ids = None, output_device = None, dim = 0) 可见二者的参数十分相似，通过device_ids参数可以指定在哪些GPU上进行优化，output_device指定输出到哪个GPU上。唯一的不同在于前者直接利用多GPU并行计算得出结果，后者则返回一个新的module，能够自动在多GPU上进行并行加速。 123456# method1new_net = nn.DataParallel(net, device_ids = [0, 1])output = new_net(input)# method 2output = nn.parallel.data_parallel(net, input, device_ids = [0, 1]) DataParallel并行的方式，是将输入一个batch的数据均分成多份，分别送到对应的GPU进行计算，然后将各个GPU得到的梯度相加。与Module相关的所有数据也会以浅复制的方式复制多份。 nn和aautograd的关系nn.Module利用的是autograd技术，其主要工作是实现前向传播。在forward函数中，nn.Module对输入的Variable进行的各种操作，本质上都用了autograd技术。这里需要对比autograd.Function和nn.Module之间的区别。 autograd.Function利用Tensor对autograd技术的扩展，为autograd实现了新的运算op，不仅要实现前向传播还要手动实现反向传播。 nn.Module利用了autograd技术，对nn的功能进行扩展，实现了深度学习中更多的层。只需实现前向传播功能，autograd即会自动实现反向传播。 nn.functional是一些autograd操作的集合，是经过封装的函数。 作为两种扩充pytorch接口的方法，我们在实际作用中应该如何选择？如果某一个操作在autograd中尚未支持，那么需要利用Function手动实现对应的前后传播和反向传播。如果某些时候利用autograd接口比较复杂，则可以利用Function将多个操作聚合，实现优化，比直接利用autograd低级别的操作要快。如果只是想在深度学习中怎加某一层，使用nn.Module进行封装则更简单高效。]]></content>
      <categories>
        <category>笔记</category>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[dlrm]]></title>
    <url>%2F2019%2F08%2F21%2Fdlrm%2F</url>
    <content type="text"><![CDATA[一个人在不接触对方的情况下，通过一种特殊的方式，和对方进行一系列的问答。如果在想当长时间内，他无法根据这些问题判断对方是人还是计算机，那么就可以认为这个计算机是智能的。 绪论首先深度学习是机器学习的一个分支。 首先，深度学习问题是一个机器学习问题，指从有限样例中通过算法总结出一般性规律，并可以运用到新的未知数据上。 其次，深度学习采用的模型一般比较复杂，模型常常由许多个线性或者非线性的组件。信息从输入到从模型中输出数据会流过各个组件。因为每个组件都加工过信息，并进而影响后续的组件。当我们得到输出时，我们并不清楚每个组件的贡献是多少。这个问题就叫做贡献度分配问题（Credit Assignment Proble，CAP）。在深度学习中，贡献分配问题是个很关键的问题。 目前可以比较好解决贡献度分配问题的模型是人工神经网络。 深度学习和神经网络并不等价，深度学习还可以采用神经网络以外的模型（比如深度信念网络是一种概率图模型）。 人工智能人工智能就是要让机器的行为看起来就像是人所表现的智能行为一样。 目前，人工智能的主要领域大体上可以分为以下几个方面： 感知：即模拟人的感知能力，对外部刺激信息（视觉和语音等）进行感知和加工。主要研究领域包括语音信息处理和计算机视觉等。 学习：即模拟人的学习能力，主要研究如何从样例或与环境交互中进行学习。主要研究领域包括语音信息处理和计算机视觉等。 认知：即模拟人的认知能力，主要研究领域包括知识表示、自然语言理解、推理、规划、决策等。 人工智能的发展历史发展历程大体上可以分为“推理期”，“知识期”和“学习期”。 推理期1956年达特茅斯会议之后，研究者对人工智能的热情高涨，之后的十几年是人工智能的黄金时期。 早期通过人类的经验，基于逻辑或者事实归纳出来一些规则，然后通过编写程序来让计算机完成一个任务。 但是人们过于乐观，低估了实现人工智能的难度。 之后人工智能的研究陷入帝国，很多人工智能项目的经费也被消减。 人工智能低谷，也叫人工智能冬天（AI Winter），指人工智能史上资金及学术界研究兴趣都大幅减少的时期。人工智能领域经历过好几个技术成熟度曲线，紧接着是失望及批评，以及研究资金断绝，紧接着在几十年后重燃的研究兴趣。1974-80奶奶级1987-93年是两个主要的低谷时期，其他还要几个较小的低谷。 知识期到了20世纪70年代，研究者意识到知识对于人工智能系统的重要性。 出现了各种各样的专家系统，并在特定的专业领域取得了很多成果。专家系统可以简单理解为“知识库+推理机”，是一类具有专门知识和经验的计算机智能系统。 专家系统一般采用知识表示和知识推理等技术来完成通常由领域专家才能解决的复杂问题，因此专家系统也被成为基于知识的系统。一个专家系统具备三要素：领域专家级知识；模拟专家思维；达到专家级的水平。 学习期对于人类的很多智能行为（比如语言理解、图片理解等），我们很难知道其中的原理，也无法描述出这些智能行为背后的“知识”。因此我们很难通过知识推理的方式来实现这些行为的智能系统。 为了解决问题，研究者开始讲研究重点转向让计算机从数据中自己学习。 机器学习的主要目的是设计和分析一些学习算法，让计算机可以从数据中自动分析获得规律，并利用学习到的规律对未知数据进行预测，从而帮助人们完成特定任务。 人工智能发展史]]></content>
      <categories>
        <category>笔记</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【notes】pytorch学习笔记2-autograd部分]]></title>
    <url>%2F2019%2F08%2F20%2Fautograd%2F</url>
    <content type="text"><![CDATA[autogradtorch.autograd是为方便用户使用，专门开发的一套自动求导引擎，它能够根据输入和前向传播过程自动构建计算图，并执行反向传播。 计算图是现代深度学习框架的核心，它为自动求导算法——反向传播提供了理论支持。 Variablepytorch在autograd模块中实现计算图相关功能，autograd中的核心数据结构是Variable。Variable封装了tensor，并记录对tensor的操作记录用来构建计算图。Variale的数据结构如图： graph LR; A[autograd.Variable] --> B(data); A[autograd.Variable] --> C(grad); A[autograd.Variable] --> D(grad_fn); Variable的构造函数需要传入tensor，同时有两个可选参数。 require_grad(bool)：是否需要对该variable进行求导。 volatile(bool)：意为“挥发”，设置为True，构建在该variable之上的图都不会求导，转为推理阶段设计。 Variable支持大部分tensor支持的函数，但其不支持部分inplace函数，因为这些函数会修改tensor自身，而在反向传播中，variable需要缓存原来的tensor来计算梯度。如果想要计算各个Variable的梯度，只需调用根节点variable的backward方法，autograd会自动沿着计算图反向传播，计算每一个叶子节点的梯度。 grad_variables：形状与variable一致，对于y.backward()，grad_variables相当于链式法则$\frac{\partial{z}}{\partial{x}}=\frac{\partial{z}}{\partial{y}}\times\frac{\partial{y}}{\partial{x}}$中的$\frac{\partial{z}}{\partial{y}}$。grad_variables也可以是tensor或序列。 retain_graph：反向传播需要缓存一些中间结果，反向传播之后，这些缓存就被清空，可通过指定这个参数不清空缓存，用来多次反向传播。 create_graph：对反向传播过程再次构建计算图，可通过backward of backward实现求高阶导数。 12from torch.autograd import Variable as Vimport torch as t 123# 从tensor中创建variable，指定需要求导a = V(t.ones(3, 4), requires_grad = True);a tensor([[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]], requires_grad=True) 1b = V(t.zeros(3, 4));b tensor([[0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]]) 1234# 函数的使用和tensor一致# 也可写成c = a + bc = a.add(b)c tensor([[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]], grad_fn=&lt;AddBackward0&gt;) 12d = c.sum()d.backward() # 反向传播 1234# 注意二者的区别# 前者在取data后变为tensor，从tensor计算sum得到float# 后者计算sum后仍然是Variablec.data.sum(), c.sum() (tensor(12.), tensor(12., grad_fn=&lt;SumBackward0&gt;)) 1a.grad tensor([[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]]) 123# 此处虽然没有指定c需要求导，但c依赖于a，而a需要求导# 因此c的requires_grad属性会自动设为Truea.requires_grad, b.requires_grad, c.requires_grad (True, False, True) 123# c.grad是None，c不是叶子节点，他的梯度是用来计算a的梯度# 虽然c.requires_grad = True，但其梯度计算完之后即被释放c.grad is None True 接下来看看autograd计算导数和我们手动推导的导数的区别。 y=x^2e^x它的导函数是： \frac{\partial{y}}{\partial{x}}=2xe^x+x^2e^x123456def f(x): y = x**2 * t.exp(x) return ydef gradf(x): dx = 2*x*t.exp(x) + x**2*t.exp(x) return dx 123x = V(t.randn(3, 4), requires_grad = True)y = f(x)y tensor([[1.3949e-01, 2.7201e-01, 4.9848e-01, 2.2968e+00], [3.2033e-01, 3.3618e-01, 2.3554e-02, 1.0507e+01], [3.9416e+01, 3.5322e+00, 9.6847e-02, 1.2743e+01]], grad_fn=&lt;MulBackward0&gt;) 12y.backward(t.ones(y.size())) # grad_variables形状与y一致x.grad tensor([[ 1.0154, -0.4398, -0.1755, 7.1583], [-0.4095, 1.7961, 0.3532, 24.3531], [76.1412, 10.0143, -0.4190, 28.6505]]) 12# autograd的计算结果与利用公式手动计算的结果一致gradf(x) tensor([[ 1.0154, -0.4398, -0.1755, 7.1583], [-0.4095, 1.7961, 0.3532, 24.3531], [76.1412, 10.0143, -0.4190, 28.6505]], grad_fn=&lt;AddBackward0&gt;) 计算图pytorch中autograd的底层采用了计算图，计算图是一种特殊的有向无环图（DAG），用于记录算子与变量之间的关系。一般用矩形表示算子，椭圆形表示变量。如表达式$\boldsymbol{z}=\boldsymbol{wx}+\boldsymbol{b}$可分解为$\boldsymbol{y}=\boldsymbol{wx}$和$\boldsymbol{z}=\boldsymbol{y}+\boldsymbol{b}$，其计算图如图所示，图中的MUL和ADD都是算子，$\boldsymbol{w}$、$\boldsymbol{x}$、$\boldsymbol{b}$为变量。 graph LR; A((W)) --> C[MUL]; B((X)) --> C[MUL]; C[MUL] --> D((y)); E((b)) --> F[Add]; D((y)) --> F[Add]; F[Add] --> G((z)); 如上有向无环图，$\boldsymbol{X}$和$\boldsymbol{b}$是叶子节点，这些节点通常有用户自己创建，不依赖于其他变量。$\boldsymbol{z}$称为根节点，是计算图的最终目标。利用链式法则很容易求各个叶子节点的梯度。 \frac{\partial{z}}{\partial{b}}=1,\frac{\partial{z}}{\partial{y}}=1\\\frac{\partial{y}}{\partial{w}}=x,\frac{\partial{y}}{\partial{x}}=w\\\frac{\partial{z}}{\partial{x}}=\frac{\partial{z}}{\partial{y}}\frac{\partial{y}}{\partial{x}}=1*w\\\frac{\partial{z}}{\partial{w}}=\frac{\partial{z}}{\partial{y}}\frac{\partial{y}}{\partial{w}}=1*x有了计算图上述链式求导可自动利用计算图的反向传播自动完成，其过程如图所示： graph LR; A((dz)) --> B[addBackward]; B[addBackward] --> C((dy)); B[addBackward] --> D((db)); C((dy)) --> E[mulBackward]; E[mulBackward] --> F((dX)); E[mulBackward] --> G((dW)); 图中记录了操作function，每个变量在图中的位置可通过其grad_fn属性在图中的位置推测得到。在反向传播的过程中，autograd沿着这个图从当前变量（根节点z）溯源，可以利用链式求导法则计算所有叶子节点的梯度。 每一个前向传播操作的函数都有与之对应的反向传播函数用来计算输入的各个variable的梯度，这些函数的函数名通常以Backward结尾。 12345x = V(t.ones(1))b = V(t.rand(1), requires_grad = True)w = V(t.rand(1), requires_grad = True)y = w * x # 等价于y = w.mul(x)z = y + b # 等价于z = y.add(b) 1x.requires_grad, b.requires_grad, w.requires_grad (False, True, True) 123# 虽然未指定y.requires_grad为True，但由于y依赖于需要求导的w# 故而y.requires_grad为Truey.requires_grad True 1x.is_leaf, w.is_leaf, b.is_leaf (True, True, True) 1y.is_leaf, z.is_leaf (False, False) 123# grad_fn可以查看这个variable的反向传播函数# z是add函数的输出，所以它的反向传播函数是AddBackwardz.grad_fn &lt;AddBackward0 at 0x7f073e390e80&gt; 1234#next_function保存grad_fn的输入，grad_fn的输入是一个tuple# 第一个是y，它是乘法（mul）的输出，所以对应的反向传播函数y.grad_fn是MulBackward# 第二个是b，它是叶子节点，由用户创建，grad_fn为None，但是有z.grad_fn.next_functions ((&lt;MulBackward0 at 0x7f073e390828&gt;, 0), (&lt;AccumulateGrad at 0x7f073e390f28&gt;, 0)) 12# variable的grad_fn对应着图中的functionz.grad_fn.next_functions[0][0] == y.grad_fn True 123# 第一个是w，叶子节点，需要求导，梯度是累加的# 第二个是x，叶子节点，不需要求导，所以为Noney.grad_fn.next_functions ((&lt;AccumulateGrad at 0x7f073e390470&gt;, 0), (None, 0)) 12# 叶子节点的grad_fn是Nonew.grad_fn, x.grad_fn (None, None) 计算$\boldsymbol{w}$的梯度时需要用到$\boldsymbol{x}$的数值（$\frac{\partial{y}}{\partial{w}}=x$），这些数值在前向过程中会保存成buffer，在计算完梯度之后会自动清空。为了能够多次反向传播需要指定retain_graph来保留这些buffer。 1y.grad_fn.saved_variables --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) &lt;ipython-input-30-284a59926bb7&gt; in &lt;module&gt; ----&gt; 1 y.grad_fn.saved_variables AttributeError: &#39;MulBackward0&#39; object has no attribute &#39;saved_variables&#39; 原因确实是版本问题，PyTorch0.3 中把许多python的操作转移到了C++中，saved_variables 现在是一个c++的对象，无法通过python访问。https://github.com/chenyuntc/pytorch-book/issues/7 可以查看这里进行学习https://github.com/chenyuntc/pytorch-book/blob/0.3/chapter3-Tensor和autograd/Autograd.ipynb,省掉上面的操作 123# 使用retain_graph保存bufferz.backward(retain_graph = True)w.grad tensor([1.]) 123# 多次反向传播，梯度累加，这也就是w中AccumulateGrad标识的含义z.backward()w.grad tensor([2.]) pytorch使用的是动态图，它的计算图在每次前向传播时都是从头开始构建的，所以它能够使用python的控制语句（如for、if等），根据需求创建计算图。这一点在自然语言处理领域中很有用，它意为你不需要事先构建所有可能用到的图的路径，图在运行时才构建。 1234567def abs(x): if x.data[0] &gt; 0: return x else: return -xx = V(t.ones(1), requires_grad = True)y = abs(x)y.backward()x.grad tensor([1.]) 1234x = V(-1 * t.ones(1), requires_grad = True)y = abs(x)y.backward()print(x.grad) tensor([-1.]) 123456789def f(x): result = 1 for ii in x: if ii.data &gt; 0: result = ii * result return resultx = V(t.arange(-2, 4).float(), requires_grad = True)y = f(x) # y = x[3] * x[4] * x[5]y.backward()x.grad tensor([0., 0., 0., 6., 3., 2.]) 变量的requires_grad属性默认为False，如果某一个节点requires_grad被设置为True，那么所有依赖它的节点requires_grad都是True。 with torch.no_grad()内的variable均为不会求导，其优先级高于requires_grad。函数可以用装饰器@torch.no_grad()。可实现一定程度的速度提升，并节省约一半显存，因为其不需要分配空间保存梯度。 详细内容可见：https://pytorch.org/docs/master/autograd.html#locally-disable-grad 1234x = t.tensor([1.], requires_grad = True)with t.no_grad(): y = x * 2y.requires_grad False 12345@t.no_grad()def doubler(x): return x * 2z = doubler(x)z.requires_grad False 在反向传播过程中非叶子节点的导数计算完之后即被清空。若想查看这些变量的梯度，有以下两种方法： 使用autograd.grad函数 使用hook 推荐使用hook方法，但在实际使用中应尽量避免修改grad的值。 123456x = V(t.ones(3), requires_grad = True)w = V(t.rand(3), requires_grad = True)y = x * w# y依赖于w，而w.requires_grad = Truez = y.sum()x.requires_grad, w.requires_grad, y.requires_grad (True, True, True) 123# 非叶子节点grad计算完之后自动清空，y.grad是Nonez.backward()x.grad, w.grad, y.grad (tensor([0.1283, 0.8326, 0.6539]), tensor([1., 1., 1.]), None) 12345678# 第一种方法：使用grad获取中间变量的梯度x = V(t.ones(3), requires_grad = True)w = V(t.rand(3), requires_grad = True)y = x * w# y依赖于w，而w.requires_grad = Truez = y.sum()# z对y的梯度，隐式调用backward()t.autograd.grad(z, y) (tensor([1., 1., 1.]),) 12345678910111213141516# 第二种方法：使用hook# hook是一个函数，输入是梯度，不应该有返回值def variable_hook(grad): print('y的梯度：\r\n',grad) x = V(t.ones(3), requires_grad = True)w = V(t.rand(3), requires_grad = True)y = x * w# 注册hookhook_handle = y.register_hook(variable_hook)z = y.sum()z.backward()# 除非你每次都要用hook，否则用完之后记得移除hookhook_handle.remove() y的梯度： tensor([1., 1., 1.]) 最后再来看看variable中grad属性和backward函数grad_variables参数的含义。 variables $\boldsymbol{x}$ 的梯度是目标函数$f(x)$对$\boldsymbol{x}$的梯度，形状与$\boldsymbol{x}$一致。 y.backward(grad_variables)中grad_variables相当于链式法则中的$\frac{\partial{z}}{\partial{x}}=\frac{\partial{z}}{\partial{y}}\frac{\partial{y}}{\partial{x}}$中的$\frac{\partial{z}}{\partial{y}}$。z是目标函数，一般是个标量，故而$\frac{\partial{z}}{\partial{y}}$的形状与$\boldsymbol{y}$的形状一致。z.backward()等价于y.backward(grad_y)。而z.backward()省略了grad_variables参数，是因为z是个标量，而$\frac{\partial{z}}{\partial{z}}=1$ 12345x = V(t.arange(0, 3).float(), requires_grad = True)y = x**2 + x*2z = y.sum()z.backward()x.grad tensor([2., 4., 6.]) 123456x = V(t.arange(0, 3).float(), requires_grad = True)y = x**2 + x*2z = y.sum()y_grad_variables = V(t.Tensor([1, 1, 1])) y.backward(y_grad_variables)x.grad tensor([2., 4., 6.]) 值得注意的是，只有对variable的操作才能使用autograd，如果variable的data直接进行操作，将无法使用反向传播。除了参数初始化，一般我们不会直接修改variable.data的值。 在pytorch中计算图的特点总结如下： autograd根据用户对variable的操作构建计算图。对variable的操作抽象为Function。 由用户创建的节点称为叶子节点，叶子节点的grad_fn为None。叶子节点中需要求导的variable，具有AccumulateGrad标识，因其梯度是累加的。 variable默认是不需要求导的，即requires_grad属性默认为False。如果某一个节点requires_grad被设置为True，那么所有依赖它的节点requires_grad都为True。 with torch.no_grad和@torch.no_grad()的作用下的节点都不会求导，优先级比requires_grad高。 多次反向传播时，梯度是累加的。反向传播的中间缓存会被清空，为进行多次反向传播需指定retain_graph=True来保存这些缓存。 非叶子节点的梯度计算完后即被清空，可以使用autograd.grad或hook技术获取非叶子节点梯度的值。 variable的grad与data形状一致，应避免直接修改variable.data，因为对data的直接操作无法利用autograd进行反向传播。 反向传播函数backward的参数grad_variables可以看成链式求导的中间结果，如果是标量，可以省略，默认为1。 pytorch采用动态图设计，可以很方便地查看中间层的输出，动态地设计计算图结构。 扩展autograd目前绝大多数函数都可以使用autograd实现反向求导，但如果需要自己写一个复杂的函数，不支持自动反向求导怎么办？答案是写一个Function，实现它的前向传播和反向传播代码，Function对应于计算图中的矩形，它接受参数，计算并返回结果。下面给出了一个例子。 123456789101112131415161718from torch.autograd import Functionclass MultiplyAdd(Function): @staticmethod def forward(ctx, w, x, b): print('type in forward', type(x)) ctx.save_for_backward(w, x)#存储用来反向传播的参数 output = w*x +b return output @staticmethod def backward(ctx, grad_output): w, x = ctx.saved_tensors # 老版本是saved_variables print('type in backward',type(x)) grad_w = grad_output * x grad_x = grad_output * w grad_b = grad_output * 1 return grad_w, grad_x, grad_b 分析如下： 自定义的Function需要继承autograd.Function，没有构造函数init，forward和backward函数都是静态方法 forward函数的输入和输出都是Tensor，backward函数的输入和输出都是Variable backward函数的输出和forward函数的输入一一对应，backward函数的输入和forward函数的输出一一对应 backward函数的grad_output参数即t.autograd.backward中的grad_variables 如果某一个输入不需要求导，直接返回None，如forward中的输入参数x_requires_grad显然无法对它求导，直接返回None即可 反向传播可能需要利用前向传播的某些中间结果，需要进行保存，否则前向传播结束后这些对象即被释放 12345678910x = V(t.ones(1))w = V(t.rand(1),requires_grad=True)b = V(t.rand(1),requires_grad=True)print('开始前向传播')z = MultiplyAdd.apply(w, x, b)print('开始反向传播')z.backward()# x不需要求导，中间过程还是会计算它的导数，但随后被清空x.grad, w.grad, b.grad 开始前向传播 type in forward &lt;class &#39;torch.Tensor&#39;&gt; 开始反向传播 type in backward &lt;class &#39;torch.Tensor&#39;&gt; (None, tensor([1.]), tensor([1.])) 12345678910x = V(t.ones(1))w = V(t.rand(1),requires_grad=True)b = V(t.rand(1),requires_grad=True)print('开始前向传播')z = MultiplyAdd.apply(w, x, b)print('开始反向传播')# 调用MultiplyAdd.backward# 会自动输出grad_w, grad_x, grad_bz.grad_fn.apply(V(t.ones(1))) 开始前向传播 type in forward &lt;class &#39;torch.Tensor&#39;&gt; 开始反向传播 type in backward &lt;class &#39;torch.Tensor&#39;&gt; (tensor([1.]), tensor([0.5986], grad_fn=&lt;MulBackward0&gt;), tensor([1.])) 在backward函数里之所以也要对variable进行操作是为了能计算梯度的梯度。 1234x = V(t.Tensor([5]), requires_grad = True)y = x ** 2grad_x = t.autograd.grad(y, x, create_graph = True)grad_x (tensor([10.], grad_fn=&lt;MulBackward0&gt;),) 1grad_grad_x = t.autograd.grad(grad_x[0],x);grad_grad_x (tensor([2.]),)]]></content>
      <categories>
        <category>笔记</category>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【notes】pytorch学习笔记1-Tensor部分]]></title>
    <url>%2F2019%2F08%2F19%2Fpytorch-1%2F</url>
    <content type="text"><![CDATA[Tensor和autograd 每个深度学习框架的设计核心是张量和计算图，在pytorch里体现为张量系统（Tensor）和自动微分系统（atutograd）。 Tensor 中文译为张量，可以简单看作一个数组。 与numpy里的ndarrays类似，但tensor支持GPU加速。 基础操作接口角度： torch.function tensor.function 存储角度： 不会修改自身数据，如a.add(b),返回一个值为加法结果的新的tensor。 会修改自身数据，如a.add_(b)，加法的值储存在a中了。 创建Tensor在pytorch中常见的新建tensor的方法： 类别 特点 函数 功能 第一类：基础方法 最灵活 Tensor(*sizes) 基础构造函数 第二类：根据sizes建立 常数型 ones(*sizes) 全1Tensor 常数型 zeros(*sizes) 全0Tensor 常数型 eyes(*sizes) 对角线为1，其他为0 概率分布型 rand/randn(*sizes) 均匀/标准分布 第三类：在一定范围内建立 等差数列型 arange(s,e,step) 从s到e，步长为step 等差数列型 linspace(s,e,steps) 从s到e，均匀切分成steps份 概率分布型 normal(mean,std)/uniform(from,to) 正态分布/均匀分布 概率分布型 randperm(m) 随机分布 其中使用Tensor函数新建tensor是最复杂多变的，它既可以接受一个list，并根据list的数据新建tensor，也可根据指定的形状新建tensor，还能传入其他的tensor。 123# 引入必要的包import torch as tfrom torch.autograd import Variable as V 12# 指定tensor的形状a = t.Tensor(2, 3);a tensor([[7.2443e+22, 4.2016e+30, 9.9708e+17], [7.2296e+31, 5.6015e-02, 4.4721e+21]]) 12# 用list的数据创建tensorb = t.Tensor([[1,2,3],[4,5,6]]);b tensor([[1., 2., 3.], [4., 5., 6.]]) 1b.tolist(),type(b.tolist()) # 把tensor转为list ([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], list) tensor.size()返回torch.Size()对象，它是tuple的子类，但其使用方式与tuple略有不同。 1b_size = b.size();b_size torch.Size([2, 3]) 1b.numel() # numelements前五个字母，b中元素总个数，等价于b.nelement() 6 12345# 创建一个和b形状一样的tensorc = t.Tensor(b_size)# 创建一个元素为2和3的tensord = t.Tensor((2, 3))c, d # 输出结果不同，明显看出torch.Size()对象和tuple的不同 (tensor([[5.8959e-35, 4.5636e-41, 1.0257e-36], [0.0000e+00, 5.0000e+00, 6.0000e+00]]), tensor([2., 3.])) tensor.shape等价于tensor.size() 1c.shape torch.Size([2, 3]) 需要注意：t.Tensor(\sizes)创建tensor时，系统不会马上分配空间，只会计算内存是否够用，使用到tensor时才会分配，而其他方法是创建后会立马分配空间。 * 1t.ones(2, 3) tensor([[1., 1., 1.], [1., 1., 1.]]) 1t.zeros(2, 3) tensor([[0., 0., 0.], [0., 0., 0.]]) 1t.linspace(1, 10 ,3) tensor([ 1.0000, 5.5000, 10.0000]) 1t.randn(2, 3) tensor([[-0.4864, 0.5022, -0.4059], [ 0.4138, 1.1588, -1.1650]]) 1t.randperm &lt;function _VariableFunctions.randperm&gt; 123# 0到n-1随机排列后的数列n = 10t.randperm(n) tensor([2, 5, 8, 3, 4, 1, 0, 7, 9, 6]) 1t.eye(2, 3) # 不要求行列数一致 tensor([[1., 0., 0.], [0., 1., 0.]]) 1t.normal(t.Tensor([0]),t.Tensor([1])) tensor([-0.5517]) 常用Tensor操作 tensor.view方法可以改变tensor的形状，但要保证前后元素总数一致。前后保持数据一致，返回的新tensor与源tensor共享内存。 在实际应用中可能经常需要增加或减少某个维度，这是squeeze和unsqueeze两个函数排上用场。 12a = t.arange(0, 6)a.view(2, 3) tensor([[0, 1, 2], [3, 4, 5]]) 12b = a.view(-1, 3) # 当某一维为-1时，会自动计算它的大小b tensor([[0, 1, 2], [3, 4, 5]]) 1b.shape, b.unsqueeze(1).shape # 注意形状，在第1维上增加“1” (torch.Size([2, 3]), torch.Size([2, 1, 3])) 1b.unsqueeze(-2) # -2表示倒数第二个维度 tensor([[[0, 1, 2]], [[3, 4, 5]]]) 12c = b.view(1, 1, 1, 2, 3)c, c.squeeze(0) # 压缩第0维的1 (tensor([[[[[0, 1, 2], [3, 4, 5]]]]]), tensor([[[[0, 1, 2], [3, 4, 5]]]])) 1c.squeeze() # 压缩所有的“1”的维度 tensor([[0, 1, 2], [3, 4, 5]]) 12a[1] = 100b # a和b共享内存，修改了a，b也变了 tensor([[ 0, 100, 2], [ 3, 4, 5]]) resize是另一种改变size的方法，和view不同的地方是resize可以改变尺寸，可以有不同数量的元素。如果新尺寸超过了旧尺寸，会自动分配空间，如果新尺寸小于旧尺寸，之前的数据依旧会保存。 12b.resize_(1, 3)b tensor([[ 0, 100, 2]]) 12b.resize_(3, 3) # 旧的数据依旧被保存，多出的数据会分配新空间。b tensor([[ 0, 100, 2], [ 3, 4, 5], [7881702260482471202, 8319104481852400229, 7075192647680159593]]) 索引操作Tensor支持和numpy.ndarray类似的索引操作，语法上也类似。 如无特殊说明，索引出来的结果与原tensor共享内存 1a = t.randn(3,4);a tensor([[ 0.8865, -0.8832, -1.0883, -0.2804], [-0.9056, 0.0635, 0.5528, -0.0222], [ 1.4919, -1.0480, -1.7623, 0.8558]]) 1a[0] # 第0行 tensor([ 0.8865, -0.8832, -1.0883, -0.2804]) 1a[:, 0] # 第0列 tensor([ 0.8865, -0.9056, 1.4919]) 1a[0][2] # 第0行第2个元素，等价于a[0,2] tensor(-1.0883) 1a[0, -1] # 第0行最后一个元素 tensor(-0.2804) 1a[:2] # 前两行 tensor([[ 0.8865, -0.8832, -1.0883, -0.2804], [-0.9056, 0.0635, 0.5528, -0.0222]]) 1a[:2, 0:2] # 前两行，第0,1列 tensor([[ 0.8865, -0.8832], [-0.9056, 0.0635]]) 1a[0:1, :2].shape, a[0, :2].shape # 注意两者的区别是形状不同，但是值是一样的 (torch.Size([1, 2]), torch.Size([2])) 123a[a &gt; 1] # 等价于a.masked_select(a&gt;1)# 选择结果与原tensor不共享内存空间 tensor([1.4919]) 1a[t.LongTensor([0,1])] # 第0行和第1行 tensor([[ 0.8865, -0.8832, -1.0883, -0.2804], [-0.9056, 0.0635, 0.5528, -0.0222]]) 常用的选择函数： 函数 功能 index_select(input, dim, index) 在指定维度dim上选取，例如选取某行某列 masked_select(input, mask) 例子如上，a[a &gt; 0],使用ByteTensor进行选取 non_zero(input) 非0元素的下标 gather(input, dim, index) 根据index，在dim维度上选取数据，输出的size与index一样 gather是一个比较复杂的操作，对于一个二维的tensor，输出的每个元素如下：12out[i][j] = input[index[i][j]][j] # dim = 0out[i][j] = input[i][index[i][j]] # dim = 1 1a = t.arange(0, 16).view(4, 4);a tensor([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14, 15]]) 123# 选取对角线上的元素index = t.LongTensor([[0,1,2,3]])a.gather(0, index) tensor([[ 0, 5, 10, 15]]) 123# 选取反对角线上的元素index = t.LongTensor([[3, 2, 1, 0]]).t()a.gather(1,index) tensor([[ 3], [ 6], [ 9], [12]]) 123# 选取反对角线上的元素，注意与上面不同index = t.LongTensor([[3, 2, 1, 0]])a.gather(0, index) tensor([[12, 9, 6, 3]]) 123# 选取两个对角线上的元素index = t.LongTensor([[0, 1, 2, 3], [3, 2, 1, 0]]).t()b = a.gather(1, index);b tensor([[ 0, 3], [ 5, 6], [10, 9], [15, 12]]) gather的逆操作是scatter_, gather把数据从input中按index取出，而scatter_是把取出的数据再放回去。注意scatter_函数是inplace操作。 1234out = input.gather(dim, index)--&gt;近似逆操作out = Tensor()out.scatter_(dim, index) 1234# 把两个对角线元素放回到指定位置c = t.zeros(4, 4)c.scatter_(1, index, b.float()) tensor([[ 0., 0., 0., 3.], [ 0., 5., 6., 0.], [ 0., 9., 10., 0.], [12., 0., 0., 15.]]) 高级索引高级索引可以看成是普通索引的扩展，但是高级索引操作的结果一般不和原Tensor共享内存。 1x = t.arange(0, 27).view(3, 3, 3);x tensor([[[ 0, 1, 2], [ 3, 4, 5], [ 6, 7, 8]], [[ 9, 10, 11], [12, 13, 14], [15, 16, 17]], [[18, 19, 20], [21, 22, 23], [24, 25, 26]]]) 1x[[1, 2], [1, 2], [2, 0]] # 元素的个数是列表的长度 元素为x[1,1,2]和x[2,2,0] tensor([14, 24]) 1x[[2,1,0],[0],[1]] # 元素为最长列表的长度 x[2,0,1] x[1,0,1] x[0,0,1] tensor([19, 10, 1]) 1x[[0,2],...] # x[0] x[2] tensor([[[ 0, 1, 2], [ 3, 4, 5], [ 6, 7, 8]], [[18, 19, 20], [21, 22, 23], [24, 25, 26]]]) Tensor类型默认的Tensor类型为FloatTensor，可通过t.get_default_tensor_type修改默认类型（如果默认类型为GPU tensor，在所有操作都在GPU上进行）。 HalfTensor是专门为GPU版本设计的，同样的元素个数，显存占用只有FloatTensor的一半，所以可以极大地缓解GPU显存不足的问题，但是由于HalfTensor所能表示的数值大小和精度有限，所以可能出现溢出等问题。 数据类型 CPU tensor GPU tensor 32bit浮点 torch.FloatTensor torch.cuda.FloatTensor 64bit浮点 torch.DoubleTensor torch.cuda.DoubleTensor 16半精度浮点 N/A torch.cuda.HalfTensor 8bit无符号整型（0~255） torch.ByteTensor torch.cuda.ByteTensor 8bit有符号整型（-128~127） torch.CharTensor torch.cuda.CharTensor 16bit有符号整型 torch.ShortTensor torch.cuda.ShortTensor 32bit有符号整型 torch.IntTensor torch.cuda.IntTensor 64bit有符号整型 torch.LongTensor torch.cuda.LongTensor 各数据类型之间可以互相转换，type(new_type)是通用的做法，同时还有float、long、half等快捷方法。CPU tensor与GPUtensor之间的互相装换通过tensor.cuda和tensor.cpu的方法实现。Tensor还有一个new方法，用法与t.Tensor一样，会调用该tensor对应类型的构造函数，生成与当前tensor类型一致的tensor。 123456# 设置默认tensor类型, 注意参数是字符串# t.set_default_tensor_type('torch.IntTensor') 会报错# TypeError: only floating-point types are supported as the default type# t.get_default_dtype() 返回 torch.float32# t.set_default_dtype(t.int) 报错 TypeError: only floating-point types are supported as the default type 1a = t.Tensor(2, 3);a tensor([[1.8609e+34, 1.8179e+31, 1.8524e+28], [9.6647e+35, 2.0076e+29, 7.3185e+28]]) 1b = a.int();b tensor([[-2147483648, -2147483648, -2147483648], [-2147483648, -2147483648, -2147483648]], dtype=torch.int32) 1c = a.type_as(b);c tensor([[-2147483648, -2147483648, -2147483648], [-2147483648, -2147483648, -2147483648]], dtype=torch.int32) 1d = b.new(2, 3);d tensor([[ 0, 775041082, 960062260], [1697986359, 926101553, 895706424]], dtype=torch.int32) 123# 查看函数new的源码a.new?? 逐元素操作这部分操作会对tensor的每个元素进行操作，输入和输出的形状相同。 函数 功能 abs/sqrt/div/exp/fmod/log/pow.. 绝对值/平方根/除法/指数/求余/对数/求幂 cos/sin/asin/atan2/cosh 三角函数 ceil/round/floor/trunc 上取整/四舍五入/下取整/只保留整数部分 clamp(input,min,max) 超过min和max部分截断 sigmod/tanh… 激活函数 对于很多基本的运算，比如加减乘除求余等运算pytorch都实现了运算符重载，可以直接使用运算符。其中camp(x, min, max)的输出满足一个分段函数： y_i= \begin{cases} min, & {x_i < min}\\\\ x_i, & {min \leq x_i \leq max}\\\\ max, & {x_i > max} \end{cases}12a = t.arange(0, 6).view(2, 3).float() # 注意要转换一下类型，否则会报错t.cos(a) tensor([[ 1.0000, 0.5403, -0.4161], [-0.9900, -0.6536, 0.2837]]) 1a % 3 # 等价于t.fmod(a, 5) tensor([[0., 1., 2.], [0., 1., 2.]]) 1a ** 2# 等价于t.power(a, 2) tensor([[ 0., 1., 4.], [ 9., 16., 25.]]) 1234# a中每个元素与3相比取较大的那一个print(a)t.clamp(a, min = 3) tensor([[0., 1., 2.], [3., 4., 5.]]) tensor([[3., 3., 3.], [3., 4., 5.]]) 归并操作这类操作会使输入形状小于输出形状，并可以沿着某一维度进行制定操作。 函数 功能 mean/sum/median/mode 均值/和/中位数/众数 norm/dist 范数/距离 std/var 标准差/方差 cumsum/cumprod 累加/累乘 几乎每个函数都有一个dim参数，用来制定在那个维度上执行。假设输入的形状是(m, n, k): 如果指定dim = 0，输出的形状为(1, n, k)或者(n, k) 如果指定dim = 1，输出的形状为(m, 1, k)或者(m, k) 如果指定dim = 2，输出的形状为(m, n, 1)或者(m, n) 也就是dim指定哪个维度，那个维度就会变成1，size中是否有1取决于keepdim，keepdim=True会保留1，keepdim默认为False。但是并非总是这样，比如cumsum。 归并运算就是对其他维度取值相同且该维度取值不同元素进行操作。 12b = t.ones(2, 3)b.sum(dim = 0, keepdim = True) tensor([[2., 2., 2.]]) 1b.sum(dim = 0) #keepdim = False tensor([2., 2., 2.]) 1b.sum(dim = 1) tensor([3., 3.]) 123a = t.arange(0, 6).view(2, 3)print(a)a.cumsum(dim = 1) #沿着行累加 tensor([[0, 1, 2], [3, 4, 5]]) tensor([[ 0, 1, 3], [ 3, 7, 12]]) cumsum可以理解为以dim这个维度上索引取值相同的看作一个整体，比如dim=0每一行就是一个整体，cumsum运算相当于dim这个维度上取值为n的值加上取值为n-1的值（这个n-1已经进行过前面的运算，不是初始的值）。 比较比较函数有的是逐元素操作，有的是归并操作。 函数 功能 gt/lt/ge/le/eq/ne 大于/小于/大于等于/小于等于/等于/不等 topk 最大的k个数 sort 排序 max/min 比较两个tensor的最大值或最小值 表中第一行的比较操作已经重载，已经可以使用a&gt;=b, a&gt;b, a!=b和a==b，其返回结果为一个ByteTensor,可以用来选取元素(高级索引)。 max和min两个操作比较特殊，以max为例： t.max(tensor):返回tensor中最大的一个数。 t.max(tensor,dim)：指定维上最大的一个数，返回tensor和下标。 t.max(tensor1,tensor2)：比较两个tensor中较大的元素。 tensor和一个数的比较可以用clamp函数。 1a = t.linspace(0, 15, 6).view(2, 3);a tensor([[ 0., 3., 6.], [ 9., 12., 15.]]) 1b = t.linspace(15, 0, 6).view(2, 3);b tensor([[15., 12., 9.], [ 6., 3., 0.]]) 1a &gt; b tensor([[False, False, False], [ True, True, True]]) 1a[a &gt; b] tensor([ 9., 12., 15.]) 1t.max(a) tensor(15.) 1t.max(a, dim = 1) torch.return_types.max( values=tensor([ 6., 15.]), indices=tensor([2, 2])) 1t.max(a, b) tensor([[15., 12., 9.], [ 9., 12., 15.]]) 123# 比较a和10较大的元素t.clamp(a, min=10) tensor([[10., 10., 10.], [10., 12., 15.]]) 线性代数pytorch的线性函数封装了Blas和Lapack。 函数 功能 trace 对角线元素（矩阵的迹） diag 对角线元素 triu/tril 矩阵的上三角/下三角，可以指定偏移量 mm/bmm 矩阵乘法，batch的矩阵乘法 addmm/addbmm/addmv 矩阵运算 t 转置 dot/cross 内积/外积 inverse 求逆矩阵 svd 奇异值分解 需要注意矩阵装置会导致储存空间不连续，需调用它的.contiguous方法将其转为连续。 12b = a.t()b.is_contiguous(),b (False, tensor([[ 0., 9.], [ 3., 12.], [ 6., 15.]])) 1b.contiguous() tensor([[ 0., 9.], [ 3., 12.], [ 6., 15.]]) Tensor和Numpytensor和numpy数组之间具有很高的相似性，彼此之间相互操作也十分高效。需要注意，numpy和tensor共享内存。当遇到tensor不支持的操作时，可先转成Numpy数组，处理后再装回tensor，其转换开销很小。 广播法则是科学运算中经常使用的一个技巧，它在快速执行向量化的同时不会占用额外的内存、显存。Numpy的广播法则定义如下： 让所有输入数组都向shape最长的数组看齐，shape中不足的部分通过在前面加1补齐。 两个数组要么在某一个维度的长度一致，要么其中一个为1，否则不能计算。 当输入数组的某个维度的长度为1时，计算时沿着此维度复制扩充成一样的形状。 pytorch当前已经支持了自动广播法则，但建议可以手动通过函数实现广播法则，更直观不易出错。 unsqueeze或者view：为数据的某一维的形状补1，实现法则1。 expand或者expand_as，重复数组，实现法则3；该操作不会复制数组，所以不会占用额外的空间。 注意:repeat实现有expand类似，但是repeat会把相同数据复制多份，因此会占用额外空间。 12a = t.ones(3, 2)b = t.zeros(2, 3, 1) 1234567# 自动广播法则# 第一步：a是二维，b是三维，所以先在较小的a前面补1，# 即：a.unsqueeze(0), a的形状变成(1, 3, 2), b的形状是(2, 3, 1),# 第二步：a和b在第一维和第三维的形状不一样，其中一个为1# 可以利用广播法则扩展，两个形状都变成了(2, 3, 2)(a + b).shape,a + b (torch.Size([2, 3, 2]), tensor([[[1., 1.], [1., 1.], [1., 1.]], [[1., 1.], [1., 1.], [1., 1.]]])) 1a.unsqueeze(0).expand(2, 3, 2) + b.expand(2, 3, 2) tensor([[[1., 1.], [1., 1.], [1., 1.]], [[1., 1.], [1., 1.], [1., 1.]]]) 123import numpy as npa = np.ones([2, 3], dtype = np.float32)a array([[1., 1., 1.], [1., 1., 1.]], dtype=float32) 1b = t.from_numpy(a);b tensor([[1., 1., 1.], [1., 1., 1.]]) 12b = t.Tensor(a) # 也可以直接讲numpy对象传入Tensor，这种情况下若numpy类型不是Float32会新建。b tensor([[1., 1., 1.], [1., 1., 1.]]) 12c = b.numpy() # a, b, c三个对象共享内存c array([[1., 1., 1.], [1., 1., 1.]], dtype=float32) 12# expand不会占用额外空间，只会在需要时才扩充，可极大地节省内存。e = t.Tensor(a).unsqueeze(0).expand(1000000000000, 2, 3) 内部结构tensor分为头信息区（Tensor）和存储区（Storage），信息区主要保存着tensor的形状（size），步长（stride）、数据类型（type）等信息，而真正的数据则保存成连续的数组。 graph LR; A[Tensor A: *size *stride * dimention...] --> C[Storage:*data *size ...]; B[Tensor B: *size *stride * dimention....] --> C[Storage:*data *size ...]; 一般来说，一个tensor有着与之对应的storage，storage是在data之上封装的接口，便于使用。不同的tensor的头信息一般不同，但却可能使用相同的storage。下面我们来看两个例子。 12a = t.arange(0, 6)a.storage() 0 1 2 3 4 5 [torch.LongStorage of size 6] 12b = a.view(2, 3)b.storage() 0 1 2 3 4 5 [torch.LongStorage of size 6] 123# 一个对象的id值可以看作它在内存中的地址# a和b storage的内存地址一样，即是同一个storageid(b.storage()) == id(a.storage()) True 1234# a改变，b也随之改变，因为它们共享storagea[1] = 100b tensor([[ 0, 100, 2], [ 3, 4, 5]]) 12c = a[2:]c.storage() 0 100 2 3 4 5 [torch.LongStorage of size 6] 12c.data_ptr(), a.data_ptr(), c.dtype # data_ptr返回tensor的首元素的内存地址# 可以看出相差16，这是因为2x8相差两个元素，每个元素占8个字节 (61509136, 61509120, torch.int64) 12c[0] = -100 # c[0]的内存地址对应a[2]内存地址a tensor([ 0, 100, -100, 3, 4, 5]) 123d = t.Tensor(c.float().storage())d[0] = 6666b tensor([[ 0, 100, -100], [ 3, 4, 5]]) 123# 下面4个共享storageid(a.storage()) == id(b.storage()) == id(c.storage()) == id(d.storage()) True 1a.storage_offset(), c.storage_offset(), a[3:].storage_offset() (0, 2, 3) 12e = b[::2, ::2] # 隔2行/列取一个元素id(e.storage()) == id(a.storage()) True 1b.stride(), e.stride() ((3, 1), (6, 2)) 1e.is_contiguous() False 可见绝大多数操作并不修改tensor的数据，只是修改头信息。这样更节省内存，同时提升了处理的速度。但是，有些操作会导致tensor不连续，这时需调用tensor.contiguous方法将他们变成连续数据，该方法复制数据到新的内存，不再与原来的数据共享storage。 另外高级索引一般不共享内存，而普通索引共享storage。 其他有关Tensor的话题持久化tensor的保存和加载十分简单，使用t.save和t.load即可完成相应功能。在save/load时可以指定使用的pickle模块，在load时还可以将GPU tensor映射到CPU或其他GPU上。 123456789if t.cuda.is_available(): a = a.cuda(1) t.save(a, 'a.pth') # 加载为b，储存于GPU1上（因为保存时就在GPU1上） b = t.load('a.pth') # 加载为c，储存在CPU c = t.load('a.pth', map_location = lambda storage,loc:storage) # 加载为d，储存于GPU0上 d = t.load('a.pth', map_location = &#123;'cuda:1':'cuda:0'&#125;) 向量化向量化计算是一种特殊的并行计算方式，通常是对不同的数据执行同样的一个或一批指令。向量化可极大第提高科学运算的效率。Python有许多操作很低效，尤其是for循环。在科学计算中要极力避免使用Python原生的for循环，尽量使用向量化的数值计算。 12345def for_loop_add(x, y): result = [] for i, j in zip(x, y): result.append(i + j) return t.Tensor(result) 12x = t.zeros(100)y = t.ones(100) 12%timeit -n 10 for_loop_add(x, y)%timeit -n 10 x + y 729 µs ± 414 µs per loop (mean ± std. dev. of 7 runs, 10 loops each) The slowest run took 4.81 times longer than the fastest. This could mean that an intermediate result is being cached. 3.5 µs ± 2.69 µs per loop (mean ± std. dev. of 7 runs, 10 loops each) 可见有好几百倍的速度差距，因此在实际使用中应尽量调用内建函数，这些函数底层由C/C++实现，能通过执行底层优化实现高效计算。 此为还需要注意几点： 大多数t.function都有一个参数out，这时产生的结果将保存在out指定的tensor之中 t.set_num_threads可以设置pytorch进行CPU多线程并行计算时所占用的线程数，来限制pytorch所占用的CPU数目。 t.set_printoptions可以用来设置打印tensor时的数值精度和格式。 1a = t.randn(2, 3); a tensor([[-0.1227, -0.0569, -0.6876], [ 1.6025, 0.6995, 0.1694]]) 1t.set_printoptions(precision = 10);a tensor([[-0.1226951405, -0.0568769276, -0.6875813603], [ 1.6024936438, 0.6995284557, 0.1693879962]])]]></content>
      <categories>
        <category>笔记</category>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[thinkload]读书-疑问]]></title>
    <url>%2F2019%2F08%2F13%2Fdeeplearning-book%2F</url>
    <content type="text"><![CDATA[2019-8-13 为什么要求算法最小化cost function却要以准确率错误率来衡量它？从cost到准确率丢失了衡量差距到底有多大的信息，这两个并不等价啊！ 如何衡量一个被选择的函数在某一目的上达到了多少效果？或者起到了多少作用？ 把输入端之前看成也连接了一个网络，这个网络是一种输出数据的网络，两个网络连接在一起看成一个大网络，而这种网络的生成目的应当是不同于后面我们人类设计网络的目的。但是输出的结果仍能一定程度上的符合人类需求，是不是说网络的每个部分可以存在不同的目标？ 有一个迷宫一样的屋子，同样也是一个挑战，找出从一楼走到天台的最短路径，有一万个人来接受挑战每个走到屋顶的人都被告知自己是否是走的最短路径，每个人都是独立的，有自己的判断标准，当他再次上楼时如何分析他们行为的变化？]]></content>
      <categories>
        <category>thinkload</category>
        <category>花书</category>
      </categories>
      <tags>
        <tag>thinkload</tag>
        <tag>花书</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[thinkload]]></title>
    <url>%2F2019%2F08%2F10%2Fthinkload%2F</url>
    <content type="text"><![CDATA[Thinkload, download something from my mind when thinking. 2019-08-10 要达到完全的确定性，在下定义的角度必然需要无穷个修饰词, 或者说解释无穷个足够的特性，或者说无穷个参数。 另一个角度，把具有存在的物体可以看做一个函数，一个从一个定义域（一个包含所有我们在意的物体的集合）映射到一个可观测领域的函数，对于这个可观测领域的人要准确确定其存在或理解其存在需要理解这个函数的所有参数。 而因为有很多观察角度，就有很多观测领域，对于每个领域的参数又显然是不同的，对一个领域准确注定对另一不同领域就不那么准确。 也可以说函数的参数确定了可观测领域，函数的参数里蕴含了可观测领域的性质，这么看物体本身到底是什么就不那么重要，更重要的是可观测领域的性质，更重要的是观察物体的角度。 那参数是怎么得到的呢？这里发现，我们认识一个新的物体的存在必然需要一些先天的或者一些先验的存在，比如视觉听觉等五感之类的，或者一些产生在大脑的感觉，更进一步是思维灵魂的东西，或者直接说自我的存在—-一个观察者自我的存在。 之前思考问题时犯了个错，不该不严谨的将物体给以“个”的量词，使得思考范围被放到离散的物体分布。 可能通过“试错”的方式学习有效，和反复试验取频率作为概率的方法有效这件事有相似性。另外，大数定律的内在含义到底是什么？为什么反复试验起作用？]]></content>
      <categories>
        <category>thinkload</category>
      </categories>
      <tags>
        <tag>thinkload</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【notes】docker学习笔记6-docker容器数据管理]]></title>
    <url>%2F2019%2F08%2F09%2Fdocker-6%2F</url>
    <content type="text"><![CDATA[docker容器的数据管理简介 docker容器的数据卷 docker的数据卷容器 docker数据卷的备份和还原 docker容器的数据卷什么是数据卷(Data Volume)docker的生存周期是与运行的程序相一致的，而我们需要数据持久化，docker容器之间也需要共享一些数据 数据卷是经过特殊设计的目录，可以绕过联合文件系统（UFS)，为一个或多个容器提供访问。 数据卷设计的目的，在于数据持久化，它完全对独立于容器的生存周期，因此docker不会在容器删除时删除其挂载的数据卷，也不会存在类似的垃圾收集机制，对容器引用的数据卷进行处理。 数据卷架构： docker数据卷独立于docker，独立运docker的生存周期。 docker数据卷位于docker的宿主机中文件系统。 docker数据卷既可以是目录也可是文件。 docker数据卷与宿主机进行数据共享。 同一目录或文件可以支持多个容器 数据卷的特点 数据卷在容器启动时初始化，如果容器使用的镜像在挂载点包含了数据，这些数据会拷贝到新初始化的数据卷中。 数据卷可以在容器之间共享和重用 可以对数据卷里的内容直接进行修改 数据卷的变化不会影响镜像的更新 卷会一直存在，即使挂载数据卷的容器已经被删除 数据卷的使用 为容器添加数据卷 1sudo docker run -v ~/container_data:/data -it ubuntu /bin/bash 在本机系统的目录:在容器中映射的目录名 注：这种方式（bind mount)已不推荐使用，应使用volume方式 123456docker volume create my_volume # 创建卷docker volume ls # 卷列表docker volume inspect my_volume #卷信息docker volume rm my_volume # 删除卷docker run -v [卷名]:[容器目录]:[选项列表] -it ubuntu /bin/bash 详情：https://deepzz.com/post/the-docker-volumes-basic.html 为数据卷添加访问权限 1sudo docker run -v [卷名]:[容器目录]:ro(访问权限) -it ubuntu /bin/bash 使用dockerfile构建包含数据卷的镜像dockerfile指令： VOLUME [“/data1”, “/data2”] 不能映射到本地目录，并且运行同一镜像的不同容器所创建的数据卷也是不一样的。 docker的数据卷容器什么是数据卷容器： 命名的容器挂载数据卷，其他容器通过挂载这个容器实现数据共享，挂载数据卷的容器，就叫做数据卷容器 图示： 挂载数据卷容器的方法 1docker run --volumes-from [CONTAINER NAME] 如果数据卷容器删除（即使同时删除挂载的数据卷）后，挂载该数据卷容器的容器的数据目录仍存在且有效。 数据卷容器的作用仅仅是将数据卷挂载的配置传递到挂载了数据卷容器的新容器中。 docker数据卷的备份与还原 数据备份方法 1docker run --volumes-from [container name] -v $(pwd):/backup ubuntu tar cvf /backup/backup.tar [container data volume] 数据还原方法 1docker run --volumes-from [container name] -v $(pwd):/backup ubuntu tar xvf /backup/backup.tar [container data volume]]]></content>
      <categories>
        <category>笔记</category>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【notes】docker学习笔记5-dockerfile]]></title>
    <url>%2F2019%2F08%2F08%2Fdocker-5%2F</url>
    <content type="text"><![CDATA[dockerfile指令指令格式注释： # Comment指令： INSTRUCTION argument FROM FROM \ FROM \:\ 必须已经存在的镜像，也就是基础镜像 必须是第一条非注释指令 MAINTAINER MAINTAINER \ 指定镜像的作者信息，包含镜像的所有者和练习方式 RUN构建构成运行的 RUN \ (shell模式) /bin/sh -c command RUN [“executable”, “param1”, “param2”] (exec模式) RUN[“/bin/bash”, “-c”, “echo hello”] EXPOSE EXPOSE \ [\…] 指定运行该镜像的容器使用的端口，但只是告诉docker会使用特定的端口号，出于安全考虑不会自动打开，在容器运行时仍需要手动指定端口映射。CMD ENTRYPOINT 指定容器启动时运行的命令 CMD [“executable”, “param1”, “param2”] (exec模式) CMD command param1 param2 (shell模式) CMD [“params1”, “params2”] (作为ENTRYPOINT指令的默认参数) 在docker run时如果指定命令的话dockerfile里的cmd命令会被覆盖掉。 ENTRYPOINT [“executable”, “param1”, “param2”] (exec模式) ENTRYPOINT command param1 param2 (shell模式) 默认不会被覆盖，如果需要覆盖需要指定docker run —entrypoint 覆盖 ADD COPY VOLUME设置镜像的目录和文件 ADD \…\ ADD [“]]></content>
      <categories>
        <category>笔记</category>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【notes】docker学习笔记4-docker客户端与守护进程]]></title>
    <url>%2F2019%2F08%2F07%2Fdocker-4%2F</url>
    <content type="text"><![CDATA[docker的C/S模式 客户端与守护进程通信的接口 命令行接口 remote API： RESTful风格API STDIN STDOUT STDERR 语言参考：https://docs.docker.com/reference/api/docker_remote_api 连接方式 unix:///var/run/docker.sock 默认方式 tcp://host:port fd://socketfd 利用socket进行通信 查看正在运行的守护进程 1ps -ef | grep docker 连接socket进行通信 12nc -U /var/run/docker.sockGET /info HTTP/1.1 以上都是在本地的访问，docker也支持远程访问。 docker守护进程的配置和操作 查看守护进程 12ps -ef | grep dockersudo status docker 守护进程的启动、停止和重启 123sudo service docker startsudo service docker stopsudo service docker restart docker的启动选项 1docker -d [OPTIONS] #所以守护形式运行 运行相关: -D, —debug = false -e, —exec-driver = “native” -g, —graph = “/var/lib/docker” —icc = true -l, —log-level = “info” —label = [] -p, —pidfile = “/var/run/docker.pid” docker服务器连接相关： -G, —group = “docker” -H, —host = [] —tls = false —tlscacert = “/home/sven/.docker/ca.pem” —tlscert = “/home/sven/.docker/cert.pem” —tlskey = “/home/sven/.docker/key.pem” —tlsverify = false RemotAPI相关： —api-enable-cors = false Registry相关： —insecure-registry = [] —registry-mirror = [] 网络设置相关： -b, —bridge = “” —bip = “” —fixed-cidr = “” —fixed-cidr-v6 = “” —dns = [] —dns-search = [] —ip = 0.0.0.0 —ip-forward = true —ip-masq = true —iptables = true —ipv6 = false —mtu = 0 启动配置文件 /etc/default/docker 注：ubuntu 16.04及以上版本使用： 修改/lib/systemd/system/docker.service中的ExecStart 加载配置： 123systemctl daemon-reloadservice docker restartdocker info docker的远程访问 第二台安装docker的服务器 保证Client API与Server API版本一致 修改docker守护进程启动选项 修改服务器端配置 -H tcp://host:post unix:///path/to/socket fd://* or fd//socketfd 守护进程默认配置： -H unix:///var/run/docker.sock 注：我的默认的是 fd:// 改为 tcp: tcp://0.0.0.0:2375 1curl http://ip:2375/info 修改客户端配置 -H tcp://host:post unix:///path/to/socket fd://* or fd//socketfd 默认配置： -H unix:///var/run/docker.sock docker -H tcp//ip:2375 # 太麻烦 export DOCKET_HOST=”tcp://ip:2357” # 使用环境变量 export DOCKET_HOST=”tcp://ip:2357” # 使用本地 怎样在设置了远程连接的服务器也支持本机连接？答：给-H再增加一个方式，-H可以设置多个值。]]></content>
      <categories>
        <category>笔记</category>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【notes】docker学习笔记3-docker镜像]]></title>
    <url>%2F2019%2F08%2F06%2Fdocker-3%2F</url>
    <content type="text"><![CDATA[查看和删除镜像 镜像的存储位置：/var/lib/docker 列出镜像1docker images [OPSIONS] [REPOSITORY] -a, —all = false # 显示所有镜像，默认并不显示中间层镜像（没有标签名的镜像） -f, —filter = [] # 过滤条件 —no-trunc = false # 不使用截断的形式来显示数据(默认使用截断显示EID，比文件名短) -q, —quiet = false # 只显示EID 镜像标签和仓库 镜像仓库 区分： REPOSITORY 仓库 REGISTRY 仓库 REGISTRY里会有很多REPOSITORY仓库，每个REPOSITORY里会有一个个独立的镜像。 标签 TAG 镜像的名字 = 镜像仓库名 : 镜像标签 —对应—&gt; 镜像ID ubuntu:latest, ubuntu:14.04, ….. 如果没有指定标签，默认为latest。 同一仓库的不同标签可以对应同一镜像ID，也就是说可以根据需求给同一镜像文件打上不同的标签。 没有标签名的镜像称作中间层镜像。 查看镜像1docker inspect [OPTIONS] CONTIANER|IMAGE [CONTAINER|IMAGE...] -f, —format=”” 删除镜像1docker rmi [OPTIONS] IMAGE [IMAGE...] -f, —force = false 强制删除 —no-prune = false 不删除未打标签的父镜像 对应多个标签的镜像文件可以直接用ID选定所有标签 1docker rmi ID 获取和推送镜像查找镜像 Docker Hub https://registry.hub.docker.com docker search 1docker search [OPTIONS] TERM —automated = false —no-trunc = false -s, stars = 0 只显示最少多少stars的 最多返回25个结果 拉取镜像1docker pull [OPTIONS] NAME [:TAG] -a, —all-tags = false 下载仓库中所有被标记的镜像 推送镜像1docker push username/IMAGE 构建镜像 保存对容器的修改，并再次使用 自定义镜像的能力 以软件的形式打包并分发服务及其运行环境 docker commit通过容器构建 1docker commit [OPTIONS] CONTAINER [REPOSITORY[:TAG]] -a, —author=”” Author e.g., “John Hannibal Smith hannibal@a-team.com” -m, —message=”” 记录构建的信息 -p, —pause = true 不暂停容器的运行 docker build通过Dockerfile文件构建 dockerfile: #First DockerfileFROM ubuntu:14.04MAINTAINER dormancypress “dormancypress@outlook.comRUN apt-get updateRUN apt-get install -y nginxEXPOSE 80 1docker build [OPTIONS] PATH|URL|- —force-rm = false —no-cache = false —pull=false -q,—quiet = false —rm = true -t, —tag=””]]></content>
      <categories>
        <category>笔记</category>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【notes】docker学习笔记2-docker容器]]></title>
    <url>%2F2019%2F08%2F06%2Fdocker-2%2F</url>
    <content type="text"><![CDATA[容器的基本操作启动容器 仅一次命令 1docker run IMAGE [COMMAND] [ARG] 启动交互式容器 1docker run -i -t IMAGE /bin/bash -i —interactive=ture | false 默认是false，为容器始终打开标准输入-t —tty=true | false 默认是false，分配一个终端 自定义容器名字 1docker run --name=自定义名 -i -t IMAGE /bin/bash 重新启动已建立的容器docker start [-i] 容器名 查看容器 不加参数是正在运行的容器，-a是所有容器，-l是最新创建的一个容器。 1docker ps [-a] [-l] 查看容器参数 1docker inspect [ID] or [name] 删除容器1docker rm 容器名 守护式容器什么是守护式容器： 能够长期运行 没有交互式会话 以守护形式运行容器：12docker run -i -t IMAGE /bin/bashCtrl + P Ctrl + Q 附加到运行中的容器1docker attach 容器名 启动守护式容器1docker run -d 镜像名 [COMMAND] [ARG...] 得知容器运行情况1docker logs [-f] [-t] [--tail] 容器名 -f —follows=true | false 默认为false 一直跟踪日志变化并返回结果-t —timestamps=true | false 默认为false 结果加上时间戳—tail= “all” 多少数量的日志 查看运行中容器进程1docker top 容器名 在运行中的容器内启动新进程1docker exec [-d] [-i] [-t] 容器名 [COMMAND] [ARG...] 停止守护式容器 发送指令等待停止 1docker stop 容器名 直接停止容器 1docker kill 容器名 在容器中部署静态网站设置容器的端口映射run [-P] -P , —publish-all = true | false 默认为false 为容器暴露的所有端口设置映射 1docker run -P -t -i ubuntu /bin/bash -p , 指定端口 容器端口 1docker run -p 80 -i -t ubuntu /bin/bash 宿主机端口:容器端口 1docker run -p 8080:80 -i -t ubuntu /bin/bash ip::容器端口 1docker run -p 0.0.0.0:80 -i -t ubuntu /bin/bash ip:宿主机端口:容器端口 1docker run -p 0.0.0.0:8080:80 -i -t ubuntu /bin/bash Nginx部署 创建映射80端口的交互式容器 1docker run -p 80 --name web -it ubuntu /bin/bash 安装Nginx 安装文本编辑器vim 1234apt-get updateapt-get upgradeapt-get install nginx -yapt-get install vim -y 创建静态页面 12mkdir -p /var/www/htmlvim index.html 修改Nginx配置文件 1vim /etc/nginx/sites-enabled/default 运行Nginx 123nginxps -efCtrl P Ctrl Q 验证网站访问 1234docker port web # 查看端口映射情况docker top web # 查看进程运行情况docker inspect web #查看ipcurl http://172.17.0.2]]></content>
      <categories>
        <category>笔记</category>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【notes】docker学习笔记1-docker基本组成]]></title>
    <url>%2F2019%2F08%2F06%2Fdocker-1%2F</url>
    <content type="text"><![CDATA[Docker的基本组成 Docker Client 客户端 Docker Daemon 守护进程 Docker Image 镜像 Docker Container 容器 Docker Registry 仓库 Docker客户端/守护进程 C/S架构 docker客户端对服务器的访问： 本地/远程 docker客户端向发送给守护进程请求，守护进程的执行结果还会传回给客户端。 Docker Image镜像 构建和打包阶段。 容器的基石，相当于保存了容器运行需要的源代码。 层叠的层叠文件系统。 bootfs（引导文件系统）-&gt; rootfs(Ubuntu) -&gt; add emacs -&gt; add Apache 联合加载（union mount）:一次加载多个文件系统（add Apache，add emacs），将所有文件系统叠加在一切。镜像可以叠加在一起，位于底部的成为基础镜像（rootfs），add emacs（副镜像）。 Docker Container容器 通过镜像启动。 启动执行阶段。 配置数据和镜像层（bootfs -&gt; ······ -&gt; add emacs) -&gt; 可写层。 写时复制：docker出现变化时都会应用到可写层，先从只读镜像层复制到可写层然后只读层的文件被隐藏。 Docker Registry仓库 保存docker镜像。 分为公有和私有。公有：Docker Hub 图示结构Docker: Docker Image: Docker Container: docker基本指令 查找镜像 1docker search tutorial 下载镜像 1docker pull learn/tutorial 启动一个容器，使用echo命令输出hello world 1docker run learn/tutorial echo 'hello world' 启动一个容器下载ping 1docker run learn/tutorial apt-get install -y ping 查看有哪些容器 1docker ps -l 提交容器，即创建一个新的镜像 1docker commit [docker ID] learn/ping 用新镜像建立一个容器 1docker run learn/ping ping www.baidu.com 查看容器信息 1docker inspect [docker ID] 查看有哪些镜像 1docker image 将镜像保存到docker hub上 1docker push /learn/ping Docker容器相关技术简介Docker依赖的Linux内核特性 Namespaces 命名空间 提供了系统资源的隔离，for轻量级虚拟化服务 五种命名空间： PID 进程隔离 NET 管理网络接口 IPC 管理跨进程通信的访问 MNT 管理挂载点 UTS 隔离内核和版本标识 Control groups 控制组 资源限制（内存上限等） 优先级设定（设定哪些进程组使用哪些资源） 资源计量 资源控制（挂起恢复） Docker容器的能力 文件系统隔离：每个容器都有自己的root文件系统 进程隔离： 每个容器都运行在自己的进程环境中 网络隔离： 容器间的虚拟网络接口和IP地址都是分开的 资源隔离和分组：使用cgroups将CPU和内存之类的资源独立分配给每个Docker容器]]></content>
      <categories>
        <category>笔记</category>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用Xshell管理虚拟机Ubuntu]]></title>
    <url>%2F2019%2F08%2F03%2Fxshell-vmware%2F</url>
    <content type="text"><![CDATA[因为使用VM虚拟机太过占用资源，所以我们可以用Xshell连接到虚拟机，来达到节省本机资源的目的。 安装SSH： 123sudo apt-get install openssh-serverservice iptables stop #关闭防火墙service ssh start #开启ssh服务 获得登录需要的ip ,在虚拟机输入： 1ifconfig ens*后面的inet后面的值就是ip。 按照我之前写过的xshell连接的教程 windows系统：Xshell下载安装+连接服务器 建立会话就ok，主机就是刚才你获得的ip，登录用的用户名和密码就是你安装时填的用户名(非root账户)和密码。 之后只需要打开虚拟机后最小化界面，从xshell登入后reboot一下虚拟机，这样从内存角度就能节省将近90多MB。 注意： reboot后就不要在打开VMware了，一直让它最小化直到关闭。]]></content>
      <categories>
        <category>Xshell</category>
      </categories>
      <tags>
        <tag>Xshell</tag>
        <tag>虚拟机</tag>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[photoshop cc 2019安装破解]]></title>
    <url>%2F2019%2F08%2F02%2Fphotoshop-cc-2019-download%2F</url>
    <content type="text"><![CDATA[Photoshop如今已经非常常用的处理图片的软件，本文就是介绍一下photoshop cc 2019安装破解的完整过程。 注：本文参考了http://www.3322.cc/soft/48343.html 下载creative cloud什么是creative cloud？creative cloud相当于adobe系列的一个应用商城，我们可以在里面安装各种adobe系列的软件。下载链接： 官网链接 网盘链接 下载完成直接按提示安装，然后注册adobe账号并登陆。 下载安装photoshop-cc-2019默认的下载位置在c盘，如果想改到其他盘可以点击右上角的三个点，出来菜单再点首选项。 然后点击creative Cloud界面，在安装位置条目处更改到你想安装到的位置。 打开creative cloud，找到photoshop的条目点击试用，photoshop自动下载安装成功。 利用补丁破解安装完成后安全起见先不要打开ps，我们先下载补丁工具。下载链接：网盘链接 其他链接 将压缩包里的adobe.photoshop.cc.2019.20.0.0-patch.exe文件解压到ps安装目录下，就是你刚才修改的安装位置，保证那个位置下有photoshop.exe文件。 然后点击运行补丁（你可能会听到一段诡异的音乐。。。）。 点击应用，等待出现文件补丁已成功完成的提示。 这样就破解完成了，这时再打开ps发现没有试用还有多少天的提醒了。 按照补丁制作者的建议，在 编辑 ==&gt; 首选项 ==&gt; 常规 ==&gt; 停用”主页”屏幕 打钩。 最后做好重启一下ps再试用。 注：这篇文章是我安装后就写了，我在安装完的七天后再次检验是否失效，如果失效我会更新补丁，如果补丁失效可以回来看我是否有更新方法。]]></content>
      <categories>
        <category>photoshop</category>
      </categories>
      <tags>
        <tag>破解</tag>
        <tag>photoshop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Xshell：在本地浏览器使用服务器的jupyter notebook]]></title>
    <url>%2F2019%2F08%2F02%2Fhexo-jupyter%2F</url>
    <content type="text"><![CDATA[有的服务器里只是命令行，无法可视化，可能就无法使用jupyter notebook。其实需要稍微修改一下连接的属性就能在本地浏览器里打开在服务器里启动的jupyter notebook，具体操作如下： 首先右击会话管理器里的服务器标签，在菜单点击属性。 然后点击左侧的隧道，然后再点击添加。 输入两个端口号，我这输入的是jupyter notebook默认的8888端口，然后点确定 然后再取消右下方转发X11连接到的选项，然后点确定。 之后双击会话管理器里的服务器进行连接，在命令行里输入jupyter notebook，启动后在浏览器里访问就会看到jupyter notebook的界面了。]]></content>
      <categories>
        <category>Xshell</category>
      </categories>
      <tags>
        <tag>Xshell</tag>
        <tag>jupyter notebook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[windows系统：Xshell下载安装+连接服务器]]></title>
    <url>%2F2019%2F08%2F01%2FXshellDownload%2F</url>
    <content type="text"><![CDATA[学习深度学习需要足够的计算资源，往往需要连接远程服务器用来计算。本篇文章就介绍一下如何在windows系统里利用xshell连接服务器。 xshell下载安装首先要下载安装包:百度网盘资源。当然也可以去官网下载安装包，选择家庭学校免费版，下载前要填一下姓名邮箱，提交后你会收到带有下载链接的邮件。 点击安装包，然后一路默认下一步就ok。如果不想安装在c盘也可以，在其他盘里专门存xshell的各种文件，安装过程中只需把主文件夹换成你刚才的文件夹就ok。 建立连接 打开xshell后，点击左上角的文件然后点新建。 然后填入服务器名称、主机、端口号,然后点确定。 双击左侧会话管理器里刚建的服务器，在弹出的窗口里填入登录用的用户名，选上记住用户名。 然后输入密码，并选上记住密码。 点击确定以后就能在黑色的shell看到已经登录成功的提示了，然后就可以在shell里进行操作了。 之后登录只需要双击左侧会话管理器里的对应标签即可。 上传下载文件 在Linux主机上，安装上传下载工具包rz及sz，使用sudo apt install lrzsz 进行安装。 从Windows上传文件，上传命令为rz；输入命令后会弹出选择要上传的本地文件的窗口。 从Linux主机下载文件，下载命令为sz ，后面跟要下载的文件名。例如： sz helloworld.py。 然后就会弹出选择要保存到本机位置的窗口。 xshell的基本操作就说这些了，这些的操作已经基本够用了]]></content>
      <categories>
        <category>Xshell</category>
      </categories>
      <tags>
        <tag>Xshell</tag>
        <tag>linux服务器</tag>
        <tag>windows</tag>
      </tags>
  </entry>
</search>
