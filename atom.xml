<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>今天写了点啥？</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://zinw623.github.io/"/>
  <updated>2020-07-17T03:24:36.205Z</updated>
  <id>https://zinw623.github.io/</id>
  
  <author>
    <name>贰三</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>python爬虫：爬取动态网站</title>
    <link href="https://zinw623.github.io/2020/01/13/dongtaipachong/"/>
    <id>https://zinw623.github.io/2020/01/13/dongtaipachong/</id>
    <published>2020-01-13T11:29:15.000Z</published>
    <updated>2020-07-17T03:24:36.205Z</updated>
    
    <content type="html"><![CDATA[<h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>本章的内容重点是学习爬取动态网页。之前爬取的网站是静态的网页，内容都在html的代码中，直接分析提取即可。然而现在不少的网站是动态的，其爬取会更复杂一些。下面是百度百科对动态网页的解释。</p><img width="90%" data-src="/uploads/pachong/1.png"><p>简单的说，动态网页是先发送请求获取没有实际我们想要的内容的一个网站的空的基本的框架，再通过后续请求往这个空的框架里填充各种数据和内容，网页需要更新内容时，只需要请求需要更新的具体内容，然后把需要更新的网站的部分替换掉，这样就不需要重新加载整个网页，其内容是可以随着时间变化的。所以之前静态网页的方法只能获得一个空的框架，得不到想要的数据。</p><p>对于动态网站的爬取主要可以通过两种方法实现：</p><ul><li>利用浏览器的开发者工具分析网站，寻找与需要爬取的目标数据对应的隐藏<code>URL</code>，<code>headers</code>等数据，借助<code>requests</code>模块实现请求，进而爬取到数据。</li><li>利用模块<code>selenium</code>模拟浏览器行为，即模拟键盘输入、点击、滑动页面等行为，获得网页完成所有请求后的源代码，再利用静态网页的方法进行后续的处理。</li></ul><p>我们学习的重点是第一种方法，学会分析网站可以更高效的实现动态网站的爬取。第二种方法较为麻烦，不如第一种需要学习新的模块，而且还有使用指定的浏览器，下载合适版本的驱动，还需要将内容从源代码里提取出来，而使用第一种方法会发现，往往不需要专门去提取数据，所以第二种方法就不再详细叙述。</p><p>动态网页的爬取需要积累经验，一个小的细节就决定了程序是否能实现，所以希望读者能去实战各种网页，积累更多的经验，来应对各种各样的网站。</p><h3 id="实例：腾讯天气数据"><a href="#实例：腾讯天气数据" class="headerlink" title="实例：腾讯天气数据"></a>实例：腾讯天气数据</h3><p>动态网页的爬取以爬取腾讯天气数据的实例来讲解。<br>首先搜索腾讯天气，在浏览器里进入腾讯天气的网站 <a href="https://tianqi.qq.com/" target="_blank" rel="noopener">https://tianqi.qq.com/</a> ，并打开开发者工具，选择<code>Network</code>，重新加载网页（见下图）。</p><img data-src="/uploads/pachong/2.png"><p>一般动态网页的后续数据加载的方式为<code>XHR</code>或者<code>JS</code>，有时也可能是<code>Doc</code>。就是图中分类的前两项和第七项。使用谷歌浏览器的开发者工具时如果遇到XHR会有暂停，而这个网站没有出现暂停，所以可以得知后续数据的请求是通过JS实现的。所以选择JS来寻找。</p><img width="70%" data-src="/uploads/pachong/3.png"><p>然后要做的就是从这些JS请求中找到请求天气数据的请求。<br>这里要用到一条经验，名字是.js结尾的可以不用看，因为这种文件是网站用来请求<code>JavaScript</code>的代码，是不会有我们需要的数据的。<br>排除掉.js文件后，先点击一个文件，点击选择右侧的<code>Preview</code>，然后继续点击浏览一遍剩余的文件的内容，找出目标数据文件即可。</p><img width="70%" data-src="/uploads/pachong/4.png"><p>最后找到了目标文件（如上图所示），请求返回的内容是一个天气数据的一个列表，之后需要做的就是利用这一请求的信息（见下图）来实现一个爬虫，来爬取天气数据，返回一个便于操作的字典数据即可。</p><img width="70%" data-src="/uploads/pachong/5.png"><p>重点看一下参数部分：<br><code>source</code>应该就是请求源，值为<code>PC</code>。<br><code>weather_type</code>为请求的天气数据类型，比如<code>forecast_1h</code>是逐小时数据，<code>forecast_24h</code>是每天的数据，<code>alarm</code>是一些天气预警等。<br><code>province</code>就是数据对应位置的省，<code>city</code>就是对应的地级市，<code>county</code>就是对应的县级市，这个就根据需求改变即可。<br><code>callback</code>的内容就涉及了一个小技巧，我们一般会删去它，它的值就对应了这个请求的响应内容中开头部分，一般为<code>jquery</code>加上一串数字，删去它的值就使得响应只是一个字典格式的字符串，没有了<code>jquery</code>那些多余的东西，我们就可以直接用<code>eval</code>函数将返回的字符串转换成python的字典数据了。<br>_的值是一个叫做时间戳的数据，用来标识请求的时间，这个可以通过<code>time</code>模块中的<code>time</code>函数来调用获取。<br>分析完请求的参数就可以通过代码来实现这个程序了。</p><img data-src="/uploads/pachong/6.png"><p>这一部分是首先引入必要的模块：<code>requests</code>和<code>time</code>。Location是一个用来储存URL部分参数的字典，Time是储存运行程序时的时间戳。</p><img data-src="/uploads/pachong/7.png"><p>只够根据需求更改URL的参数的值。</p><img data-src="/uploads/pachong/8.png"><p><code>headers</code>是表头的数据，用<code>requests</code>的<code>get</code>函数进行请求，html里是响应的text文本，是一个字典格式的字符串，直接用<code>eval</code>函数将字符串转化成python的字典数据，然后直接用python字典数据的知识提取需要的数据就可以。这样一个简单的动态网页的爬虫就完成了。</p><p>这个程序完整的源代码为：</p><img data-src="/uploads/pachong/9.png"><p>对于这个例子来说，我们不用再分析网页源代码，唯一稍费功夫的地方就是去找到那个隐藏的请求的信息，这个方法就需要读者多去练习，提高分析网页请求的能力。</p><h4 id="练习任务："><a href="#练习任务：" class="headerlink" title="练习任务："></a>练习任务：</h4><ul><li>尝试爬取豆瓣喜剧的排行榜数据。</li><li>尝试在百度图片里爬取python相关的图片。</li></ul><h3 id="迁移拓展"><a href="#迁移拓展" class="headerlink" title="迁移拓展"></a>迁移拓展</h3><p>完成一个爬虫的思路主要为：</p><img data-src="/uploads/pachong/10.png"><p>各种爬虫基本上都是在这四步的基础上进行扩展，比如动态网页就是在第一步获取请求信息中，充分运用开发者工具分析找出关键的隐藏请求，之后的处理基本就与<br>静态的爬虫流程无异。所以，当读者想要实现一些特殊爬虫的时候，就是基于这四步，更改这四步中某几步的细节或方法来迁移性地实现新的类型的爬虫，下面就举几个例子来详细叙述一些。</p><h4 id="QQ空间动态爬取"><a href="#QQ空间动态爬取" class="headerlink" title="QQ空间动态爬取"></a>QQ空间动态爬取</h4><p>QQ空间的好友动态爬取是属于动态网站的爬取。<br>首先，第一步是获取请求的信息，在浏览器上登录你的QQ空间，然后打开开发者工具，然后刷新空间里的动态，然后尝试找到获取动态请求的请求，第一步就完成了。<br>第二步就是根据第一步获得的信息来进行代码实现，这里有一点需要注意，你想拿到的是你QQ好友的动态，所以发送的请求里要有你的账号信息才行，而这一些标识你身份的信息，比如你的账号和密码，都已经隐藏在<code>cookie</code>里，所以说，一般<code>headers</code>里的参数要填全，虽然可能有些参数不填不影响请求，但这需要你分析出那些是不必要的参数，所以说，为了省事还是填全为好。<br>第三、四步就是处理响应内容，这个请求的响应内容一般是一个xml文件，合理解码处理后可以获得一个具有HTML格式的字符串，可以直接用<code>Beautifulsoup</code>模块处理或正则表达式等，这里注意一点，一些没有在某个节点下的文本会被<code>beautifulsoup</code>模块直接放到一个</p><p>标签下，所以响应下的字符串里即使不全是html格式的字符串也可以直接输入给<code>beautifulsoup</code>函数。</p><h4 id="手机app的爬虫"><a href="#手机app的爬虫" class="headerlink" title="手机app的爬虫"></a>手机app的爬虫</h4><p>对于app的爬虫和常见的动态爬虫主要的差别也就出现在第一步，也就是如何获得请求信息。我们知道，依赖网络的app本质上也是一次次的隐藏起来的请求，动态网页我们有开发者工具这种具有抓包功能的工具，它把各种带有请求数据的数据包抓住复制了一份供我们分析。所以为了实现app的爬虫只需要找到一个能过替代开发者工具的抓包工具，只要把请求的数据包抓取下来，有了请求的信息，剩下的就和动态网页的爬虫没有什么区别了。<br>常见的手机抓包工具有：fiddler，charles等<br>这类抓包工具的使用网上都有教程，这里就不赘述了。<br>比如fiddler教程：<a href="https://www.cnblogs.com/hzg1981/p/5610530.html" target="_blank" rel="noopener">https://www.cnblogs.com/hzg1981/p/5610530.html</a><br>总结一下，基本上绝大部分爬虫基本都可以基于那四步实现，所以当遇到特殊的爬虫，可以根据那四步从其他的之前做过的爬虫中迁移性地想出适合特定的条件的爬虫，做到融会贯通，面对各式各样的网站都能应对自如。    </p><h4 id="练习："><a href="#练习：" class="headerlink" title="练习："></a>练习：</h4><ul><li>实现QQ空间动态的爬虫</li><li>爬取斗鱼直播app每一个推荐分类下的排名前十的直播间的主播名称、房间号、观看人数、房间名称、排名等信息。</li></ul><h4 id="补充知识"><a href="#补充知识" class="headerlink" title="补充知识"></a>补充知识</h4><p>最后再补充一些读者做爬虫时可能遇到的问题。</p><ul><li>中文乱码：<ul><li>使用代码reponse.encoding = reponse.apparent_encoding </li><li>请求头‘Accept-Encoding’中的br问题：<br>  去除br或者用brotl包解码<br><a href="https://www.jianshu.com/p/70c3994efcd8?utm_source=oschina-app" target="_blank" rel="noopener">https://www.jianshu.com/p/70c3994efcd8?utm_source=oschina-app</a></li></ul></li></ul><ul><li>有些网站不能频繁请求，可以通过设置时间间隔解决，代码为：time.sleep(1)。</li><li>有的网站会有反爬机制，同一个ip发送请求过多会被封ip，这时可以使用代理    ip，即使用requests.get的proxies参数。<br>  获取免费代理ip的网站：<a href="https://www.xicidaili.com/" target="_blank" rel="noopener">https://www.xicidaili.com/</a><br>  requests设置代理ip的方法：<a href="https://www.cnblogs.com/z-x-y/p/9355223.html" target="_blank" rel="noopener">https://www.cnblogs.com/z-x-y/p/9355223.html</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;p&gt;本章的内容重点是学习爬取动态网页。之前爬取的网站是静态的网页，内容都在html的代码中，直接分析提取即可。然而现在不少的网站是动态的，其爬取
      
    
    </summary>
    
      <category term="python爬虫" scheme="https://zinw623.github.io/categories/python%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="python" scheme="https://zinw623.github.io/tags/python/"/>
    
      <category term="爬虫" scheme="https://zinw623.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>翻译：pytorch数据加载和处理</title>
    <link href="https://zinw623.github.io/2019/09/04/DATA-LOADING-AND-PROCESSING/"/>
    <id>https://zinw623.github.io/2019/09/04/DATA-LOADING-AND-PROCESSING/</id>
    <published>2019-09-04T11:59:29.000Z</published>
    <updated>2019-09-05T08:20:45.416Z</updated>
    
    <content type="html"><![CDATA[<p>在解决任何机器学习问题时，都需要花费大量的精力来准备数据。PyTorch提供了许多工具来简化数据加载，希望能使代码更具可读性。在这篇教程里，我们将看看如何从非平凡的数据集中加载和预处理/扩增数据。</p><a id="more"></a><p>为了运行这篇教程，请确保一下模块已经安装了：</p><ul><li><code>scikit-image</code>：为了图像的输入输出和转化</li><li><code>pandas</code>：为了跟容易解析csv</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function, division</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> skimage <span class="keyword">import</span> io, transform</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms, utils</span><br><span class="line"></span><br><span class="line"><span class="comment"># 忽略警告</span></span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">'ignore'</span>)</span><br><span class="line"></span><br><span class="line">plt.ion()   <span class="comment"># 交互模式</span></span><br></pre></td></tr></table></figure><p>我们将要处理的数据集是人脸姿态。这以为着一张脸被标注成这样：</p><p>@import “landmarked_face2.png”</p><p>总共有68个标注点标注在每张脸上。</p><ul><li>注意：<br>  从<a href="https://download.pytorch.org/tutorial/faces.zip" target="_blank" rel="noopener">这里</a>下载数据集，图片在‘data/faces/’目录下。这个数据集是基于imagenet中被标记为‘face’的一些图片通过应用优秀的<a href="https://blog.dlib.net/2014/08/real-time-face-pose-estimation.html" target="_blank" rel="noopener">dlib的姿态估计</a>生成。</li></ul><p>数据集带有一个csv文件，其中带有类似于下面的注释：</p><pre><code>image_name,part_0_x,part_0_y,part_1_x,part_1_y,part_2_x, ... ,part_67_x,part_67_y0805personali01.jpg,27,83,27,98, ... 84,1341084239450_e76e00b7e7.jpg,70,236,71,257, ... ,128,312</code></pre><p>让我们快速读取csv并且得到(N, 2)数组的标注，N是指的标注点的个数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">landmarks_frame = pd.read_csv(<span class="string">'data/faces/face_landmarks.csv'</span>)</span><br><span class="line"></span><br><span class="line">n = <span class="number">65</span></span><br><span class="line">img_name = landmarks_frame.iloc[n, <span class="number">0</span>]</span><br><span class="line">landmarks = landmarks_frame.iloc[n, <span class="number">1</span>:].as_matrix()</span><br><span class="line">landmarks = landmarks.astype(<span class="string">'float'</span>).reshape(<span class="number">-1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Image name: &#123;&#125;'</span>.format(img_name))</span><br><span class="line">print(<span class="string">'Landmarks shape: &#123;&#125;'</span>.format(landmarks.shape))</span><br><span class="line">print(<span class="string">'First 4 Landmarks: &#123;&#125;'</span>.format(landmarks[:<span class="number">4</span>]))</span><br></pre></td></tr></table></figure><p>输出：</p><pre><code>Image name: person-7.jpgLandmarks shape: (68, 2)First 4 Landmarks: [[32. 65.][33. 76.][34. 86.][34. 97.]]</code></pre><p>让我们写一个简单的展示一张图片和它的标注点的帮助函数，使用它来显示一个样本。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_landmarks</span><span class="params">(image, landmarks)</span>:</span></span><br><span class="line">    <span class="string">"""显示一张带有标注点的函数"""</span></span><br><span class="line"></span><br><span class="line">    plt.imshow(image)</span><br><span class="line">    plt.scatter(landmarks[:, <span class="number">0</span>], landmarks[:, <span class="number">1</span>], s = <span class="number">10</span>, marker = <span class="string">'.'</span>, c = <span class="string">'r'</span>)</span><br><span class="line">    plt.pause(<span class="number">0.001</span>)    <span class="comment"># 暂停一会等待更新</span></span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">show_landmarks(io.imread(os.path.join(<span class="string">'data/faces/'</span>, img_name)), landmarks)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>@import “sphx_glr_data_loading_tutorial_001.png”</p><h3 id="Dataset类"><a href="#Dataset类" class="headerlink" title="Dataset类"></a>Dataset类</h3><p><code>torch.utils.data.Dataset</code>是表示数据集的抽象类。你的自定义数据集应该继承<code>Dataset</code>并覆盖以下方法：</p><ul><li><code>__len__</code>以便<code>len(dataset)</code>返回dataset的大小</li><li><code>__getitem__</code>来支持索引操作，像是<code>dataset[i]</code>用来获得第i个样本</li></ul><p>让我们创建一个我们脸部表注数据集的dataset吧。我们将在<code>__init__</code>中读取csv但是留在<code>__getitem__</code>中读取图片。这是为了内存效率因为所有的图片不是一次储存在内存中，而是按需要储存。</p><p>我们数据集的赝本将会是字典<code>{&#39;image&#39;:image, &#39;landmarks&#39;:landmarks}</code>。我们数据集将获得一个选填参数<code>transform</code>以便对样本进行所有必要的处理。我们将在下一个章节看到<code>transform</code>的有效性。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FaceLandmarksDataset</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, csv_file, root_dir, transform = None)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        参数：</span></span><br><span class="line"><span class="string">            csv_file(string):csv file的路径</span></span><br><span class="line"><span class="string">            root_dir(string):所有图片的目录</span></span><br><span class="line"><span class="string">            transform(callable, 选填):被应用到样本上的transforms</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.landmarks_frame = pd.read_csv(csv_file)</span><br><span class="line">        self.root_dir = root_dir</span><br><span class="line">        self.transform = transform</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> len(self.landmarks_frame)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, idx)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> torch.is_tensor(idx):</span><br><span class="line">            idx = idx.tolist()</span><br><span class="line">        </span><br><span class="line">        img_name = os.path.join(self.root_dir, self.landmarks_frame.iloc[idx, <span class="number">0</span>])</span><br><span class="line">        images = io.imread(img_name)</span><br><span class="line">        landmarks = self.landmarks_frame.iloc[idx, <span class="number">1</span>:]</span><br><span class="line">        landmarks = np.array([landmarks])</span><br><span class="line">        landmarks = landmarks.astype(<span class="string">'float'</span>).reshape(<span class="number">-1</span>, <span class="number">2</span>)</span><br><span class="line">        sample = &#123;<span class="string">'image'</span>:image, <span class="string">'landmarks'</span>:landmarks&#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.transform:</span><br><span class="line">            sample = self.transform(sample)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> sample</span><br></pre></td></tr></table></figure><p>让我们举例使用这个类并且在迭代这个数据集。我们将打印出前四个样本的大小和它们的标注点。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">face_dataset = FaceLandmarksDataset(csv_file = <span class="string">'data/faces/face_landmarks.csv'</span>, root_dir = <span class="string">'data/faces/)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">fig = plt.figure()</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">for i in range(len(face_dataset)):</span></span><br><span class="line"><span class="string">    sample = face_dataset[i]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    print(i, sample['</span>image<span class="string">'].shape, sample['</span>landmarks<span class="string">'].shape)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    ax = plt.subplot(1, 4, i + 1)</span></span><br><span class="line"><span class="string">    plt.tight_layout()</span></span><br><span class="line"><span class="string">    ax.set_title('</span>Sample <span class="comment"># &#123;&#125;'.format(i))</span></span><br><span class="line">    ax.axis(<span class="string">'off'</span>)</span><br><span class="line">    show_landmarks(**sample)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> i == <span class="number">3</span>:</span><br><span class="line">        plt.show()</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在解决任何机器学习问题时，都需要花费大量的精力来准备数据。PyTorch提供了许多工具来简化数据加载，希望能使代码更具可读性。在这篇教程里，我们将看看如何从非平凡的数据集中加载和预处理/扩增数据。&lt;/p&gt;
    
    </summary>
    
      <category term="翻译" scheme="https://zinw623.github.io/categories/%E7%BF%BB%E8%AF%91/"/>
    
      <category term="pytorch文档（1.2.0）" scheme="https://zinw623.github.io/categories/%E7%BF%BB%E8%AF%91/pytorch%E6%96%87%E6%A1%A3%EF%BC%881-2-0%EF%BC%89/"/>
    
    
      <category term="pytorch" scheme="https://zinw623.github.io/tags/pytorch/"/>
    
      <category term="翻译" scheme="https://zinw623.github.io/tags/%E7%BF%BB%E8%AF%91/"/>
    
  </entry>
  
  <entry>
    <title>翻译：pytorch官网60分钟教程（1.2.0）</title>
    <link href="https://zinw623.github.io/2019/09/03/pytorch-60-minutes/"/>
    <id>https://zinw623.github.io/2019/09/03/pytorch-60-minutes/</id>
    <published>2019-09-03T14:17:34.000Z</published>
    <updated>2019-09-03T14:24:01.701Z</updated>
    
    <content type="html"><![CDATA[<p>译者博客：<a href="https://zinw623.github.io/">https://zinw623.github.io/</a></p><h2 id="什么是PYTORCH"><a href="#什么是PYTORCH" class="headerlink" title="什么是PYTORCH?"></a>什么是PYTORCH?</h2><p><a href="https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py" target="_blank" rel="noopener">[原英文网站]</a></p><p>它是基于python的科学计算包，读者定位为两种：</p><ul><li>替代使用Numpy来使用GPU的功能</li><li>最灵活快速的深度学习研究平台</li></ul><h3 id="准备开始"><a href="#准备开始" class="headerlink" title="准备开始"></a>准备开始</h3><h4 id="Tensors"><a href="#Tensors" class="headerlink" title="Tensors"></a>Tensors</h4><p>Tensors是类似于加上能在GPU上进行加速计算功能的Numpy的ndarrays。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure><blockquote><p>注意：<br>    未初始化的矩阵被声明，但是在使用前不会包含确切的已知的值。当一个未初始化的矩阵被创建，分配内存中当时的任何值都将作为初始值出现。</p></blockquote><p>构建一个5x3的矩阵，未初始化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.empty(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure><p>输出:</p><pre><code>tensor([[0., 0., 0.],        [0., 0., 0.],        [0., 0., 0.],        [0., 0., 0.],        [0., 0., 0.]])</code></pre><p>构建一个随机的初始化过的矩阵：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure><p>输出：</p><pre><code>tensor([[0.6259, 0.0797, 0.8297],        [0.6732, 0.7944, 0.2363],        [0.6775, 0.2497, 0.3846],        [0.8515, 0.5171, 0.6957],        [0.7759, 0.6000, 0.1323]])</code></pre><p>构建一个dtype为long且用0填充的矩阵：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.zeros(<span class="number">5</span>, <span class="number">3</span>, dtype=torch.long)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure><p>输出：</p><pre><code>tensor([[0, 0, 0],        [0, 0, 0],        [0, 0, 0],        [0, 0, 0],        [0, 0, 0]])</code></pre><p>构建一个直接从data里构建tensor：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">5.5</span>, <span class="number">3</span>])</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure><p>输出：</p><pre><code>tensor([5.5000, 3.0000])</code></pre><p>或者从已有的tensor创建tensor。这些方法将重用输入tensor的内容，例如dtype，除非使用者提供新的值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = x.new_ones(<span class="number">5</span>, <span class="number">3</span>, dtype = torch.double) <span class="comment"># new_*方法接受了大小（sizes）</span></span><br><span class="line">print(x)</span><br><span class="line"></span><br><span class="line">x = torch.randn_like(x, dtype = torch.float) <span class="comment"># 重写了dtype</span></span><br><span class="line">print(x)                                     <span class="comment"># 结构具有相同的size</span></span><br></pre></td></tr></table></figure><p>输出：</p><pre><code>tensor([[1., 1., 1.],        [1., 1., 1.],        [1., 1., 1.],        [1., 1., 1.],        [1., 1., 1.]], dtype=torch.float64)tensor([[ 0.5955, -0.2528, -0.2648],        [ 0.7689,  0.2396, -0.0121],        [ 1.3478,  0.0460,  0.0255],        [ 0.1266, -1.1526, -0.5546],        [-0.2001, -0.0542, -0.6439]])</code></pre><p>得到它的size（大小）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(x.size())</span><br></pre></td></tr></table></figure><p>输出：</p><pre><code>torch.Size([5, 3])</code></pre><blockquote><p>注意：<br>    <code>torch.Size</code>事实上是元组，所以它支持所有的元组操作。</p></blockquote><h4 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h4><p>有多种操作的语法。在下面的例子里，我们将看一下加法操作。</p><p>加法：语法1</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(x + y)</span><br></pre></td></tr></table></figure><p>输出：</p><pre><code>tensor([[ 1.1550,  0.5950, -0.0519],        [ 1.3954,  0.9232,  0.8904],        [ 1.7020,  0.8187,  0.0265],        [ 0.3831, -0.6057, -0.2829],        [ 0.5647,  0.5976,  0.1128]])</code></pre><p>加法：语法2</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(torch.add(x, y))</span><br></pre></td></tr></table></figure><p>输出：</p><pre><code>tensor([[ 1.1550,  0.5950, -0.0519],        [ 1.3954,  0.9232,  0.8904],        [ 1.7020,  0.8187,  0.0265],        [ 0.3831, -0.6057, -0.2829],        [ 0.5647,  0.5976,  0.1128]])</code></pre><p>加法：提供一个输出向量作为参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">result = torch.emtpy(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">torch.add(x, y, out = result)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure><p>输出：</p><pre><code>tensor([[ 1.1550,  0.5950, -0.0519],        [ 1.3954,  0.9232,  0.8904],        [ 1.7020,  0.8187,  0.0265],        [ 0.3831, -0.6057, -0.2829],        [ 0.5647,  0.5976,  0.1128]])</code></pre><p>加法：in-place</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 把x加到y上</span></span><br><span class="line">y.add_(x)</span><br></pre></td></tr></table></figure><p>输出：</p><pre><code>tensor([[ 1.1550,  0.5950, -0.0519],        [ 1.3954,  0.9232,  0.8904],        [ 1.7020,  0.8187,  0.0265],        [ 0.3831, -0.6057, -0.2829],        [ 0.5647,  0.5976,  0.1128]])</code></pre><blockquote><p>注意：<br>    任何改变张量的in-place操作后固定带一个<code>_</code>。例如：<code>x.copy_(y)</code>,<code>x.t_()</code>，将改变<code>x</code>的值。</p></blockquote><p>你可以使用类似于标准的numpy索引的所有附加功能。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(x[:, <span class="number">1</span>])</span><br></pre></td></tr></table></figure><p>输出：</p><pre><code>tensor([-0.2528,  0.2396,  0.0460, -1.1526, -0.0542])</code></pre><p>改变大小：如果你想resize/reshape张量，你可以使用<code>torch.view</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">y = x.view(<span class="number">16</span>)</span><br><span class="line">z = x.view(<span class="number">-1</span>, <span class="number">8</span>)  <span class="comment"># size -1 从其他维度推断</span></span><br><span class="line">print(x.size(), y.size(), z.size())</span><br></pre></td></tr></table></figure><p>输出：</p><pre><code>torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])</code></pre><p>如果你有一个元素的张量，可以使用<code>.item()</code>获得python number的值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">1</span>)</span><br><span class="line">print(x)</span><br><span class="line">print(x.item())</span><br></pre></td></tr></table></figure><p>输出：</p><pre><code>tensor([-0.8748])-0.8748161792755127</code></pre><p>然后阅读：<br>100+张量操作，<a href="https://pytorch.org/docs/torch" target="_blank" rel="noopener">here</a></p><h3 id="numpy桥"><a href="#numpy桥" class="headerlink" title="numpy桥"></a>numpy桥</h3><p>将Torch Tensor转换成numpy array，反之也很简单。</p><p>Torch Tensor和numpy array潜在地共享内存（如果torch tensor在CPU上），并且改变一个将会使另一个改变。</p><h4 id="将NumPy-Array转换成Torch-Tensor"><a href="#将NumPy-Array转换成Torch-Tensor" class="headerlink" title="将NumPy Array转换成Torch Tensor"></a>将NumPy Array转换成Torch Tensor</h4><p>看如何通过改变np array自动地改变Torch Tensor。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.ones(<span class="number">5</span>)</span><br><span class="line">b = torch.from_numpy(a)</span><br><span class="line">np.add(a, <span class="number">1</span>, out = a)</span><br><span class="line">print(a)</span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure><p>输出：</p><pre><code>[2. 2. 2. 2. 2.]tensor([2., 2., 2., 2., 2.], dtype=torch.float64)</code></pre><h3 id="CUDA张量"><a href="#CUDA张量" class="headerlink" title="CUDA张量"></a>CUDA张量</h3><p>tensors可以通过<code>.to</code>方法被移动到任何设备。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 只有CUDA可用时运行这个cell</span></span><br><span class="line"><span class="comment"># 我们使用"torch.device"控制张量出入GPU</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    device = torch.device(<span class="string">"cuda"</span>)           <span class="comment"># 一个CUDA device对象</span></span><br><span class="line">    y = torch.ones_like(x, device = device) <span class="comment"># 直接在GPU上创建tensor</span></span><br><span class="line">    x = x.to(device)                        <span class="comment"># 或者使用字符串".to("cuda")"</span></span><br><span class="line">    z = x + y</span><br><span class="line">    print(z)</span><br><span class="line">    print(z.to(<span class="string">"cpu"</span>, torch.double))        <span class="comment"># ''.to''也能一起改变dtype</span></span><br></pre></td></tr></table></figure><p>输出：</p><pre><code>tensor([0.1252], device=&apos;cuda:0&apos;)tensor([0.1252], dtype=torch.float64)</code></pre><h2 id="AUTOGRAD-自动求导"><a href="#AUTOGRAD-自动求导" class="headerlink" title="AUTOGRAD:自动求导"></a>AUTOGRAD:自动求导</h2><p><a href="https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py" target="_blank" rel="noopener">[原英文网站]</a></p><p>Pytorch所有神经网络的核心是<code>autograd</code>包。让我们简单地看一下这个，然后我们将要去训练我们的第一个神经网络。</p><p><code>autograd</code>包为所有tensors操作提供自动求导。它是一个定义即运行的框架，这以为着你的代码如何运行你的反向传播就如何被定义，每一次迭代都可以不同。</p><p>让我们用更简单的术语和一些例子来看看。</p><h3 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h3><p><code>torch.Tensor</code>是包的核心类。如果你设置了它的属性<code>.requires_grad</code>为<code>True</code>，它开始时会追踪所有作用在它之上的操作。当你完成你的计算时你可以通过调用<code>.backward()</code>并且自动地计算所有梯度。这个张量的梯度将会被累积到<code>.grad</code>这个属性里。</p><p>为了组织张量追踪历史，你可以调用<code>.detach()</code>来从计算历史中将它分离，并且防止了在未来计算中被追踪。</p><p>为了防止追踪历史（并且使用内存），你也可以将代码块包装到<code>with torch.no_grad():</code>。这在当评估模型时非常有帮助，因为模型可能有<code>requires_grad=True</code>的可训练参数，但是我们并不需要梯度。</p><p>为了autograd的执行还有另一个非常重要的类 - <code>Function</code>。</p><p><code>Tensor</code>和<code>Function</code>是相互关联的并且建立一个无环图，这图编码了计算的完整历史。每一个张量都有一个<code>.grad_fn</code>属性，参照了创建这个<code>Tensor</code>的<code>Function</code>（除了被用户创建的张量 - 它们的<code>grad_fn is None</code>）。</p><p>如果你想要计算衍生物，你可以调用一个<code>Tensor</code>的<code>.backward()</code>方法。如果<code>Tensor</code>是一个标量（即只有一个元素），你不需要为<code>.backward()</code>指定任何参数，然而如果它有更多的元素，你需要指定一个<code>gradient</code>参数，它是一个匹配形状的张量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure><p>创建一个张量并且设置<code>requires_grad=True</code>并追踪它的计算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">2</span>, <span class="number">2</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure><p>输出：</p><pre><code>tensor([[1., 1.],        [1., 1.]], requires_grad=True)</code></pre><p>进行一个张量计算：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y = x + <span class="number">2</span></span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure><p>输出：</p><pre><code>tensor([[3., 3.],        [3., 3.]], grad_fn=&lt;AddBackward0&gt;)</code></pre><p><code>y</code>作为一个操作的结果被创建，所以它有一个<code>grad_fn</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(y.grad_fn)</span><br></pre></td></tr></table></figure><p>输出：</p><pre><code>&lt;AddBackward0 object at 0x7f3772e36588&gt;</code></pre><p>对<code>y</code>做更多的操作</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">z = y * y * <span class="number">3</span></span><br><span class="line">out = z.mean()</span><br><span class="line"></span><br><span class="line">print(z, out)</span><br></pre></td></tr></table></figure><p>输出：</p><pre><code>tensor([[27., 27.],        [27., 27.]], grad_fn=&lt;MulBackward0&gt;) tensor(27., grad_fn=&lt;MeanBackward0&gt;)</code></pre><p><code>.requires_grad_( ... )</code>in-place改变已存在张量的<code>requires_grad=True</code>标示。如果没有给定输入默认的标示是<code>False</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">a = ((a * <span class="number">3</span>) / (a - <span class="number">1</span>))</span><br><span class="line">print(a.requires_grad)</span><br><span class="line">a.requires_grad_(<span class="literal">True</span>)</span><br><span class="line">print(a.requires_grad)</span><br><span class="line">b = (a * a).sum()</span><br><span class="line">print(b.grad_fn)</span><br></pre></td></tr></table></figure><p>输出：</p><pre><code>FalseTrue&lt;SumBackward0 object at 0x7f3772e36dd8&gt;</code></pre><h3 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h3><p>现在开始反向传播。因为<code>out</code>包含一个单独的标量，<code>out.backward()</code>等价于<code>out.backward(torch.tensor(1.))</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">out.backward()</span><br></pre></td></tr></table></figure><p>打印梯度d(out)/dx</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(x.grad)</span><br></pre></td></tr></table></figure><p>输出：</p><pre><code>tensor([[4.5000, 4.5000],        [4.5000, 4.5000]])</code></pre><p>你应当有一个<code>4.5</code>的矩阵。让我们调用<code>out</code>的张量“o”，我们有$o=\frac{1}{4}\sum_{i}z_i,z_i=3(x_i+2)^2$和$z_i\mid_{x_i=1}=27$。然而，$\frac{\partial{o}}{\partial{x_i}}\mid_{x_i=1}=\frac{9}{2}=4.5$。</p><p>数学上的，如果你有一个向量值的函数$\vec{y}=f(\vec{x})$，然后$\vec{y}$关于$\vec{x}$的梯度是雅克比矩阵：</p><p>$$<br>J =<br>\left[<br>\begin{matrix}<br>\frac{\partial{y_1}}{\partial{x_1}}  &amp; \cdots &amp;\frac{\partial{y_1}}{\partial{x_n}}  \<br> \vdots &amp; \ddots &amp; \vdots \<br> \frac{\partial{y_m}}{\partial{x_1}}  &amp; \cdots &amp; \frac{\partial{y_m}}{\partial{x_n}}   \<br>\end{matrix}<br>\right]<br>$$</p><p>总的来说，<code>torch.autograd</code>是为了计算向量-雅克比积(vector-Jacobian product)。那是给定向量$v=(v_1  v_2  \cdots v_m)^T$，计算积$v^T \cdot J$。如果$v$恰好是标量函数$l=g(\vec{y})$的梯度，就是$v=(\frac{\partial{l}}{\partial{y_1}}\cdots\frac{\partial{l}}{\partial{y_m}})^T$，然后通过链式法则，向量-雅克比积将会成为$l$关于$\vec{x}$的梯度：</p><p>$$<br>J^T\cdot v =<br>\left[<br>\begin{matrix}<br>\frac{\partial{y_1}}{\partial{x_1}}  &amp; \cdots &amp;\frac{\partial{y_1}}{\partial{x_n}}  \<br> \vdots &amp; \ddots &amp; \vdots \<br> \frac{\partial{y_m}}{\partial{x_1}}  &amp; \cdots &amp; \frac{\partial{y_m}}{\partial{x_n}}   \<br>\end{matrix}<br>\right]<br>\left[<br>\begin{matrix}<br>\frac{\partial{l}}{\partial{y_1}} \<br>\vdots \<br>\frac{\partial{l}}{\partial{y_m}}<br>\end{matrix}<br>\right]=<br>\left[<br>\begin{matrix}<br>\frac{\partial{l}}{\partial{x_1}} \<br>\vdots \<br>\frac{\partial{l}}{\partial{x_n}}<br>\end{matrix}<br>\right]<br>$$</p><p>（注意：$v^T\cdot J$给出了一个行向量，这个可以通过$J^T\cdot v$得到列向量。）</p><p>向量-雅克比积的这个特征使得给一个非标量输出的模型输入一个的外部梯度非常便捷。</p><p>现在让我们看看一个向量-雅克比积的例子。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">3</span>, requires_grad = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">y = x * <span class="number">2</span></span><br><span class="line"><span class="keyword">while</span> y.data.norm() &lt; <span class="number">1000</span>:</span><br><span class="line">    y = y * <span class="number">2</span></span><br><span class="line"></span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure><p>输出：</p><pre><code>tensor([-1350.9803,   805.9799,  -188.3773], grad_fn=&lt;MulBackward0&gt;)</code></pre><p>现在<code>y</code>不在是标量，<code>torch.autograd</code>不能直接计算完整的雅克比矩阵，但是如果我们只是想要向量-雅克比积，只需简单地输入一个向量给<code>backward</code>作为参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">v = torch.tensor([<span class="number">0.1</span>, <span class="number">1.0</span>, <span class="number">0.0001</span>], dtype = torch.float)</span><br><span class="line">y.backward(v)</span><br><span class="line"></span><br><span class="line">print(x.grad)</span><br></pre></td></tr></table></figure><p>输出：</p><pre><code>tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])</code></pre><p>你也能通过把代码块封装到<code>with torch.no_grad():</code>中来停止autograd追踪带有<code>.requires_grad=True</code>的张量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">print(x.requires_grad)</span><br><span class="line">print((x ** <span class="number">2</span>).requires_grad)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    print((x ** <span class="number">2</span>).requires_grad)</span><br></pre></td></tr></table></figure><p>输出:</p><pre><code>TrueTrueFalse</code></pre><h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><p><a href="https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html" target="_blank" rel="noopener">[原英文网站]</a></p><p>神经网络可以通过<code>torch.nn</code>包来构建。</p><p>现在你看一眼<code>autograd</code>，<code>nn</code>依赖于<code>autograd</code>来定义模型并且对他们求导。一个<code>nn.Module</code>包括层（layers)和一个<code>forward(input)</code>方法，这个方法会返回<code>outupt</code>。</p><p>例如，看这个分类数字图像的网络：</p><p><img alt data-src="/uploads/torch-60-tu/mnist.png"></p><center>卷积网络</center><p>这是一个简单的前馈神经网络。它接受一个输入，将它输入穿过许多层然后一个接着另一个，然后最后给出一个输出。</p><p>一个典型的神经网络的训练流程如下：</p><ul><li>定义一个带有许多可学习的参数（或权重）的神经网络。</li><li>在输入的训练集上迭代。</li><li>通过网络处理输入。</li><li>计算损失（loss 输出与正确有多远）。</li><li>反向传播求网络的梯度。</li><li>更新网络权重，典使用一个简单典型的更新规则：<code>weight = weight - learning_rate * gradient</code></li></ul><h3 id="定义网络"><a href="#定义网络" class="headerlink" title="定义网络"></a>定义网络</h3><p>让我们来定义网络：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        <span class="comment"># 1是图片通道，6是输出通道，3x3正方形卷积</span></span><br><span class="line">        <span class="comment"># 核（kernel）</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">3</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">3</span>)</span><br><span class="line">        <span class="comment"># 一个仿射操作：y = wx + b</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">6</span> * <span class="number">6</span>, <span class="number">128</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># 在(2, 2)窗口上的最大池化</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv1(x)), (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">        <span class="comment"># 如果是正方形size可以指定一个数</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv2(x)), <span class="number">2</span>)</span><br><span class="line">        x = x.view(<span class="number">-1</span>, self.num_flat_feature(x))</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> num_features</span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line">print(net)</span><br></pre></td></tr></table></figure><p>输出：</p><pre><code>Net((conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))(conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))(fc1): Linear(in_features=576, out_features=120, bias=True)(fc2): Linear(in_features=120, out_features=84, bias=True)(fc3): Linear(in_features=84, out_features=10, bias=True))</code></pre><p>你只需要定义一个<code>forward</code>函数，<code>backward</code>函数（计算梯度的函数）会自动使用<code>autograd</code>定义。你在<code>forward</code>函数里使用任何张量操作。</p><p>模型的可学习参数通过<code>net.parameters()</code>获得</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">params = list(net.parameters())</span><br><span class="line">print(len(params))</span><br><span class="line">print(params[<span class="number">0</span>].size())     <span class="comment"># conv1的权重</span></span><br></pre></td></tr></table></figure><p>输出：</p><pre><code>10torch.Size([6, 1, 3, 3])</code></pre><p>让我们尝试一个随机的32x32的输入。注意：这个网络（LeNet）期望输入的大小是32x32。在MNIST数据集上使用这个网络，请把数据集的图片的大小转换成32x32。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">input = torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line">out = net(input)</span><br><span class="line">print(out)</span><br></pre></td></tr></table></figure><p>输出：</p><pre><code>tensor([[ 0.0057, -0.0952,  0.1055, -0.0916, -0.1350,  0.0857, -0.0890,  0.0326,        -0.0554,  0.1451]], grad_fn=&lt;AddmmBackward&gt;)</code></pre><p>清零所有参数的梯度的缓冲和用随机的梯度进行反向传播。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net.zero_grad()</span><br><span class="line">out.backward(torch.randn(<span class="number">1</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure><blockquote><p>注意：<br>    <code>torch.nn</code>只支持mini-batches(小的批处理)。整个<code>torch.nn</code>包只支持输入是样本的mini-batch，而不是单一的样本。<br>    例如，<code>nn.Conv2d</code>将接受一个<code>样本数x通道数x高x宽</code>的四维张量。<br>    如果你有一个单一的样本，只需使用<code>input.unsqueeze(0)</code>来增加一个伪batch维度。</p></blockquote><p>在进行下一步之前，让我们重新回顾你目前看过的所有的类。</p><p>回顾：</p><ul><li><code>torch.Tensor</code> - 一个支持像是<code>backward()</code>的自动求导的多维数组。也保留关于这个张量的梯度。</li><li><code>nn.Module</code> - 神经网络模块。封装参数的便捷方式。帮助他们移动到GPU，导出，加载等。</li><li><code>nn.Parameter</code> - 一种Tensor，当它被分配给一个<code>Module</code>时，它会被自动注册为一个参数。</li><li><code>autograd.Function</code> - 执行前向和一个自动求导操作的反向定义。每个<code>Tensor</code>操作创建至少一个单一的<code>Function</code>节点，与创建<code>Tensor</code>的和编码它的历史的函数。</li></ul><p>至此，我们讨论了：</p><ul><li>定义一个神经网络</li><li>执行了输入和调用反向传播</li></ul><p>还剩下：</p><ul><li>计算损失</li><li>更新网络的权重</li></ul><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>损失函数接受一对输入（output, target)，和计算输出与目标之间距离的值。</p><p>在nn包下有很多不同的<a href="https://pytorch.org/docs/nn.html#loss-functions" target="_blank" rel="noopener">损失函数</a>。一个简单的损失是：<code>nn.MSELoss</code>,这个是计算输入和目标的均方误差。</p><p>例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">output = net(input)</span><br><span class="line">target = torch.randn(<span class="number">10</span>)</span><br><span class="line">target = target.view(<span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line"></span><br><span class="line">loss = criterion(output, target)</span><br><span class="line">print(loss)</span><br></pre></td></tr></table></figure><p>输出：</p><pre><code>tensor(0.9991, grad_fn=&lt;MseLossBackward&gt;)</code></pre><p>现在，如果你使用它的<code>.grad_fn</code>属性沿着<code>loss</code>的方向向后移动，你将看到像这样的计算图：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">input -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d</span><br><span class="line">      -&gt; view -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear</span><br><span class="line">      -&gt; MSELoss</span><br><span class="line">      -&gt; loss</span><br></pre></td></tr></table></figure><p>所以，当我们调用<code>loss.backward()</code>，整个图对loss求导，并且图中所有有<code>requires_grad=True</code>的张量有一个累积梯度的<code>.grad</code>张量。</p><p>为了说明这一点，我们后退几步：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(loss.grad_fn)  <span class="comment"># MSELoss</span></span><br><span class="line">print(loss.grad_fn.next_functions[<span class="number">0</span>][<span class="number">0</span>]) <span class="comment"># Linear</span></span><br><span class="line">print(loss.grad_fn.next_functions[<span class="number">0</span>][<span class="number">0</span>].next_function[<span class="number">0</span>][<span class="number">0</span>])   <span class="comment"># Relu</span></span><br></pre></td></tr></table></figure><p>输出：</p><pre><code>&lt;MseLossBackward object at 0x7ff716c28630&gt;&lt;AddmmBackward object at 0x7ff716c28400&gt;&lt;AccumulateGrad object at 0x7ff716c28400&gt;</code></pre><h3 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h3><p>为了反向传播误差我们必须做的全部只是调用<code>loss.backward()</code>。不过，您需要清除现有的梯度，否则梯度将累积为现有梯度。</p><p>现在，我们将调用<code>loss.backward()</code>，看一下conv1的偏置梯度在反向之前和之后的区别。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">net.zero_grad()         <span class="comment"># 清零所有参数的梯度缓冲</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">'conv1.bias.grad before backward'</span>)</span><br><span class="line">print(net.conv1.bias.grad)</span><br><span class="line"></span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'conv1.bias.grad after backward'</span>)</span><br><span class="line">print(net.conv1.bias.grad)</span><br></pre></td></tr></table></figure><p>输出：</p><pre><code>conv1.bias.grad before backwardtensor([0., 0., 0., 0., 0., 0.])conv1.bias.grad after backwardtensor([ 0.0081,  0.0029,  0.0248, -0.0054,  0.0051,  0.0008])</code></pre><p>现在，我们看如何使用损失函数。</p><p>我们剩下需要学习的是：</p><ul><li>更新网络的权重</li></ul><h3 id="更新参数"><a href="#更新参数" class="headerlink" title="更新参数"></a>更新参数</h3><p>在实践中使用的最简单的更新规则是随机梯度下降（SGD)。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line"><span class="keyword">for</span> f <span class="keyword">in</span> net.parameters():</span><br><span class="line">    f.data.sub_(f.grad.data * learning_rate)</span><br></pre></td></tr></table></figure><p>然而，当你使用神经网络时，你想要使用不同的更新规则像是SGD，Nesterov-SGD，Adam，RMSProp等等。为了其中这些，你可以使用一个小的包：<code>torch.optim</code>来执行所有这些方法。使用它非常简单：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建你的优化器</span></span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr = <span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在训练循环里</span></span><br><span class="line">optimizer.zero_grad()   <span class="comment"># 清零梯度缓冲</span></span><br><span class="line">output = net(input)</span><br><span class="line">loss = criterion(output, target)</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()        <span class="comment"># 更新</span></span><br></pre></td></tr></table></figure><blockquote><p>注意：<br>    观察到必须是手动地使用<code>optimizer.zero_grad()</code>清零梯度缓冲的。这是因为梯度是累积的，这个在反向传播里解释了。</p></blockquote><h2 id="训练一个分类器"><a href="#训练一个分类器" class="headerlink" title="训练一个分类器"></a>训练一个分类器</h2><p><a href="https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html" target="_blank" rel="noopener">[原英文网站]</a></p><p>就是这个。你已经看过如何定义神经网络，计算损失和更新网络的参数。</p><p>现在你可能会思考了，</p><h3 id="那数据呢？"><a href="#那数据呢？" class="headerlink" title="那数据呢？"></a>那数据呢？</h3><p>总体来说，当你不得不处理图片，文本，音频，视频数据，你可以使用标准的python库来把它们加载成numpy数组，然后你可以将数组转化成<code>torch.*Tensor</code>。</p><ul><li>对于图像，像是Pillow,OpenCV包是有效的</li><li>对于音频，有scipy或librosa包</li><li>对于文本，原始的python或基于Cython加载，或NLTK和SpaCy都是有效的。</li></ul><p>特别的对于视觉方面，我们已经创建了一个叫做<code>torchvision</code>的包，它提供了常见数据集（Imagenet，CIFAR10，MNIST等等）的数据加载器和图片的数据处理器，也就是<code>torchvision.datasets</code>和<code>torch.utils.data.DataLoader</code>。</p><p>这提供了很大的便利和避免了编写样本代码。</p><p>对于这个教程，我们将使用CIFAR10数据集。它有类别：飞机、汽车、鸟、猫、鹿、狗、青蛙、马、船、卡车。CIFAR-10中的图片都是3x32x32的，也就是32x32像素大小、三颜色通道的图片。</p><p><img alt data-src="/uploads/torch-60-tu/cifar10.png"></p><center>cifar10</center><h3 id="训练一个图片分类器"><a href="#训练一个图片分类器" class="headerlink" title="训练一个图片分类器"></a>训练一个图片分类器</h3><p>我们将依次进行下面的几步：</p><ol><li>使用<code>torchvision</code>加载和标准化CIFAR10的训练集和测试集</li><li>定义卷积神经网络</li><li>定义损失函数</li><li>在训练集上训练神经网络</li><li>在测试集上测试网络</li></ol><h4 id="1-加载和规范化CIFAR10"><a href="#1-加载和规范化CIFAR10" class="headerlink" title="1.加载和规范化CIFAR10"></a>1.加载和规范化CIFAR10</h4><p>使用<code>torchvision</code>，非常简单地加载CIFAR10</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br></pre></td></tr></table></figure><p>torchvision datasets输出是在范围[0, 1]的PILImage的图片。我们可以将他们处理成规范化过的范围在[-1, 1]的张量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.Compose(</span><br><span class="line">    [transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainset = torchvision.datasets.CIFAR10(root=<span class="string">'./data'</span>, train = <span class="literal">True</span>, download = <span class="literal">True</span>, transform=transform)</span><br><span class="line">trainloader = torch.utils.data.DataLoader(trainset, batch_size = <span class="number">4</span>, shuffle=<span class="literal">True</span>, num_workers = <span class="number">2</span>)</span><br><span class="line">testset = torchvision.datasets.CIFAR10(root=<span class="string">'./data'</span>, train = <span class="literal">True</span>, download = <span class="literal">True</span>, transform = transform)</span><br><span class="line">testloader = torch.utils.data.DataLoader(testset, batch_size = <span class="number">4</span>, shuffle=<span class="literal">False</span>, num_workers = <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">classes = (<span class="string">'plane'</span>, <span class="string">'car'</span>, <span class="string">'bird'</span>, <span class="string">'cat'</span>, <span class="string">'deer'</span>, <span class="string">'dog'</span>, <span class="string">'frog'</span>, <span class="string">'horse'</span>, <span class="string">'ship'</span>, <span class="string">'truck'</span>)</span><br></pre></td></tr></table></figure><p>输出：</p><pre><code>Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gzExtracting ./data/cifar-10-python.tar.gz to ./dataFiles already downloaded and verified</code></pre><p>让我们展示一些训练集图片。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 展示图片的函数</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">imshow</span><span class="params">(img)</span>:</span></span><br><span class="line">    img = img / <span class="number">2</span> + <span class="number">0.5</span></span><br><span class="line">    npimg = img.numpy()</span><br><span class="line">    plt.imshow(np.transpose(npimg, (<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获得一些随机的训练集图片</span></span><br><span class="line">dataiter = iter(trainloader)</span><br><span class="line">images, labels = dataiter.next()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 展示图片</span></span><br><span class="line">imshow(torchvision.utils.make_grid(images))</span><br><span class="line"><span class="comment"># 打印标签</span></span><br><span class="line">print(<span class="string">' '</span>.join(<span class="string">'%5s'</span> % classes[labels[j]] <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">4</span>)))</span><br></pre></td></tr></table></figure><p><img alt data-src="/uploads/torch-60-tu/sphx_glr_cifar10_tutorial_001.png"></p><p>输出：</p><pre><code>frog  ship   cat plane</code></pre><h4 id="2-定义一个卷积神经网络"><a href="#2-定义一个卷积神经网络" class="headerlink" title="2.定义一个卷积神经网络"></a>2.定义一个卷积神经网络</h4><p>从之前的神经网络章节复制神经网络，并且修改成三通道的图片（它之前定义的是一通道的图片）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.pool(F.relu(self.conv1(x)))</span><br><span class="line">        x = self.pool(F.relu(self.conv2(x)))</span><br><span class="line">        x = x.view(<span class="number">-1</span>, <span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">net = Net()</span><br></pre></td></tr></table></figure><h4 id="3-定义损失函数和优化器"><a href="#3-定义损失函数和优化器" class="headerlink" title="3.定义损失函数和优化器"></a>3.定义损失函数和优化器</h4><p>让我们使用分类交叉熵损失和带有动量的SGD。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr = <span class="number">0.001</span>, momentum = <span class="number">0.9</span>)</span><br></pre></td></tr></table></figure><h4 id="4-训练网络"><a href="#4-训练网络" class="headerlink" title="4.训练网络"></a>4.训练网络</h4><p>这时事情变得有趣起来。我们简单地循环一下我们的数据迭代器，把输入喂给网络并且做优化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">2</span>):   <span class="comment"># 循环几次数据集</span></span><br><span class="line"></span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(trainloader, <span class="number">0</span>):</span><br><span class="line">        <span class="comment"># 获得输入；数据是[输入，标签]的列表</span></span><br><span class="line">        inputs, labels = data</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 清零参数的梯度</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 前向+反向+优化</span></span><br><span class="line">        outputs = net(inputs)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 打印统计信息</span></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">2000</span> == <span class="number">1999</span>:    <span class="comment"># 每2000个mini-batches打印一次</span></span><br><span class="line">            print(<span class="string">'[%d, %5d] loss: %.3f'</span> % (epoch + <span class="number">1</span>, i + <span class="number">1</span>, running_loss / <span class="number">2000</span>))</span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">'Finished Training'</span>)</span><br></pre></td></tr></table></figure><p>输出：</p><pre><code>[1,  2000] loss: 2.169[1,  4000] loss: 1.808[1,  6000] loss: 1.659[1,  8000] loss: 1.553[1, 10000] loss: 1.488[1, 12000] loss: 1.455[2,  2000] loss: 1.379[2,  4000] loss: 1.346[2,  6000] loss: 1.320[2,  8000] loss: 1.305[2, 10000] loss: 1.275[2, 12000] loss: 1.262Finished Training</code></pre><h4 id="5-在测试集上测试网络"><a href="#5-在测试集上测试网络" class="headerlink" title="5.在测试集上测试网络"></a>5.在测试集上测试网络</h4><p>我们已经在训练集上训练了两次网络。但是我们需要检测网络是否有学习。</p><p>我们将通过网络预测输出的类别标签检测这个，并且和事实做对比。如果预测是正确的，我们向正确预测的列表里添加该样本。</p><p>OK，第一步。让我们从测试集中取出图片来展示熟悉一下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">dataiter = iter(testloader)</span><br><span class="line">images, labels = dataiter.next()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印图片</span></span><br><span class="line">imshow(torch.vision.utils.make_grid(images))</span><br><span class="line">print(<span class="string">'GroundTruth: '</span>, <span class="string">' '</span>.join(<span class="string">'%5s'</span> % classes[labels[j]] <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">4</span>)))</span><br></pre></td></tr></table></figure><p><img alt data-src="/uploads/torch-60-tu/sphx_glr_cifar10_tutorial_002.png"></p><p>输出：</p><pre><code>GroundTruth:    cat  ship  ship plane</code></pre><p>ok，现在让我们看看神经网络认为上面的例子是什么：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">output = net(images)</span><br></pre></td></tr></table></figure><p>输出是这10类的信念值（energies）。越高信念值的一类，模型越认为图片是这一类。所以，让我们最高信念值的标签：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">_, predicted = torch.max(output, <span class="number">1</span>)</span><br><span class="line">print(<span class="string">'Predicted:'</span>, <span class="string">' '</span>.join(<span class="string">'%5s'</span> % classes[predicted[j]] <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">4</span>)))</span><br></pre></td></tr></table></figure><p>输出：</p><pre><code>Predicted:    cat plane plane  ship</code></pre><p>结果看起来不错。</p><p>让我们看一下网络在整个数据集上表现怎么样。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">correct = <span class="number">0</span></span><br><span class="line">total = <span class="number">0</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> testloader:</span><br><span class="line">        images, labels = data</span><br><span class="line">        outputs = net(images)</span><br><span class="line">        _, predicted = torch.max(outputs.data, <span class="number">1</span>)</span><br><span class="line">        total += labels.size(<span class="number">0</span>)</span><br><span class="line">        correct += (predicted == labels).sum().item()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Accuracy of the network on the 10000 test images: %d %%'</span> % (</span><br><span class="line">    <span class="number">100</span> * correct / total))</span><br></pre></td></tr></table></figure><p>输出：</p><pre><code>Accuracy of the network on the 10000 test images: 54 %</code></pre><p>看起来比碰运气（10%的准确率，从10类中随机选一类）要好的多。像是网络已经学习了一些东西。</p><p>嗯<del>~</del>，哪些类表现的好，哪些类表现的不好呢：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">class_correct = list(<span class="number">0.</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>))</span><br><span class="line">class_total = list(<span class="number">0.</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>))</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> testloader:</span><br><span class="line">        images, labels = data</span><br><span class="line">        outputs = net(images)</span><br><span class="line">        _, predicted = torch.max(outputs, <span class="number">1</span>)</span><br><span class="line">        c = (predicted == labels).squeeze()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">            label = labels[i]</span><br><span class="line">            class_correct[label] += c[i].item()</span><br><span class="line">            class_total[label] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    print(<span class="string">'Accuracy of %5s : %2d %%'</span> % (</span><br><span class="line">        classes[i], <span class="number">100</span> * class_correct[i] / class_total[i]))</span><br></pre></td></tr></table></figure><p>输出：</p><pre><code>Accuracy of plane : 46 %Accuracy of   car : 63 %Accuracy of  bird : 50 %Accuracy of   cat : 37 %Accuracy of  deer : 40 %Accuracy of   dog : 51 %Accuracy of  frog : 70 %Accuracy of horse : 48 %Accuracy of  ship : 76 %Accuracy of truck : 64 %</code></pre><p>ok，那下一步干什么？</p><p>我们如何在GPU上运行神经网络？</p><h3 id="在GPU上训练"><a href="#在GPU上训练" class="headerlink" title="在GPU上训练"></a>在GPU上训练</h3><p>就像如何把张量移动到GPU上，把神经网络移动到GPU上。</p><p>如果我们有可用的CUDA， 首先让我们定义设备为第一个可得到的cuda设备。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">"cuda:0"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设我们有一个CUDA机器，然后我们打印CUDA设备：</span></span><br><span class="line"></span><br><span class="line">print(device)</span><br></pre></td></tr></table></figure><p>输出：</p><pre><code>cuda:0</code></pre><p>这个章节的剩余内容都假设<code>device</code>是一个CUDA设备。</p><p>然后这些方法将在所有module上递归运行，把它们的参数和缓冲转换成CUDA tensors。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net.to(device)</span><br></pre></td></tr></table></figure><p>记得你也要将每次迭代的输入和标签都送到GPU：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">inputs, labels = data[<span class="number">0</span>].to(device), data[<span class="number">1</span>].to(device)</span><br></pre></td></tr></table></figure><p>为什么我没有注意到与CPU相比的巨大加速?因为你的网络真的很小。</p><p><strong>练习</strong>: 尝试增加网络的宽度（第一个<code>nn.Conv2d</code>的参数2和第二个<code>nn.Conv2d</code>的参数1必须相等），看看你得到怎样的加速。</p><p><strong>已完成的目标</strong>:</p><ul><li>高层次地理解了pytorch张量的库和神经网络</li><li>训练了一个小的图片分类的神经网络</li></ul><h3 id="在多GPU上训练"><a href="#在多GPU上训练" class="headerlink" title="在多GPU上训练"></a>在多GPU上训练</h3><p>如果你想使用你全部的GPU看到巨大的加速，请参考下一章节</p><h2 id="选读：数据并行"><a href="#选读：数据并行" class="headerlink" title="选读：数据并行"></a>选读：数据并行</h2><p><a href="https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html#imports-and-parameters" target="_blank" rel="noopener">[原英文网站]</a></p><p>在这个教程里，我们将使用<code>DataParallel</code>学习如何使用多个GPUS（译者注：一机多卡）</p><p>pytorch很容易实现多GPU。你能把模型放到一个GPU上：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">"cuda:0"</span>)</span><br><span class="line">model.to(device)</span><br></pre></td></tr></table></figure><p>然后你可以把你的所有数据放到GPU上：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mytensor = my_tensor.to(device)</span><br></pre></td></tr></table></figure><p>请注意只是调用<code>my_tensor.to(device)</code>返回一个新的在GPU上的<code>my_tensor</code>的拷贝，而不是覆盖<code>my_tensor</code>。你需要分配一个新的tensor并使用这个在GPU上的tensor。</p><p>在多GPU上执行正向和反向传播是很自然的。然而，pytorch将默认使用一个GPU。你可以使用<code>DataParallel</code>使你的模型并行运行来简单地在多GPU上运行你的操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = nn.DataParallel(model)</span><br></pre></td></tr></table></figure><p>这就是本教程的核心。我们将在下面更详细地探讨它。</p><h3 id="引入和参数"><a href="#引入和参数" class="headerlink" title="引入和参数"></a>引入和参数</h3><p>引入pytorch模块和定义参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"></span><br><span class="line"><span class="comment"># Parameters and DataLoaders</span></span><br><span class="line">input_size = <span class="number">5</span></span><br><span class="line">output_size = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">30</span></span><br><span class="line">data_size = <span class="number">100</span></span><br></pre></td></tr></table></figure><p>设备：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">"cuda:0"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br></pre></td></tr></table></figure><h3 id="虚拟数据集"><a href="#虚拟数据集" class="headerlink" title="虚拟数据集"></a>虚拟数据集</h3><p>建立一个虚拟（随机）的数据集，你只需执行getitem。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RandomDataset</span><span class="params">(Dataset)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, length)</span>:</span></span><br><span class="line">        self.len = length</span><br><span class="line">        self.data = torch.randn(length, size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.data[index]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.len</span><br><span class="line"></span><br><span class="line">rand_loader = DataLoader(dataset=RandomDataset(input_size, data_size),</span><br><span class="line">                         batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h3 id="简单的模型"><a href="#简单的模型" class="headerlink" title="简单的模型"></a>简单的模型</h3><p>作为小样，我们的模型只需获得输入，执行线性操作，然后得出输出。然而，你能使用<code>DataParallel</code>在任何模型上(CNN, RNN, Capsule Net等等。)</p><p>我们在模型中放置了一个print语句来监视输入和输出张量的大小。请注意批号为0的打印内容。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="comment"># Our model</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, output_size)</span>:</span></span><br><span class="line">        super(Model, self).__init__()</span><br><span class="line">        self.fc = nn.Linear(input_size, output_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span></span><br><span class="line">        output = self.fc(input)</span><br><span class="line">        print(<span class="string">"\tIn Model: input size"</span>, input.size(),</span><br><span class="line">              <span class="string">"output size"</span>, output.size())</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><h3 id="创建模型和数据并行"><a href="#创建模型和数据并行" class="headerlink" title="创建模型和数据并行"></a>创建模型和数据并行</h3><p>这是本教程的核心内容。首先，我们需要创建一个模型实例和检查是否有多个GPU。如果我们有多个GPU，我们可以使用<code>nn.DataParallel</code>封装你的模型。然后我们把模型通过<code>model.to(device)</code>放到GPU上。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">model = Model(input_size, output_size)</span><br><span class="line"><span class="keyword">if</span> torch.cuda.device_count() &gt; <span class="number">1</span>:</span><br><span class="line">  print(<span class="string">"Let's use"</span>, torch.cuda.device_count(), <span class="string">"GPUs!"</span>)</span><br><span class="line">  <span class="comment"># dim = 0 [30, xxx] -&gt; [10, ...], [10, ...], [10, ...] on 3 GPUs</span></span><br><span class="line">  model = nn.DataParallel(model)</span><br><span class="line"></span><br><span class="line">model.to(device)</span><br></pre></td></tr></table></figure><p>输出：</p><pre><code>Let&apos;s use 2 GPUs!</code></pre><h3 id="运行模型"><a href="#运行模型" class="headerlink" title="运行模型"></a>运行模型</h3><p>现在我们看一下输入和输出张量的大小。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> rand_loader:</span><br><span class="line">    input = data.to(device)</span><br><span class="line">    output = model(input)</span><br><span class="line">    print(<span class="string">"Outside: input size"</span>, input.size(),</span><br><span class="line">          <span class="string">"output_size"</span>, output.size())</span><br></pre></td></tr></table></figure><p>输出：</p><pre><code>In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])        In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])        In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])        In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])        In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])        In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])        In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])        In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])</code></pre><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>如果你没有或一个GPU，当你批处理30个输入，模型会得到30个预期的输出。但是如果你有多GPU，然后你会得到像这样的结果。</p><h4 id="2-GPUs"><a href="#2-GPUs" class="headerlink" title="2 GPUs"></a>2 GPUs</h4><p>如果你有2个，你将看到：</p><pre><code>Let&apos;s use 2 GPUs!    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])    In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])    In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])</code></pre><h4 id="3GPUs"><a href="#3GPUs" class="headerlink" title="3GPUs"></a>3GPUs</h4><p>如果你有3个GPU，你将看到：</p><pre><code>Let&apos;s use 3 GPUs!    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])</code></pre><h4 id="8GPUs"><a href="#8GPUs" class="headerlink" title="8GPUs"></a>8GPUs</h4><p>如果你有8个，你将看到：</p><pre><code>Let&apos;s use 8 GPUs!    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])</code></pre><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>DataParallel会自动地分割你的数据，并将工作订单发送到多个gpu上的多个模型。在每个模型完成它们的工作后，DataParallel收集并合并结果，然后返回给你。</p><p>更多的信息可以查看<a href="https://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html." target="_blank" rel="noopener">https://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html.</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;译者博客：&lt;a href=&quot;https://zinw623.github.io/&quot;&gt;https://zinw623.github.io/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;什么是PYTORCH&quot;&gt;&lt;a href=&quot;#什么是PYTORCH&quot; class=&quot;headerlink&quot;
      
    
    </summary>
    
      <category term="翻译" scheme="https://zinw623.github.io/categories/%E7%BF%BB%E8%AF%91/"/>
    
      <category term="pytorch文档（1.2.0）" scheme="https://zinw623.github.io/categories/%E7%BF%BB%E8%AF%91/pytorch%E6%96%87%E6%A1%A3%EF%BC%881-2-0%EF%BC%89/"/>
    
    
      <category term="pytorch" scheme="https://zinw623.github.io/tags/pytorch/"/>
    
      <category term="翻译" scheme="https://zinw623.github.io/tags/%E7%BF%BB%E8%AF%91/"/>
    
  </entry>
  
  <entry>
    <title>【翻译】pytorch中文文档（1.2.0）- Package Reference/torch.autograd</title>
    <link href="https://zinw623.github.io/2019/08/28/pytorch-doc-torch-autograd/"/>
    <id>https://zinw623.github.io/2019/08/28/pytorch-doc-torch-autograd/</id>
    <published>2019-08-28T08:00:31.000Z</published>
    <updated>2019-08-29T02:43:38.985Z</updated>
    
    <content type="html"><![CDATA[<h2 id="AUTOMATIC-DIFFERENTIATION-PACKAGE-TORCH-AUTOGRAD"><a href="#AUTOMATIC-DIFFERENTIATION-PACKAGE-TORCH-AUTOGRAD" class="headerlink" title="AUTOMATIC DIFFERENTIATION PACKAGE - TORCH.AUTOGRAD"></a>AUTOMATIC DIFFERENTIATION PACKAGE - TORCH.AUTOGRAD</h2><p><code>torch.autograd</code>提供了许多实现任意标量值函数自动求导的类和函数。它只需要对已有的代码最小的改动 —— 你只需要声明<code>Tensor</code>的关键词<code>requires_grad=True</code>表明梯度需要被计算。</p><p>torch.autograd.backward(tensors, grad_tensors=None, retain_graph=None, create_graph=False, grad_variables=None) <a href="https://pytorch.org/docs/stable/_modules/torch/autograd.html#backward" target="_blank" rel="noopener">[源码]</a></p><p>计算所给张量关于图的叶子节点的梯度的总和。<br>图通过链式法则求导。如果<code>tensors</code>是非标量（也就是data是超过一个元素）且需要梯度，然后雅克比向量积（jacobian vector product）将会被计算，这种情况下需要函数另外指定<code>grad_tensors</code>。它应当是一个匹配长度的序列，包含了雅克比向量积里的向量，通常是需要求导的函数关于相应张量的梯度（<code>None</code>对于不需要求导的张量也是可接受的值。<br>这个函数会累积函数到叶子节点，在调用前你可能需要清零。</p><p>参数：</p><ul><li>tensor( tensor序列 ) ：将要被计算导数的tensors</li><li>grad_tensors( tensor序列或None ) ：雅克比向量积的向量，通常对相应张量的各元素求梯度。对于标量张量或无需求导的张量使用None值。如果一个None值对所有grad_tensors都是可接受的，那么这个参数是可选的。</li><li>retain_graph( _bool, 可选 ) ：如果为<code>False</code>，用于计算grad的图将被释放。请注意，几乎在所有情况下，都不需要将此选项设置为True，而且通常可以以更有效的方式解决此问题。默认值为create_graph的值。</li><li>create_graph(bool, 可选 ) ：如果为<code>True</code>，导数的图将会被构建，允许计算更高阶的导数。默认值为<code>False</code>。</li></ul><p>torch.autograd.grad(output,inputs, grad_outputs=None, retain_grad=None, create_graph=False,only_inputs=True,allow_unused=False) <a href="https://pytorch.org/docs/stable/_modules/torch/autograd.html#grad" target="_blank" rel="noopener">[源码]</a></p><p>计算和返回output关于inputs的梯度的和。<br><code>grad_outputs</code>应当是长度和<code>output</code>相同的序列，包含了雅克比向量积中的向量，通常需要预计算关于每个outputs的梯度。如果output不需要grad，那么梯度可以是<code>None</code>。<br>如果<code>only_inputs</code>是<code>True</code>，函数将只返回一个关于指定inputs的梯度的列表。如果是False，那么剩余叶子节点的梯度也将会计算，并且将会被累积到它们的<code>.grad</code>属性。</p><p>参数：</p><ul><li>output( tensor序列 ) ：被导函数的输出</li><li>inputs( tensor序列 ) ：关于的inputs，梯度会被返回，不会累积到<code>.grad</code>属性</li><li>grad_outputs( tensor序列 ) ：雅克比向量积的向量。通常是关于每个输入的梯度。标量张量或不需要梯度的可用None指定。如果None对所有grad_tensors可接受，则此参数可选。默认为None。</li><li>retain_graph( _bool, 可选 ) ：如果为<code>False</code>，用于计算grad的图将被释放。请注意，几乎在所有情况下，都不需要将此选项设置为True，而且通常可以以更有效的方式解决此问题。默认值为create_graph的值。</li><li>create_graph(bool, 可选) ：如果为<code>True</code>，导数的图将会被构建，允许计算更高阶的导数。默认值为<code>False</code>。</li><li>allow_unused( bool, 可选 ) ：如果为<code>False</code>，当计算输出出错时（因此他们的梯度永远是0）指明不使用的inputs。默认为<code>False</code></li></ul><h3 id="局部禁用梯度计算"><a href="#局部禁用梯度计算" class="headerlink" title="局部禁用梯度计算"></a>局部禁用梯度计算</h3><p>CLASS torch.autograd.no_grad  <a href="https://pytorch.org/docs/stable/_modules/torch/autograd/grad_mode.html#no_grad" target="_blank" rel="noopener">[源码]</a></p><p>禁用梯度计算的上下文管理器</p><p>当你确定不会调用 <code>Tensor.backward()</code> ，禁用梯度计算在推断时很有效。它将会减少计算带来的内存消耗。否则requires_grad = True</p><p>在这个模式下，即使输入是有requires_grad=True，也会是require_grad=False<br>的计算结果。</p><p>当使用<code>enable_grad</code>上下文管理器，这种模式将无效。</p><p>这种上下文管理器也可以是管理线程局部，它将会在其他线程失效。</p><p>也可以作为装饰器。</p><p>例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.tensor([<span class="number">1</span>], requires_grad = <span class="literal">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">···     y = x * <span class="number">2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y.requires_grad</span><br><span class="line"><span class="literal">False</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>@torch.no_grad()</span><br><span class="line">··· <span class="function"><span class="keyword">def</span> <span class="title">doubler</span><span class="params">(x)</span>:</span></span><br><span class="line">···     <span class="keyword">return</span> x * <span class="number">2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>z = doubler(x)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>z.requires_grad</span><br><span class="line"><span class="literal">False</span></span><br></pre></td></tr></table></figure><p>CLASS torch.autograd.enable_grad <a href="https://pytorch.org/docs/stable/_modules/torch/autograd/grad_mode.html#enable_grad" target="_blank" rel="noopener">[源码]</a></p><p>启动梯度计算的上下文管理器</p><p>如果使用<code>no_grad</code>或<code>set_grad_enabled</code>利用过的话，可以使用来启动梯度计算。</p><p>这种上下文管理器也可以是管理线程局部，它将会在其他线程失效。</p><p>也可以作为装饰器。</p><p>例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.tensor([<span class="number">1</span>], requires_grad = <span class="literal">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">···     <span class="keyword">with</span> torch.enable_grad():</span><br><span class="line">···         y = x * <span class="number">2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y.requires_grad</span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y.backward()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.grad</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>@torch.enable_grad()</span><br><span class="line">··· <span class="function"><span class="keyword">def</span> <span class="title">doubler</span><span class="params">(x)</span>:</span></span><br><span class="line">···     <span class="keyword">return</span> x * <span class="number">2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">with</span> torch.no_grad()</span><br><span class="line">···     z = doubler(x)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>z.requires_grad</span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure><p>CLASS torch.autograd.set_grad_enabled(mode)</p><p>设置是否进行梯度计算的上下文管理器。</p><p><code>set_grad_enabled</code>将启动或禁用梯度，通过<code>mode</code>来确定。它可以被用作上下文管理器也可以是函数。</p><p>当使用<code>enable_grad</code>上下文管理器，<code>set_grad_enable(False)</code>将无效。</p><p>上下文管理器也可以管理线程局部，它将会在其他线程无效。</p><p>参数：</p><ul><li>mode(bool)：表示决定了梯度计算启动（True）或禁用（False）。这可以条件性地控制梯度计算的使用。</li></ul><p>例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.tensor([<span class="number">1</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>is_train = <span class="literal">False</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">with</span> torch.set_grad_enabled(is_train):</span><br><span class="line">···     y = x * <span class="number">2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y.requires_grad</span><br><span class="line"><span class="literal">False</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.set_grad_enabled(<span class="literal">False</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = x * <span class="number">2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y.requires_grad</span><br><span class="line"><span class="literal">False</span></span><br></pre></td></tr></table></figure><h3 id="Tensors的In-place操作"><a href="#Tensors的In-place操作" class="headerlink" title="Tensors的In-place操作"></a>Tensors的In-place操作</h3><p>在autograd中使用in-place操作是困难的事，并且我们在大多数情况下不鼓励使用。Autograd的缓存区积极地释放和重用非常高效,很少场合in-place操作能明显地降低内存的使用。如果不是你的操作在很大的内存压力下，你可能永远不会使用它们。</p><h4 id="In-place正确性检查"><a href="#In-place正确性检查" class="headerlink" title="In-place正确性检查"></a>In-place正确性检查</h4><p>所有<code>Tensor</code>s记录应用于它们的in-place操作，并且如果执行过程检测张量为了反向传播而被保存在某个函数中，但是后来被in-place修改，一旦反向传播开始将会抛出错误。这确保如果你使用了in-place操作而没有看到任何错误，你会确定计算梯度是正确的。</p><h3 id="Variable-弃用"><a href="#Variable-弃用" class="headerlink" title="Variable(弃用)"></a>Variable(弃用)</h3><p>WARNING:</p><p>Variable API被弃用。Variables对张量自动求导没必要。自动求导把Tensors的<code>requires_grad</code>设为<code>True</code>会自动支持。</p><p>另外，现在构造用工厂方法像是<code>torch.randn()</code>,<code>torch.zeros</code>,<code>torch.ones()</code>来构造tensors（通过<code>requires_grad=True</code>）,以及类似下面的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">autograd_tensor = torch.randn((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>), requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h3 id="Tensor-autograd-的函数"><a href="#Tensor-autograd-的函数" class="headerlink" title="Tensor autograd 的函数"></a>Tensor autograd 的函数</h3><h4 id="CLASS-torch-Tensor"><a href="#CLASS-torch-Tensor" class="headerlink" title="CLASS torch.Tensor"></a>CLASS torch.Tensor</h4><p>backward(gradient=None, retain_graph=None, create_graph=False) <a href="https://pytorch.org/docs/stable/_modules/torch/tensor.html#Tensor.backward" target="_blank" rel="noopener">[源码]</a></p><p>计算关于叶子节点的当前张量的梯度。</p><p>图是通过链式法则求导。如果张量是非标量（也就是超过一个元素）并且需要梯度，函数需要另外指定<code>gradient</code>。它需要是匹配类型和位置的一个张量，这个张量包含了关于<code>self</code>的梯度。</p><p>这个函数会累积梯度到叶子节点 —— 你可能需要在调用它前清零梯度。</p><p>参数：</p><ul><li>gradient( Tensor或None )：关于这个tensor的梯度。如果它是一个张量，除非create_graph为真，它将自动转换成一个不需要grad的张量。对于标量张量或不需要grad的张量，可以指定为None值。如果一个None值是可接受的，那么这个参数是可选的。</li><li>retain_graph( _bool, 可选 ) ：如果为<code>False</code>，用于计算grad的图将被释放。请注意，几乎在所有情况下，都不需要将此选项设置为True，而且通常可以以更有效的方式解决此问题。默认值为create_graph的值。</li><li>create_graph(bool, 可选 ) ：如果为<code>True</code>，导数的图将会被构建，允许计算更高阶的导数。默认值为<code>False</code>。</li></ul><p>detach()</p><p>返回一个从当前图分离出的张量。</p><p>结果不需要梯度。</p><p>注意：<br>    返回的张量将和原来的张量共享内存。In-place对它们之一的修改将会被看到，而且可能触发正确性检查的错误。<strong>重要提示</strong>:从前，in-place对于size/stride/storage(像是resize_/resize_as_/set_/transpose_)会同时改变返回的向量和原始向量。现在，这种inplace不在会改变原始向量，并且相反会触发错误。并且对于稀疏张量也是如此。</p><p>detach_()</p><p>从已经创建过的图中分离张量，做成叶子节点。视图（views）不能分离。</p><p>grad</p><p>这个属性默认是<code>None</code>，当第一次调用<code>backward()</code>会计算<code>self.</code>的梯度变成张量。这个属性将包含被计算的梯度并且未来调用<code>backward()</code>将会累积到其中。</p><p>if_leaf</p><p>按照惯例所有requires_grad为False的张量都将是叶子节点(leaf)。</p><p>对于requires_grad为True的张量，如果它是被使用者创建的它将会也是叶子节点。这意思是它们不是某个操作的结果，所以<code>grad_fn</code>是None。</p><p>只有叶子节点的张量在调用backward()时填充它们的grad。为了填充非叶子节点的grad，你可以使用<code>retain_grad()</code>。</p><p>例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.rand(<span class="number">10</span>, requires_grad = <span class="literal">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.is_leaf</span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = torch.rand(<span class="number">10</span>, requires_grad = <span class="literal">True</span>).cuda()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b.is_leaf</span><br><span class="line"><span class="literal">False</span></span><br><span class="line"><span class="comment"># b是被从cpu移入cuda的操作创建的</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c = torch.rand(<span class="number">10</span>, requires_grad = <span class="literal">True</span>) + <span class="number">2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c.is_leaf</span><br><span class="line"><span class="literal">False</span></span><br><span class="line"><span class="comment"># c被加法操作创建</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>d = torch.rand(<span class="number">10</span>).cuda()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>d.is_leaf</span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="comment"># d不需要梯度所以没有操作创建它（被autograd引擎追踪）</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>e = torch.rand(<span class="number">10</span>).cuda().requires_grad()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>e.is_leaf</span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="comment"># e需要梯度并且没有操作创建它</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>f = torch.rand(<span class="number">10</span>, requires_grad=<span class="literal">True</span>, device = <span class="string">"cuda"</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>f.is_leaf</span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="comment"># f需要梯度，没有操作创建它</span></span><br></pre></td></tr></table></figure><p>register_hook(hook)  <a href="https://pytorch.org/docs/stable/_modules/torch/tensor.html#Tensor.register_hook" target="_blank" rel="noopener">[源码]</a></p><p>注册一个反向传播hook</p><p>这个hook在每次求关于相应张量的导数时被调用。这个hook应当有下面的签名：</p><pre><code>hook(grad) -&gt; Tensor or None</code></pre><p>这个hook不能修改它的变量，但是它能选择性地返回一个新的梯度，这个新的梯度可以用作代替<code>grad</code>。</p><p>这个函数返回了一个handle,通过方法<code>handle.remove()</code>来从模块中移出hook。</p><p>例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>v = torch.tensor([<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>], requires_grad = <span class="literal">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>h = v.register_hook(<span class="keyword">lambda</span> grad: grad * <span class="number">2</span>) <span class="comment"># 加倍梯度</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>v.backward(torch.tensor([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>]))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>v.grad</span><br><span class="line"></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">4</span></span><br><span class="line"><span class="number">6</span></span><br><span class="line">[torch.FloatTensor of size (<span class="number">3</span>, )]</span><br></pre></td></tr></table></figure><p>requires_grad</p><p>如果需要计算这个Tensor的梯度就设为<code>True</code>，否则为<code>False</code>。</p><p>注意：</p><pre><code>张量需要计算梯度这一事实并不意味着`grad`属性一定会被填充，详情请看`is_leaf`。</code></pre><p>retain_grad() <a href="https://pytorch.org/docs/stable/_modules/torch/tensor.html#Tensor.retain_grad" target="_blank" rel="noopener">[源码]</a></p><p>启动非叶子节点张量的.grad属性。</p><h3 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h3><h4 id="CLASS-torch-autograd-Function-源码"><a href="#CLASS-torch-autograd-Function-源码" class="headerlink" title="CLASS torch.autograd.Function [源码]"></a>CLASS torch.autograd.Function <a href="https://pytorch.org/docs/stable/_modules/torch/autograd/function.html#Function" target="_blank" rel="noopener">[源码]</a></h4><p>记录操作历史并且定义求导操作的公式。</p><p>每一次对<code>Tensor</code>进行计算，就创建一个新的函数对象，这个函数对象执行了计算并且记录了发生了什么。历史以函数的有向无环图（DAG)的形式保留下来，通过边缘来表示依赖关系（<code>input</code> &lt;- <code>output</code>）。然后，当backward被调用时，图按照拓扑排序执行，通过调用每个<code>Function</code>对象的<code>backward()</code>方法，把返回的梯度传递给下一个<code>Function</code>s。</p><p>一般，使用者与functions交互的唯一方法就是创建子类和定义新的操作。这个是扩展torch.autograd的推荐方法。</p><p>每个函数对象只能使用一次（在一次前向传播中）。</p><p>例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="class"><span class="keyword">class</span> <span class="title">Exp</span><span class="params">(Function)</span>:</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>    @staticmethod</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(ctx, i)</span>:</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>        result = i.exp()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>        ctx.save_for_backward(result)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>        <span class="keyword">return</span> result</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>    @staticmethod</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(ctx, grad_output)</span>:</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>        result = ctx.saved_tensors</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>        <span class="keyword">return</span> grad_output * result</span><br></pre></td></tr></table></figure><p>STATIC backward(ctx, *grad_outputs)  <a href="https://pytorch.org/docs/stable/_modules/torch/autograd/function.html#Function.backward" target="_blank" rel="noopener">[源码]</a></p><p>定义为求导计算定义公式</p><p>这个函数会被所有子类覆盖。</p><p>它必须接受一个上下文<code>ctx</code>作为第一个变量，跟着许多<code>forward()</code>的输出，并且它应当返回与<code>forward()</code>的输入相同的张量。每一个变量都是关于给定输出的梯度，每一个返回的值都应当是相应输入的梯度。</p><p>在前向传播时上下文被用作检索被保存的张量。它也有个属性<code>ctx.needs_input_grad</code>是个bool值的元组，表示每个输入是否需要梯度。例如，如果<code>forward()</code>的第一个输入需要计算关于输出的梯度，那么<code>backward()</code>将有ctx.needs_input_grad[0] = True。</p><p>STATIC forward(ctx, <em>args, *</em>kwargs)   <a href="https://pytorch.org/docs/stable/_modules/torch/autograd/function.html#Function.forward" target="_blank" rel="noopener">[源码]</a></p><p>执行操作。</p><p>这个函数会被所有子类覆盖。</p><p>它必须接受一个上下文ctx作为第一个变量，跟着任意数量的变量（tensors或其他类型）。</p><p>上下文被用作储存张量，这些张量将会在反向传播里被恢复。</p><h3 id="数值梯度检查"><a href="#数值梯度检查" class="headerlink" title="数值梯度检查"></a>数值梯度检查</h3><p>torch.autograd.gradcheck(func, inputs, eps=1e-06,atol=1e-05,rtol=0.001,raise_exception=True, check_sparse_nnz=False,nondet_tol=0.0)  <a href="https://pytorch.org/docs/stable/_modules/torch/autograd/gradcheck.html#gradcheck" target="_blank" rel="noopener">[源码]</a></p><p>通过分析关于张量的梯度的有限差分来检查梯度计算</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;AUTOMATIC-DIFFERENTIATION-PACKAGE-TORCH-AUTOGRAD&quot;&gt;&lt;a href=&quot;#AUTOMATIC-DIFFERENTIATION-PACKAGE-TORCH-AUTOGRAD&quot; class=&quot;headerlink&quot; tit
      
    
    </summary>
    
      <category term="翻译" scheme="https://zinw623.github.io/categories/%E7%BF%BB%E8%AF%91/"/>
    
      <category term="pytorch文档（1.2.0）" scheme="https://zinw623.github.io/categories/%E7%BF%BB%E8%AF%91/pytorch%E6%96%87%E6%A1%A3%EF%BC%881-2-0%EF%BC%89/"/>
    
    
      <category term="pytorch" scheme="https://zinw623.github.io/tags/pytorch/"/>
    
      <category term="翻译" scheme="https://zinw623.github.io/tags/%E7%BF%BB%E8%AF%91/"/>
    
  </entry>
  
  <entry>
    <title>【翻译】pytorch中文文档（1.2.0）- Notes部分</title>
    <link href="https://zinw623.github.io/2019/08/26/pytorch-doc-notes/"/>
    <id>https://zinw623.github.io/2019/08/26/pytorch-doc-notes/</id>
    <published>2019-08-26T10:30:45.000Z</published>
    <updated>2019-08-27T03:52:42.064Z</updated>
    
    <content type="html"><![CDATA[<h2 id="autograd机制"><a href="#autograd机制" class="headerlink" title="autograd机制"></a>autograd机制</h2><blockquote><p>对应的英文版文档<a href="https://pytorch.org/docs/stable/notes/autograd.html" target="_blank" rel="noopener">https://pytorch.org/docs/stable/notes/autograd.html</a></p></blockquote><p>这篇笔记将会展示自动求导是如何工作和如何记录操作的概述，没有绝对的必要去理解这些全部内容，但是我们推荐最好熟悉它，因为它会帮助你写出更有效率、更简洁的程序，并且在调试时会帮助到你。</p><h3 id="在反向传播（backwards）时排除子图"><a href="#在反向传播（backwards）时排除子图" class="headerlink" title="在反向传播（backwards）时排除子图"></a>在反向传播（backwards）时排除子图</h3><p>每一个张量都有一个标示：<code>requires_grad</code>，它使得在梯度计算时精细地排除子图并且变得更有效率。</p><h4 id="requires-grad"><a href="#requires-grad" class="headerlink" title="requires_grad"></a><code>requires_grad</code></h4><p>如果一个操作仅有一个输入且需要梯度，那么它的输出也需要梯度。相反，只有所有的输入都不需要梯度，输出才会不需要梯度。如果子图中所有的张量都不需要梯度，那么反向传播就不会再其中执行。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">5</span>, <span class="number">5</span>)   <span class="comment"># requires_grad = False by default</span></span><br><span class="line">y = torch.randn(<span class="number">5</span>, <span class="number">5</span>)   <span class="comment"># requires_grad = False by default</span></span><br><span class="line">z = torch.randn((<span class="number">5</span>, <span class="number">5</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">a = x + y</span><br><span class="line">a.requires_grad</span><br></pre></td></tr></table></figure><pre><code>False</code></pre><p>当你想要冻结你模型的一部分或者你提前知道你将不使用一些参数的梯度。例如，如果你想要微调一个预训练过的卷积神经网络，将要冻结的部分的requires_grad标示切换就足够了，并且知道计算到最后一层才会被保存到中间缓存区，其中仿射变换将使用需要梯度的权重，并且网络的输出也将会需要它们。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">model = torchvision.models.resnet18(pretrained = <span class="literal">True</span>)</span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">    param.requires_grad = <span class="literal">False</span></span><br><span class="line"><span class="comment"># Replace the last fully-connected layer</span></span><br><span class="line"><span class="comment"># Parameters of newly constructed modules have </span></span><br><span class="line"><span class="comment"># requires_grad = True by default</span></span><br><span class="line">model.fc = nn.Linear(<span class="number">512</span>, <span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Optimize only the classifier</span></span><br><span class="line">optimizer = optim.SGD(model.fc.parameters(), lr = <span class="number">1e-2</span>,</span><br><span class="line">                     momentum = <span class="number">0.9</span>)</span><br></pre></td></tr></table></figure><h3 id="autograd如何编码历史信息"><a href="#autograd如何编码历史信息" class="headerlink" title="autograd如何编码历史信息"></a>autograd如何编码历史信息</h3><p>Autograd是反向的自动求导系统。概念上，autograd记录了一张图，这张图记录了所有的操作，当你运行这些操作时它们会产生数据。得到的这张图是一个有向无环图，图的叶子节点是输入张量，根节点是输出张量。通过从根节点到叶子节点跟踪这张图，你可以自动地使用链式法则来计算梯度。</p><p>在内部，autograd将会将这张图表示为 <code>Function</code> 对象组成的图（真正的表达），函数可以通过 <code>apply()</code> 来求图的值。当计算前向传播时，autograd同时做到执行请求操作和建立用来表示计算梯度的函数的图（每一个 <code>torch.Tensor</code> 的 <code>.grad_fn</code> 属性是进入这张（用来计算梯度的）图的入口）。当前向传播完成后，我们求出这场图在反向传播时的值来计算梯度。</p><p>一件需要注意的重要的事是图在每次迭代时会重新建立图，并且这允许使用任意的python控制语句，即使这些语句每次迭代都会改变图的整个形状和大小。你不必在启动训练之前编写出所有可能的路径-what you run is what you differentiate（你运行什么就会对什么求导）。</p><h3 id="autograd的In-place操作"><a href="#autograd的In-place操作" class="headerlink" title="autograd的In-place操作"></a>autograd的In-place操作</h3><p>在autograd中使用in-place操作是困难的事，并且我们在大多数情况下不鼓励使用。Autograd的缓存区积极地释放和重用非常高效,很少场合in-place操作能明显地降低内存的使用。如果不是你的操作在很大的内存压力下，你可能永远不会使用它们。</p><p>对于限制in-place操作的适用范围有两个主要的原因：</p><ol><li>in-place操作能潜在地覆盖梯度计算所需要的值。</li><li>每一个in-place操作确实需要实施重写计算图。out-of-place版本仅是分配新的对象并且保持对旧图的引用。而in-place操作需要把所有输入的creator改为代表这些操作的<code>Function</code>。这会比较棘手，特别是有很多Tensors共享相同的内存（storage）（例如通过索引或转置创建），并且如果被修改的输入的储存（storage）被其他的任何的Tensor引用，那么in-place会抛出错误。</li></ol><h3 id="In-place正确性检查"><a href="#In-place正确性检查" class="headerlink" title="In-place正确性检查"></a>In-place正确性检查</h3><p>每一个tensor都保留一个版本记数器（version counter），当张量在任何操作中被使用后，它每次都会递增。当<code>Function</code>为反向传播保存任何张量时，这些保留的张量的版本计数器也会被保存。一旦你是用<code>self.saved_tensors</code>它将会被检查，并且如果它大于被保存的值将会抛出错误。这确保了如果你是用in-place操作并且没有看到任何操作，你就能确定被计算出的梯度是正确的。</p><h2 id="广播语义"><a href="#广播语义" class="headerlink" title="广播语义"></a>广播语义</h2><blockquote><p>对应的英文版文档：<a href="https://pytorch.org/docs/stable/notes/broadcasting.html" target="_blank" rel="noopener">https://pytorch.org/docs/stable/notes/broadcasting.html</a></p></blockquote><p>许多pytorch的操作支持<code>NumPy广播语义</code>。</p><p>简而言之，如果pytorch操作支持广播，那么它的张量参数会被自动地扩展成相等的大小（无需复制数据）</p><h3 id="一般语义"><a href="#一般语义" class="headerlink" title="一般语义"></a>一般语义</h3><p>如果满足以下规则，那么两个张量是可广播的：</p><ul><li>每个张量至少有一个维度。</li><li>当迭代维度的大小时，从末尾（trailing）的维度开始，维度大小必须相等，或者它们中的一个的维度大小为1，或者它们中的一个的维度不存在。<br>例如：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">x = torch.empty(<span class="number">5</span>, <span class="number">7</span>, <span class="number">3</span>)</span><br><span class="line">y = torch.empty(<span class="number">5</span>, <span class="number">7</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># 相同形状的永远是可广播的（上面的规则永远成立）</span></span><br><span class="line"></span><br><span class="line">x = torch.empty((<span class="number">0</span>,))</span><br><span class="line">y = torch.empty(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"><span class="comment"># x和y不是可广播的，因为x没有至少一个维度</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 可以排列出的维度</span></span><br><span class="line">x = torch.empty(<span class="number">5</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line">y = torch.empty(   <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"><span class="comment"># x和y是可广播的</span></span><br><span class="line"><span class="comment"># 第一末尾的维度：都是1。</span></span><br><span class="line"><span class="comment"># 第二靠近尾部的维度：y是1</span></span><br><span class="line"><span class="comment"># 第三靠近尾部的维度：x的大小等于y的大小</span></span><br><span class="line"><span class="comment"># 第四靠近尾部的维度：y的该维度不存在</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 但是：</span></span><br><span class="line">x = torch.empty(<span class="number">5</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">1</span>)</span><br><span class="line">y = torch.empty(  <span class="number">3</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line"><span class="comment"># x和y是不可广播的</span></span><br><span class="line"><span class="comment"># 因为第三靠近尾部的维度x的2不等于y的3</span></span><br></pre></td></tr></table></figure><p>如果两个张量x，y是“可广播的”，那么结果张量的大小是按照下面的方法计算的：</p><ul><li>如果x和y的维度的长度不相等，就在维度个数更少的张量的维度前面加1，使两个张量的维度相等。</li><li>然后，对于每个维度的大小，最后得出的结果的维度大小是x和y中维度大小最大的那一个的值。</li></ul><p>例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可以列出各维度来使阅读更容易</span></span><br><span class="line">x = torch.empty(<span class="number">5</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line">y = torch.empty(   <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">(x + y).size()</span><br></pre></td></tr></table></figure><pre><code>torch.Size([5, 3, 4, 1])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 但是没有必要:</span></span><br><span class="line">x = torch.empty(<span class="number">1</span>)</span><br><span class="line">y = torch.empty(<span class="number">3</span>, <span class="number">1</span>, <span class="number">7</span>)</span><br><span class="line">(x + y).size()</span><br></pre></td></tr></table></figure><pre><code>torch.Size([3, 1, 7])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.empty(<span class="number">5</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line">y = torch.empty(<span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">(x + y).size()</span><br></pre></td></tr></table></figure><pre><code>---------------------------------------------------------------------------RuntimeError                              Traceback (most recent call last)&lt;ipython-input-9-d19949393c3d&gt; in &lt;module&gt;      1 x = torch.empty(5, 2, 4, 1)      2 y = torch.empty(3, 1, 1)----&gt; 3 (x + y).size()RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1</code></pre><h3 id="In-place语义"><a href="#In-place语义" class="headerlink" title="In-place语义"></a>In-place语义</h3><p>一个复杂的问题是in-place操作不允许in-place张量由于广播而改变形状。</p><p>例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.empty(<span class="number">5</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line">y = torch.empty(<span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">(x.add_(y)).size()</span><br></pre></td></tr></table></figure><pre><code>torch.Size([5, 3, 4, 1])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 但是：</span></span><br><span class="line">x = torch.empty(<span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">y = torch.empty(<span class="number">3</span>, <span class="number">1</span>, <span class="number">7</span>)</span><br><span class="line">(x.add_(y)).size()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 一般广播会将x的size改成(3, 3, 7)</span></span><br><span class="line"><span class="comment"># 但是x为in-place张量</span></span><br></pre></td></tr></table></figure><pre><code>---------------------------------------------------------------------------RuntimeError                              Traceback (most recent call last)&lt;ipython-input-16-300828b970da&gt; in &lt;module&gt;      2 x = torch.empty(1, 3, 1)      3 y = torch.empty(3, 1, 7)----&gt; 4 (x.add_(y)).size()      5       6 # 一般广播会将x的size改成(3, 3, 7)RuntimeError: output with shape [1, 3, 1] doesn&apos;t match the broadcast shape [3, 3, 7]</code></pre><h3 id="反向传播兼容性"><a href="#反向传播兼容性" class="headerlink" title="反向传播兼容性"></a>反向传播兼容性</h3><p>pytorch的较早版本允许在不同形状的张量上执行逐点函数（pointwise functions），只要这些张量的元素数量相等。然后逐点函数将把各张量看作一维的张量然后执行。pytorch现在支持广播而“一维”逐点计算已经被弃用了，并且在张量不可广播但元素数目一样的情况下将生成Python警告。</p><p>注意，如果两个张量形状不同，但可广播且元素数目相同，则引入广播会导致反向传播不兼容的变化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.add(torch.ones(<span class="number">4</span>, <span class="number">1</span>), torch.randn(<span class="number">4</span>))</span><br></pre></td></tr></table></figure><pre><code>tensor([[2.8629, 0.4929, 0.8330, 0.1047],        [2.8629, 0.4929, 0.8330, 0.1047],        [2.8629, 0.4929, 0.8330, 0.1047],        [2.8629, 0.4929, 0.8330, 0.1047]])</code></pre><p>这个例子之前生成size为[4,1]的张量，但是现在生成了一个size为[4,4]的张量。为了帮助识别代码里出现由于广播而导致的反向传播不兼容的情况，可以将<code>torch.utils.backcompat.broadcast_warning</code>设为True，在这种情况下将生成Python警告</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.utils.backcompat.broadcast_warning.enable = <span class="literal">True</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.add(torch.ones(<span class="number">4</span>, <span class="number">1</span>), torch.ones(<span class="number">4</span>))</span><br></pre></td></tr></table></figure><pre><code>tensor([[2., 2., 2., 2.],        [2., 2., 2., 2.],        [2., 2., 2., 2.],        [2., 2., 2., 2.]])</code></pre><p>译者注：</p><p>文档里说运行会出现下面这个warning，但是实际运行没用出现，咱也不知道为啥，咱也不知道问谁。</p><p>__main__:1: UserWarning: self and other do not have the same shape, but are broadcastable, and have the same number of elements.<br>Changing behavior in a backwards incompatible manner to broadcasting rather than viewing as 1-dimensional.</p><p>未完待译。。。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;autograd机制&quot;&gt;&lt;a href=&quot;#autograd机制&quot; class=&quot;headerlink&quot; title=&quot;autograd机制&quot;&gt;&lt;/a&gt;autograd机制&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;对应的英文版文档&lt;a href=&quot;https://
      
    
    </summary>
    
      <category term="翻译" scheme="https://zinw623.github.io/categories/%E7%BF%BB%E8%AF%91/"/>
    
      <category term="pytorch文档（1.2.0）" scheme="https://zinw623.github.io/categories/%E7%BF%BB%E8%AF%91/pytorch%E6%96%87%E6%A1%A3%EF%BC%881-2-0%EF%BC%89/"/>
    
    
      <category term="pytorch" scheme="https://zinw623.github.io/tags/pytorch/"/>
    
      <category term="翻译" scheme="https://zinw623.github.io/tags/%E7%BF%BB%E8%AF%91/"/>
    
  </entry>
  
  <entry>
    <title>【翻译】pytorch中文文档（1.2.0）- Package Reference/torch</title>
    <link href="https://zinw623.github.io/2019/08/26/pytorch-doc-torch/"/>
    <id>https://zinw623.github.io/2019/08/26/pytorch-doc-torch/</id>
    <published>2019-08-26T09:07:08.000Z</published>
    <updated>2019-08-27T03:52:56.177Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TORCH"><a href="#TORCH" class="headerlink" title="TORCH"></a>TORCH</h2><p>torch package包含多维张量和定义好的数学运算的数据结构。另外，它提供了许多实用程序用于有效的序列化张量和任意类型，以及其他有用的实用程序。</p><p>它支持CUDA环境，使你能在NVIDIA GPU上进行你的张量计算，这要求你compute capability&gt;=3.0。</p><h3 id="Tensors"><a href="#Tensors" class="headerlink" title="Tensors"></a>Tensors</h3><ul><li><p><em>torch.is_tensor(obj)</em> <a href https: pytorch.org docs stable _modules torch.html#is_tensor"">[源码]</a></p><p>  如果是一个PyTorch tensor返回True</p><ul><li><p><em>Parameters</em></p><p>  <strong>obj</strong>(<em>Object</em>)-要测试的对象</p></li></ul></li><li><p><em>torch.is_storage(obj)</em> <a href="https://pytorch.org/docs/stable/_modules/torch.html#is_storage" target="_blank" rel="noopener">[源码]</a></p><p>  如果obj是pytorch storage对象就返回True</p><ul><li><p><em>Parameters</em></p><p>  <strong>obj</strong>( <em>Object</em> ) - 要测试的对象</p></li></ul></li><li><p><em>torch.is_floating_point(input)</em> ——&gt; (<em>bool</em>)</p><p>  如果<code>input</code>的数据类型是浮点型数据（即<code>torch.float64</code>, <code>torch.float32</code>和<code>torch.float16</code>中的一种），就返回True。</p><ul><li><p>_Parameters</p><p>  <strong>input</strong>(<em>Tensor</em>) —— 要测试的pytorch张量</p></li></ul></li><li><p><em>torch.set_default_dtype(d)</em> <a href="https://pytorch.org/docs/stable/_modules/torch.html#set_default_dtype" target="_blank" rel="noopener">[源码]</a></p><p>  把默认的浮点数类型的dtype设为<code>d</code>。这种类型将被用做<code>torch.tensor()</code>类型推断的默认浮点类型。这个默认的浮点dtype是初始为torch.float32</p><ul><li><p><em>Parameters</em></p><p>  <strong>d</strong>(<code>torch.dtype</code>) —— 将被设为默认的浮点型dtype</p><p>例如:</p></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor([<span class="number">1.2</span>, <span class="number">3</span>]).dtype     <span class="comment"># 初始默认的浮点型是torch.float32</span></span><br><span class="line">torch.float32</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.set_default_dtype(torch.float64)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor([<span class="number">1.2</span>, <span class="number">3</span>]).dtype     <span class="comment"># 新的浮点型tensor</span></span><br><span class="line">torch.float64</span><br></pre></td></tr></table></figure><ul><li><p>torch.get_default_dtype() ——&gt; torch.dtype</p><p>  获得当前默认浮点型<code>torch.dtype</code>。</p><p>  例如：</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.get_default_dtype()  <span class="comment"># 初始默认的浮点型为torch.float32</span></span><br><span class="line">torch.float32</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.set_default_dtype(torch.float64)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.get_default_dtype()  <span class="comment"># 默认改为torch.float64</span></span><br><span class="line">torch.float64</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.set_default_tensor_type(torch.FloatTensor) </span><br><span class="line"><span class="comment"># 设置tensor type也会影响这个</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.get_default_dtype()  <span class="comment"># 改为了对应于torch.FloatTensor对应的dtype：torch.float32</span></span><br><span class="line">torch.float32</span><br></pre></td></tr></table></figure><ul><li><p>torch.set_default_tensor_type( <em>t</em> ) <a href="https://pytorch.org/docs/stable/_modules/torch.html#set_default_tensor_type" target="_blank" rel="noopener">[源码]</a></p><p>  设置默认的<code>torch.Tensor</code>类型为浮点型tensor类型<code>t</code>。这种类型也将会被勇作为torch.tensor()类型推断的默认浮点型类型。</p><p>  默认的浮点型tensor type是初始化为<code>torch.FloatTensor</code>。</p><ul><li><p><em>Parameters</em><br> <strong>t</strong>( <em>type or string</em> ) —— 浮点型tensor type或者它的名字</p><p>例如：</p></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor([<span class="number">1.2</span>, <span class="number">3</span>]).dtype     <span class="comment"># 初始的默认浮点型是torch.float32</span></span><br><span class="line">torch.float32</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.set_default_tensor_type(torch.DoubleTensor)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor([<span class="number">1.2</span>, <span class="number">3</span>]).dtype     <span class="comment"># 新的浮点型tensor</span></span><br><span class="line">torch.float64</span><br></pre></td></tr></table></figure><ul><li><p>torch.numel( <em>input</em> ) ——&gt; int</p><p>  返回<code>input</code>张量里元素的数量</p><ul><li><p><em>Parameters</em></p><p>  <strong>input</strong>( <em>Tensor</em> ) —— 输入的向量</p><p>例如：</p></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.randn(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.numel(a)</span><br><span class="line"><span class="number">120</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.zeros(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.numel(a)</span><br><span class="line"><span class="number">16</span></span><br></pre></td></tr></table></figure><ul><li><p>torch.set_printoptions( <em>precision=None, threshold=None, edgeitems=None, linewidth=None, profile=None, sci_mode=None</em> ) <a href="https://pytorch.org/docs/stable/_modules/torch/_tensor_str.html#set_printoptions" target="_blank" rel="noopener">[源码]</a><br>  为print设置选项。从NumPy中无耻地拿出的项目</p><ul><li><p><em>Parameters</em></p><ul><li><strong>precision</strong> —— 输出浮点型精度的位数（默认为4）</li><li><strong>threshold</strong> —— 输出时的阈值，当数组总和超过阈值，会被截断输出（默认为1000）</li><li><strong>edgeitems</strong> —— 每个维度统计的数组条目的数量（默认为3）</li><li><strong>linewidth</strong> —— 为了插入换行符每行设置的字符数（默认为80）。Thresholded矩阵将会忽略这个参数</li><li><strong>profile</strong> —— Sane为了好的打印的默认设置，可以用以上任何选项覆盖掉（可以是 <em>default,short,full</em> 中任意一个）</li><li><strong>sci_mode</strong> —— 是（True）否（False）使用科学符号。如果指定是None（默认），那么值将会被_Formatter定义。</li></ul></li></ul></li></ul><ul><li><p>torch.set_flush_denormal( <em>mode</em> ) ——&gt; bool</p><p>  禁用CPU上的非规格化的浮点数</p><p>  如果你的系统支持非规格化数字（flushing denormal numbers）并且成功配置非规格化模式（flush denormal mode）将会返回True。<code>set_flush_denormal()</code>仅使用在支持SSE3的x86架构。</p><ul><li><p><em>Parameters</em></p><p>  <strong>mode</strong>( <em>boor</em> ) —— 控制是否使用非规格化模式（flush denormal mode）</p><p>例如：</p></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.set_flush_denormal(<span class="literal">True</span>)</span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor([<span class="number">1e-323</span>], dtype = torch.float32)</span><br><span class="line">tensor([<span class="number">0</span>,], dtype=torch.float64)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.set_flush_denormal(<span class="literal">False</span>)</span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor([<span class="number">1e-323</span>], dtype=torch.float64)</span><br><span class="line">tensor(<span class="number">9.88131e-324</span> *</span><br><span class="line">        [ <span class="number">1.0000</span>], dtype=torch.float64)</span><br></pre></td></tr></table></figure><h4 id="Creation-Ops"><a href="#Creation-Ops" class="headerlink" title="Creation Ops"></a>Creation Ops</h4><ul><li><p>NOTE</p><p>  随机抽样创建操作被列在<strong>Random sampling</strong>之下，包括：<code>torch.rand()</code> <code>torch.rand_like()</code> <code>torch.randn_like()</code> <code>torch.randint()</code> <code>torch.randint_like()</code> <code>torch.randperm()</code> 你可能也使用<code>torch.empty()</code>并使用In-place random sampling方法来更广泛的分布范围中取样的值来创建<code>torch.Tensor</code>s</p></li></ul><ul><li><p>torch.tensor(data, dtype=None, device=None, requires_grad=False, pin_memory=False) ——&gt; Tensor</p><p>  通过<code>data</code>构造一个张量</p><ul><li><p>WARNING</p><p>  <code>torch.tensor()</code>总是拷贝<code>data</code>。如果你有一个Tensor<code>data</code>而且想避免拷贝，请使用<code>torch.Tensor.requires_grad_()</code>或<code>torch.Tensor.detach()</code>。如果你有一个NumPy<code>ndarray</code>并且向避免拷贝，请使用<code>torch.as_tensor()</code>。</p></li><li><p>WARNING</p><p>  当data是一个tensor x，<code>torch.tensor()</code>会读取出“the data”，无论之前传入过什么，并且构建一个leaf variable（）。因此<code>torch.tensor(x)</code>等价于<code>x.clone().detach()</code>，<code>torch.tensor(x, requires_grad = True)</code>等价于<code>x.clone().detach().requires_grad_(True)</code>。对于等价的操作，推荐使用<code>clone()</code>和<code>detach()</code>。</p></li><li><p><em>Parameters</em></p><ul><li><strong>data</strong>( <em>array_like</em> ) —— 为tensor初始化data。可以是list，tuple，Numpy <code>ndarray</code>，scalar，和其他类型。</li><li><strong>dtype</strong>(<code>torch.dtype</code>, optional) —— 返回tensor期望的数据类型。默认：如果为None，从<code>data</code>中推断数据类型</li><li><strong>device</strong>(<code>torch.device</code>, optional) —— 返回tensor期望使用的硬件。默认：如果是None，对当前张量类型使用当前硬件（参考<code>torch.set_default_tensor_type()</code>)。<code>device</code>可以是提供CPU张量类型的CPU和支持CUDA张量类型的CUDA设备。</li><li><strong>requires_grad</strong>( <em>bool</em>, optional) —— 如果自动求导应当记录返回张量的操作。默认：<code>False</code></li><li><strong>pin_memory</strong>( <em>bool</em>, optional) —— 如果设置，返回的向量分配到锁页内存（pinned memory)。这仅对CPU张量有效。默认：<code>False</code>。<br>例如：</li></ul></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor([[<span class="number">0.1</span>, <span class="number">1.2</span>], [<span class="number">2.2</span>, <span class="number">3.1</span>], [<span class="number">4.9</span>, <span class="number">5.2</span>]])</span><br><span class="line">tensor([[ <span class="number">0.1000</span>, <span class="number">1.2000</span>],</span><br><span class="line">        [ <span class="number">2.2000</span>, <span class="number">3.1000</span>],</span><br><span class="line">        [ <span class="number">4.9000</span>, <span class="number">5.2000</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor([<span class="number">0</span>, <span class="number">1</span>])  <span class="comment"># 基于data判断类型</span></span><br><span class="line">tensor([ <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor([[<span class="number">0.11111</span>, <span class="number">0.222222</span>, <span class="number">0.3333333</span>]],</span><br><span class="line">                    dtype=torch.float64,</span><br><span class="line">                    device=torch.device(<span class="string">'cuda:0'</span>))  <span class="comment"># 创建一个torch.cuda.DoubleTensor</span></span><br><span class="line">tensor([[ <span class="number">0.1111</span>, <span class="number">0.2222</span>, <span class="number">0.3333</span>]], dtype=torch.float64, device=<span class="string">'cuda:0'</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor(<span class="number">3.14159</span>)  <span class="comment"># 创建一个标量</span></span><br><span class="line">tensor(<span class="number">3.1416</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor([])  <span class="comment"># 创建一个空张量（size为(0,))</span></span><br><span class="line">tensor([])</span><br></pre></td></tr></table></figure><h4 id="Indexing-Slicing-Joining-Mutating-Ops"><a href="#Indexing-Slicing-Joining-Mutating-Ops" class="headerlink" title="Indexing, Slicing, Joining, Mutating Ops"></a>Indexing, Slicing, Joining, Mutating Ops</h4><h3 id="Generators"><a href="#Generators" class="headerlink" title="Generators"></a>Generators</h3><h3 id="Random-sampling"><a href="#Random-sampling" class="headerlink" title="Random sampling"></a>Random sampling</h3><h4 id="In-place-random-sampling"><a href="#In-place-random-sampling" class="headerlink" title="In-place random sampling"></a>In-place random sampling</h4><h4 id="Quasi-random-sampling"><a href="#Quasi-random-sampling" class="headerlink" title="Quasi-random sampling"></a>Quasi-random sampling</h4><h3 id="Serialization"><a href="#Serialization" class="headerlink" title="Serialization"></a>Serialization</h3><h3 id="Parallelism"><a href="#Parallelism" class="headerlink" title="Parallelism"></a>Parallelism</h3><h3 id="Locally-disabling-gradient-computation"><a href="#Locally-disabling-gradient-computation" class="headerlink" title="Locally disabling gradient computation"></a>Locally disabling gradient computation</h3><h3 id="Math-operations"><a href="#Math-operations" class="headerlink" title="Math operations"></a>Math operations</h3><h4 id="Pointwise-Ops"><a href="#Pointwise-Ops" class="headerlink" title="Pointwise Ops"></a>Pointwise Ops</h4><h4 id="Reduction-Ops"><a href="#Reduction-Ops" class="headerlink" title="Reduction Ops"></a>Reduction Ops</h4><h4 id="Comparision-Ops"><a href="#Comparision-Ops" class="headerlink" title="Comparision Ops"></a>Comparision Ops</h4><h4 id="Spectral-Ops"><a href="#Spectral-Ops" class="headerlink" title="Spectral Ops"></a>Spectral Ops</h4><h4 id="Other-Operations"><a href="#Other-Operations" class="headerlink" title="Other Operations"></a>Other Operations</h4><h4 id="BLAS-and-LAPACK-Operations"><a href="#BLAS-and-LAPACK-Operations" class="headerlink" title="BLAS and LAPACK Operations"></a>BLAS and LAPACK Operations</h4><h3 id="Utilities"><a href="#Utilities" class="headerlink" title="Utilities"></a>Utilities</h3>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;TORCH&quot;&gt;&lt;a href=&quot;#TORCH&quot; class=&quot;headerlink&quot; title=&quot;TORCH&quot;&gt;&lt;/a&gt;TORCH&lt;/h2&gt;&lt;p&gt;torch package包含多维张量和定义好的数学运算的数据结构。另外，它提供了许多实用程序用于有效的序列化张量和
      
    
    </summary>
    
      <category term="翻译" scheme="https://zinw623.github.io/categories/%E7%BF%BB%E8%AF%91/"/>
    
      <category term="pytorch文档（1.2.0）" scheme="https://zinw623.github.io/categories/%E7%BF%BB%E8%AF%91/pytorch%E6%96%87%E6%A1%A3%EF%BC%881-2-0%EF%BC%89/"/>
    
    
      <category term="pytorch" scheme="https://zinw623.github.io/tags/pytorch/"/>
    
      <category term="翻译" scheme="https://zinw623.github.io/tags/%E7%BF%BB%E8%AF%91/"/>
    
  </entry>
  
  <entry>
    <title>【notes】pytorch学习笔记4-pytorch常用工具</title>
    <link href="https://zinw623.github.io/2019/08/23/pytorch-tools/"/>
    <id>https://zinw623.github.io/2019/08/23/pytorch-tools/</id>
    <published>2019-08-23T10:30:45.000Z</published>
    <updated>2019-08-23T11:37:34.021Z</updated>
    
    <content type="html"><![CDATA[<h3 id="pytorch中常见的工具"><a href="#pytorch中常见的工具" class="headerlink" title="pytorch中常见的工具"></a>pytorch中常见的工具</h3><p>在训练神经网络的过程中需要用到很多工具、其中最重要的三部分是数据、可视化和GPU加速。本章主要介绍pytorch在这几方面常用的工具，合理使用这些工具能极大地提高编码效率。</p><h4 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h4><p>在解决深度学习问题的过程中，往往需要花费大量的精力去处理数据，包括图像、文本、语音或其他二进制数据等。数据的处理对训练神经网络来说十分重要，良好的数据处理不仅会加速模型训练，也会提高模型效果。</p><h5 id="数据加载"><a href="#数据加载" class="headerlink" title="数据加载"></a>数据加载</h5><p>在pytorch中，数据加载可以通过自定义的数据集对象实现。数据集对象被抽象成Dataset类，实现自定义的数据集需要继承Dataset，并实现两个Python魔法方法。</p><ul><li>__getitem__：返回一条数据或一个样本。obj[index]等价于obj.__getitem__(index)。</li><li>__len__：返回样本的数量。len(obj)等价于obj.__len__()。</li></ul><p>这里我们用Kaggle经典挑战赛“Dogs vs.Cat”</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">%env LS_COLORS = <span class="literal">None</span></span><br><span class="line">!tree --charset ascii data/dogcat/</span><br></pre></td></tr></table></figure><pre><code>env: LS_COLORS=Nonedata/dogcat/|-- cat.0.jpg|-- cat.1.jpg|-- cat.2.jpg|-- cat.3.jpg|-- cat.4.jpg|-- cat.5.jpg|-- cat.6.jpg|-- dog.0.jpg|-- dog.1.jpg|-- dog.10.jpg|-- dog.1000.jpg`-- dog.10000.jpg0 directories, 12 files</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DogCat</span><span class="params">(data.Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, root)</span>:</span></span><br><span class="line">        imgs = os.listdir(root)</span><br><span class="line">        <span class="comment"># 所有图片的绝对路径</span></span><br><span class="line">        self.imgs = [os.path.join(root, img) <span class="keyword">for</span> img <span class="keyword">in</span> imgs]</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        img_path = self.imgs[index]</span><br><span class="line">        <span class="comment"># dog-&gt;1 cat-&gt;0</span></span><br><span class="line">        label = <span class="number">1</span> <span class="keyword">if</span> <span class="string">'dog'</span> <span class="keyword">in</span> img_path.split(<span class="string">'/'</span>)[<span class="number">-1</span>] <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">        pil_img = Image.open(img_path)</span><br><span class="line">        array = np.asarray(pil_img)</span><br><span class="line">        data = t.from_numpy(array)</span><br><span class="line">        <span class="keyword">return</span> data, label</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.imgs)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dataset = DogCat(<span class="string">'./data/dogcat/'</span>)</span><br><span class="line">img, label = dataset[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> img, label <span class="keyword">in</span> dataset:</span><br><span class="line">    print(img.size(), img.float().mean(), label)</span><br></pre></td></tr></table></figure><pre><code>torch.Size([375, 499, 3]) tensor(116.7904) 1torch.Size([499, 327, 3]) tensor(133.5602) 1torch.Size([144, 175, 3]) tensor(166.6151) 0torch.Size([292, 269, 3]) tensor(157.4856) 1torch.Size([375, 499, 3]) tensor(96.8243) 0torch.Size([375, 499, 3]) tensor(120.7302) 1torch.Size([280, 300, 3]) tensor(71.6653) 0torch.Size([396, 312, 3]) tensor(131.8400) 0torch.Size([303, 400, 3]) tensor(129.1319) 0torch.Size([374, 500, 3]) tensor(119.7826) 0torch.Size([412, 263, 3]) tensor(152.9542) 1torch.Size([414, 500, 3]) tensor(156.6921) 0</code></pre><p>通过上面的代码，我们学习了如何自定义自己的数据集，并可以依次获取。但这里返回的数据不适用实际使用，因其具有如下两方面问题：</p><ul><li>返回样本的形状之一，每张图片的大小不一样，这对于需要去batch训练的神经网络来说很不友好。</li><li>返回样本的数值较大，为归一化至[-1, 1]<br>针对上述问题，pytorch提供了torchvision。它是一个视觉工具包，提供了很多视觉图像处理的工具，其中transforms模块提供了对PIL Image对象和Tensor对象的常用操作。</li></ul><p>针对上述问题，pytorch提供<a href="https://github.com/pytorch/vision/" target="_blank" rel="noopener">torchvision</a>。它是一个视觉工具包，提供了很多视觉图像处理的工具，其中transforms模块提供了对PIL Image对象和Tensor对象的常用操作。<br>对PIL Image的常见操作如下。</p><ul><li>Resize：调整图片尺寸</li><li>CenterCrop、RandomCrop、RandomSizedCrop：裁剪图片。</li><li>Pad：填充。</li><li>ToTensor：将PIL Image对象转成Tensor，会自动将[0, 225]归一化至[0, 1]。</li></ul><p>对Tensor的常见操作如下。</p><ul><li>Normalize：标准化，即减均值，除以标准差。</li><li>ToPILImage：将Tensor转为PIL Image对象。</li></ul><p>如果要对图片进行多个操作，可通过Compose将这些操作拼接起来，类似于nn.Sequential。注意，这些操作定义后是以对象的形式存在，真正使用时需要调用它的__call__方法，类似于nn.Module。下面利用这些操作来优化上面的dataset。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms <span class="keyword">as</span> T</span><br><span class="line"></span><br><span class="line">transform = T.Compose([</span><br><span class="line">    T.Resize(<span class="number">224</span>), <span class="comment"># 缩放图片（Image），保持长宽比不变，最短边为224像素</span></span><br><span class="line">    T.CenterCrop(<span class="number">224</span>), <span class="comment"># 从中间取出224 x224的图片</span></span><br><span class="line">    T.ToTensor(),   <span class="comment"># 将图片转换成Tensor并归一化至[0, 1]</span></span><br><span class="line">    T.Normalize(mean = [<span class="number">.5</span>, <span class="number">.5</span>, <span class="number">.5</span>], std = [<span class="number">.5</span>, <span class="number">.5</span> , <span class="number">.5</span>]) <span class="comment"># 标准化至[-1, 1]</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DogCat</span><span class="params">(data.Dataset)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, root, transforms = None)</span>:</span></span><br><span class="line">        imgs = os.listdir(root)</span><br><span class="line">        self.imgs = [os.path.join(root, img) <span class="keyword">for</span> img <span class="keyword">in</span> imgs]</span><br><span class="line">        self.transforms = transforms</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        </span><br><span class="line">        img_path = self.imgs[index]</span><br><span class="line">        label = <span class="number">0</span> <span class="keyword">if</span> <span class="string">'dog'</span> <span class="keyword">in</span> img_path.split(<span class="string">'/'</span>)[<span class="number">-1</span>] <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line">        data = Image.open(img_path)</span><br><span class="line">        <span class="keyword">if</span> self.transforms:</span><br><span class="line">            data = self.transforms(data)</span><br><span class="line">        <span class="keyword">return</span> data, label</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.imgs)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision.transforms <span class="keyword">import</span> ToPILImage</span><br><span class="line">dataset = DogCat(<span class="string">'./data/dogcat/'</span>, transforms = transform)</span><br><span class="line">img, label = dataset[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> img, label <span class="keyword">in</span> dataset:</span><br><span class="line">    print(img.size(), label)</span><br></pre></td></tr></table></figure><pre><code>torch.Size([3, 224, 224]) 0torch.Size([3, 224, 224]) 0torch.Size([3, 224, 224]) 1torch.Size([3, 224, 224]) 0torch.Size([3, 224, 224]) 1torch.Size([3, 224, 224]) 0torch.Size([3, 224, 224]) 1torch.Size([3, 224, 224]) 1torch.Size([3, 224, 224]) 1torch.Size([3, 224, 224]) 1torch.Size([3, 224, 224]) 0torch.Size([3, 224, 224]) 1</code></pre><p>除了上述操作之外，transforms还可以通过Lambda封装自定义的转换策略。例如，想对PIL Image进行随机旋转，则可写成trans = T.Lambda(lambda img: img.rotate(random() * 360))。</p><p>torchvision已经预先实现了常用的Dataset，包括前面使用过的CIFAR-10，以及ImageNet、COCO、MNIST、LSUN等数据集，可通过调用torchvision.datasets下相应对象来调用相关数据集。<br>还有一个经常使用到的Dataset——ImageFolder，它的实现和上述DogCat很相似。ImageFolder假设所有的文件按文件夹保存，每个文件夹下储存同一个类别的图片，文件夹名为类名，其构造函数如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ImageFolder(root, transform = <span class="literal">None</span>, target_transform = <span class="literal">None</span>, loader = default_loader)</span><br></pre></td></tr></table></figure><p>它主要有以下四个参数。</p><ul><li>root：在root指定的路径下寻找图片。</li><li>transform：对PIL Image进行转换操作，transform的输入是使用loader读取图片的返回对象。</li><li>target_transform：对label的转换。</li><li>loader：指定加载图片的函数，默认操作是读取为PIL Image对象。</li></ul><p>label是按照文件夹名顺序排序后存成字典的，即{类名:类序号}，一般来说最好直接将文件夹命名为从0开始的数字，这样会和ImageFolder实际的label一致，如果不是这种命名规范，建议通过self.class_to_idx属性了解label和文件夹名的映射关系。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!tree --charset ASCII data/dogcat2/</span><br></pre></td></tr></table></figure><pre><code>data/dogcat2/|-- cat|   |-- cat.0.jpg|   |-- cat.1.jpg|   |-- cat.2.jpg|   |-- cat.3.jpg|   `-- cat.4.jpg`-- dog    |-- dog.0.jpg    |-- dog.1.jpg    |-- dog.10.jpg    |-- dog.1000.jpg    `-- dog.10000.jpg2 directories, 10 files</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> ImageFolder</span><br><span class="line">dataset = ImageFolder(<span class="string">'data/dogcat2/'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataset.class_to_idx</span><br></pre></td></tr></table></figure><pre><code>{&apos;cat&apos;: 0, &apos;dog&apos;: 1}</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataset.imgs</span><br></pre></td></tr></table></figure><pre><code>[(&apos;data/dogcat2/cat/cat.0.jpg&apos;, 0), (&apos;data/dogcat2/cat/cat.1.jpg&apos;, 0), (&apos;data/dogcat2/cat/cat.2.jpg&apos;, 0), (&apos;data/dogcat2/cat/cat.3.jpg&apos;, 0), (&apos;data/dogcat2/cat/cat.4.jpg&apos;, 0), (&apos;data/dogcat2/dog/dog.0.jpg&apos;, 1), (&apos;data/dogcat2/dog/dog.1.jpg&apos;, 1), (&apos;data/dogcat2/dog/dog.10.jpg&apos;, 1), (&apos;data/dogcat2/dog/dog.1000.jpg&apos;, 1), (&apos;data/dogcat2/dog/dog.10000.jpg&apos;, 1)]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 没有任何transform，所以返回的还是PIL Image对象</span></span><br><span class="line">dataset[<span class="number">0</span>][<span class="number">1</span>] <span class="comment"># 第一维是第几张图，第二维为1是label，为0是Image对象</span></span><br><span class="line">dataset[<span class="number">0</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p><img alt="png" data-src="/uploads/tools/output_21_0.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加上transform</span></span><br><span class="line">normalize = T.Normalize(mean = [<span class="number">0.4</span>, <span class="number">0.4</span>, <span class="number">0.4</span>], std = [<span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>])</span><br><span class="line">transform = T.Compose([</span><br><span class="line">    T.RandomResizedCrop(<span class="number">224</span>),</span><br><span class="line">    T.RandomHorizontalFlip(),</span><br><span class="line">    T.ToTensor(),</span><br><span class="line">    normalize,</span><br><span class="line">])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataset = ImageFolder(<span class="string">'data/dogcat2/'</span>, transform = transform)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataset.class_to_idx</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 深度学习中图片数据一般保存为CHW，即通道数 x 高 x 宽</span></span><br><span class="line"><span class="keyword">for</span> img, index <span class="keyword">in</span> dataset:</span><br><span class="line">    print(img.size(), index)</span><br></pre></td></tr></table></figure><pre><code>torch.Size([3, 224, 224]) 0torch.Size([3, 224, 224]) 0torch.Size([3, 224, 224]) 0torch.Size([3, 224, 224]) 0torch.Size([3, 224, 224]) 0torch.Size([3, 224, 224]) 1torch.Size([3, 224, 224]) 1torch.Size([3, 224, 224]) 1torch.Size([3, 224, 224]) 1torch.Size([3, 224, 224]) 1</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">to_img = T.ToPILImage()</span><br><span class="line"><span class="comment"># 0.2和0.4是标准差和均值的近似</span></span><br><span class="line">to_img(dataset[<span class="number">0</span>][<span class="number">0</span>]*<span class="number">0.2</span> + <span class="number">0.4</span>)</span><br></pre></td></tr></table></figure><p><img alt="png" data-src="/uploads/tools/output_26_0.png"></p><p>Dataset只负责数据的抽象，一次调用__getattr__只返回一个样本。而训练神经网络是对一个batch的数据进行操作，同时还需要对数据进行shuffle和并行加速等。对此，pytorch提供了DataLoader帮助我们实现这些功能。<br>DataLoader的函数定义如下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DataLoader(dataset, batch_size = <span class="number">1</span>, shuffle = <span class="literal">False</span>, sampler = <span class="literal">None</span>, num_workers = <span class="number">0</span>, collate_fn = default_collate, pin_memory = <span class="literal">False</span>, drop_last = <span class="literal">False</span>)</span><br></pre></td></tr></table></figure><ul><li>dataset：加载的数据集（Dataset对象）</li><li>batch_size：batch size（批大小）</li><li>shuffle：是否将数据打乱</li><li>sampler：样本抽样，后续会详细介绍。</li><li>num_workers：使用多进程加载的进程数，0代表不使用多进程。</li><li>collate_fn：如何将多个样本数据拼接成一个batch，一般使用默认的拼接方式即可。</li><li>pin_memory：是否将数据保存在pin memory区，pin memory中的数据转到GPU会快一些。</li><li>drop_last：dataset中的数据个数可能不是batch_size的整数倍，drop_last为True会将多出来不足一个batch的数据丢失。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataloader = DataLoader(dataset, batch_size = <span class="number">3</span>, shuffle = <span class="literal">True</span>, num_workers = <span class="number">0</span>, drop_last = <span class="literal">False</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dataiter = iter(dataloader)</span><br><span class="line">imgs, labels = next(dataiter)</span><br><span class="line">imgs.size()</span><br></pre></td></tr></table></figure><pre><code>torch.Size([3, 3, 224, 224])</code></pre><p>dataloader是一个可迭代对象，我们可以向使用迭代器一样使用它，利用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> batch_datas, batch_labels <span class="keyword">in</span> dataloader:</span><br><span class="line">    train()</span><br></pre></td></tr></table></figure><p>或</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dataiter = iter(dataloader)</span><br><span class="line">batch_datas, batch_labels = next(dataiter)</span><br></pre></td></tr></table></figure><p>在数据处理中，有时会出现某个样本无法读取等问题，例如某张图片损坏。这时__getitem__函数中将出现异常，此时最好的解决方案即是将出错的样本剔除。如果遇到这种情况实在无法处理，则可以返回None对象，然后在Dataloader中实现自定义的collate_fn，将空对象过滤掉。但要注意，在这种情况下dataloader返回的一个batch的样本数目会少于batch_size。<br>对丢弃样本异常图片而言，这种做法会更好一些，因为它能保证每个batch样本的数目仍是batch_size。但在大多数情况下，最好的方式还是对数据进行彻底清除。</p><p>DataLoader里并没有太多的魔法方法，它封装了python的标准库multiprocessing使其能够实现多进程加速。在Dataset和DataLoader的使用方面有以下建议。</p><ol><li>高负载的操作放在__getitem__中，如加载图片</li><li>dataset中应尽量只包含只读对象，避免修改任何可变对象。</li></ol><p>第一点是因为多进程会并行地调用__getitem__函数，将负载高的放在__getitem__函数中能够实现并行加速。第二点是因为dataloader使用多进程加载，如果在Dataset中使用了可变对象，可能会有意想不到的冲突。<br>在多线程/多进程中，修改一个可变对象需要加锁，但是dataloader的设计使得其很难加锁（在实际使用中也应尽量避免锁的存在）。<br>如果一定要修改可变对象，建议使用python标准库queue<br>使用python multiprocessing库的另一个问题是，在使用多进程时，如果主程序异常终止（比如用“Ctrl+C”快捷键强行退出），相应的数据加载进程可能无法正常退出。这时需要手动强行终止进程。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ps x | grep &lt;cmdline&gt; | awk <span class="string">'&#123;print $1&#125;'</span> | xargs <span class="built_in">kill</span></span><br></pre></td></tr></table></figure><ul><li>ps x:获取当前用户的所有进程。</li><li>grep <cmdline>: 找到已经停止的pytorch程序的进程，例如你是通过python train.py启动的，那就需要些grep ‘python train.py’。</cmdline></li><li>awk ‘{print $1}’：获取进程的pid</li><li>xargs kill：终止进程，根据需要可能要写成xargs kill -9强制终止进程。</li></ul><p>pytorch还单独提供一个sampler模块，用来对数据进行采样。常用的有随机采样器RandomSampler，当dataloader的shuffle参数为True时，就是调用的这个。<br>这里介绍一个很有用的采样方法：WeightedRandomSampler，它会根据每个样本的权重选取数据，在样本比例不均衡的问题中，可用它进行重采样。</p><p>构建WeightedRandomSampler时需提供两个参数：每个样本的权重weights、共选取的样本总数num_samples，以及一个可选参数replacement。权重越大的样本被选中的概率越大，待选取的样本数量一般小于全部的样本数目。replacement用于指定是否可以重复选取某一个样本，默认为True，即允许在一个epoch中重复采样某一个数据。如果设为False，则当某一类样本被全部选取完，但其样本数目仍未达到num_samples时，sampler将不会从该类中选取数据，此时可能导致weights参数失效。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!tree --charset ASCII data/dogcat/</span><br></pre></td></tr></table></figure><pre><code>data/dogcat/|-- cat.0.jpg|-- cat.1.jpg|-- cat.2.jpg|-- cat.5.jpg|-- cat.6.jpg|-- dog.0.jpg|-- dog.1.jpg|-- dog.10.jpg|-- dog.1000.jpg`-- dog.10000.jpg0 directories, 10 files</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">dataset = DogCat(<span class="string">'data/dogcat'</span>,transforms = transform)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 狗的图片被取出的概率是猫的概率的两倍</span></span><br><span class="line"><span class="comment"># 两类图片被取出的概率与weights的绝对大小无关，只和比值有关</span></span><br><span class="line">weights = [<span class="number">2</span> <span class="keyword">if</span> label == <span class="number">1</span> <span class="keyword">else</span> <span class="number">1</span> <span class="keyword">for</span> data, label <span class="keyword">in</span> dataset]</span><br><span class="line">weights</span><br></pre></td></tr></table></figure><pre><code>[1, 1, 2, 1, 1, 2, 2, 2, 2, 1]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data.sampler <span class="keyword">import</span> WeightedRandomSampler</span><br><span class="line">sampler = WeightedRandomSampler(weights, num_samples=<span class="number">12</span>, replacement=<span class="literal">True</span>)</span><br><span class="line">dataloader = DataLoader(dataset, batch_size = <span class="number">3</span>, sampler=sampler)</span><br><span class="line"><span class="keyword">for</span> datas, labels <span class="keyword">in</span> dataloader:</span><br><span class="line">    print(labels.tolist())</span><br></pre></td></tr></table></figure><pre><code>[0, 0, 1][0, 1, 1][1, 1, 1][0, 0, 0]</code></pre><p>一共只有10个样本，却返回了12个，说明样本被重复返回，这就是replacement参数的作用</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># replacement改为False</span></span><br><span class="line">sampler = WeightedRandomSampler(weights, num_samples=<span class="number">10</span>, replacement=<span class="literal">False</span>)</span><br><span class="line">dataloader = DataLoader(dataset, batch_size = <span class="number">5</span>, sampler=sampler)</span><br><span class="line"><span class="keyword">for</span> datas, labels <span class="keyword">in</span> dataloader:</span><br><span class="line">    print(labels.tolist())</span><br></pre></td></tr></table></figure><pre><code>[0, 1, 0, 1, 1][1, 1, 0, 0, 0]</code></pre><p>在这种情况下，num_samples等于dataset的样本总数，为了不重复选取，sampler会将每个样本都返回，这样就失去了weight参数的意义。</p><p>从上面的例子可见sampler在样本采样中的作用：如果指定了sampler，shuffle将不再生效，并且sampler.num_samples<br>会覆盖dataset的实际大小，即一个epoch返回的图片总数取决于sampler.num_samples。</p><h4 id="计算机视觉工具包：torchvision"><a href="#计算机视觉工具包：torchvision" class="headerlink" title="计算机视觉工具包：torchvision"></a>计算机视觉工具包：torchvision</h4><p>计算机视觉是深度学习中最重要的一类应用，为了方便研究者使用，pytorch团队专门开发一个视觉工具包torchvision，这个包独立于pytorch。</p><p>torchvision主要包含以下三部分。</p><ul><li>model：提供深度学习中各种经典网络的网络结构及预训练好的模型，包括AlexNet、VGG系列、ResNet系列、Inception系列等。</li><li>datasets：提供常用的数据集加载，设计上都是继承torch.utils.data.Dataset，主要包括MNIST、CIFAR10/100、ImageNet、coco等。</li><li>transforms：提供常用的数据预处理操作，主要包括对Tensor及PIL Image对象的操作。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> models</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载预训练好的模型，如果不存在会下载</span></span><br><span class="line"><span class="comment"># 预训练好的模型保存在 ~/.torch/models/下面</span></span><br><span class="line">resnet34 = models.resnet34(pretrained = <span class="literal">True</span>,num_classes = <span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改最后的全连接层为10分类问题（默认是ImageNet上的1000分类）</span></span><br><span class="line">resnet34.fc = nn.Linear(<span class="number">512</span>, <span class="number">10</span>)</span><br></pre></td></tr></table></figure><pre><code>Downloading: &quot;https://download.pytorch.org/models/resnet34-333f7ec4.pth&quot; to /root/.cache/torch/checkpoints/resnet34-333f7ec4.pth100%|██████████| 83.3M/83.3M [00:35&lt;00:00, 2.49MB/s]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">transform = T.Compose([</span><br><span class="line">    T.Resize(<span class="number">32</span>), <span class="comment"># 缩放图片（Image），保持长宽比不变，最短边为224像素</span></span><br><span class="line">    T.CenterCrop(<span class="number">32</span>), <span class="comment"># 从中间取出224 x224的图片</span></span><br><span class="line">    T.ToTensor(),   <span class="comment"># 将图片转换成Tensor并归一化至[0, 1]</span></span><br><span class="line">    T.Normalize(mean = [<span class="number">.5</span>], std = [<span class="number">.5</span>,]) <span class="comment"># 标准化至[-1, 1]</span></span><br><span class="line">])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="comment"># 指定数据集路径为data，如果数据集不存在则进行下载</span></span><br><span class="line"><span class="comment"># 通过train = False获取测试集</span></span><br><span class="line">dataset = datasets.MNIST(<span class="string">'data/'</span>,download=<span class="literal">True</span>, train = <span class="literal">False</span>, transform = transform)</span><br></pre></td></tr></table></figure><p>torchvision还提供了两个常用的函数。一个是make_grid，它能将多张图片拼接在一个网络中；另一个是save_img，它能将Tensor保存成图片。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">len(dataset)</span><br></pre></td></tr></table></figure><pre><code>10000</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">dataloader = DataLoader(dataset, shuffle = <span class="literal">True</span>, batch_size = <span class="number">16</span>)</span><br><span class="line"><span class="keyword">from</span> torchvision.utils <span class="keyword">import</span> make_grid, save_image</span><br><span class="line">dataiter = iter(dataloader)</span><br><span class="line">img = make_grid(next(dataiter)[<span class="number">0</span>], <span class="number">4</span>) <span class="comment"># 拼成4*4网络图片，且会转成3通道</span></span><br><span class="line">to_img(img)</span><br></pre></td></tr></table></figure><p><img alt="png" data-src="/uploads/tools/output_52_0.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">img2 = make_grid(next(dataiter)[<span class="number">0</span>], <span class="number">4</span>) <span class="comment"># 拼成4*4网络图片，且会转成3通道</span></span><br><span class="line">save_image(img2, <span class="string">'a.png'</span>)</span><br><span class="line">Image.open(<span class="string">'a.png'</span>)</span><br></pre></td></tr></table></figure><p><img alt="png" data-src="/uploads/tools/output_53_0.png"></p><h4 id="可视化工具"><a href="#可视化工具" class="headerlink" title="可视化工具"></a>可视化工具</h4><h5 id="visdom"><a href="#visdom" class="headerlink" title="visdom"></a>visdom</h5><p><a href="https://github.com/facebookresearch/visdom" target="_blank" rel="noopener">visdom</a>是facebook专门为pytorch开发的一款可视化工具，开源于2017年3月。</p><p>visdom可以创造、组织和共享多种数据的可视化，包括数值、图像、文本，甚至是视频，支持pytorch、torch及numpy。用户可通过编程组织可视化空间或通过用户接口为数据打造仪表盘，检查试验结果和调试代码。<br>visdom中有一下两个重要概念。</p><ul><li>env：环境。不同环境的可视化结果相互隔离，互不影响，在使用时如果不指定env，默认使用main。不同用户。不同程序一般使用不同的env。</li><li>pane：窗格。窗格可用于可视化图像、数值或打印文本等，其可以拖动、缩放、保存和关闭。一个程序可使用同一个env中的不同pane，每个pane可视化或记录某一信息。<br>“clear”按钮可以清空当前env的所有pane，“save”按钮可将当前env保存成json文件，保存路径位于~/.visdom/目录下。修改env的名字后单击fork，可将当前env另存为新文件。</li></ul><p>通过命令pip install visdom即可完成visdom的安装。安装完成，须通过python -m visdom.server命令启动visdom服务，或通过nohup python -m visdom.server &amp;命令将服务放至后台运行。visdom服务是一个web server服务，默认绑定8097端口，客户端与服务器间通过tornado进行非阻塞交互。</p><p>使用visdom时有两点需要注意的地方。</p><ul><li>需手动指定保存env，可在web界面单击save按钮或在程序中调用save方法，否则visdom服务重启后，env等信息会丢失</li><li>客户端与服务器之间的交互采用tornado异步框架，可视化操作不会阻塞当前程序，网络异常也不会导致程序退出。<br>visdom以Plotly为基础。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> visdom</span><br><span class="line">vis = visdom.Visdom(env = <span class="string">u'test1'</span>)</span><br><span class="line">x = t.arange(<span class="number">1</span>, <span class="number">30</span>, <span class="number">0.01</span>)</span><br><span class="line">y = t.sin(x)</span><br><span class="line">vis.line(X = x, Y = y, win = <span class="string">'sinx'</span>,opts = &#123;<span class="string">'title'</span>:<span class="string">'y = sin(x)'</span>&#125;)</span><br></pre></td></tr></table></figure><pre><code>WARNING:root:Setting up a new session...&apos;sinx&apos;</code></pre><p>下面我们逐一分析这几行代码。</p><ul><li>vis = visdom.Visdom(env = u’test1’)，用于构建一个客户端，客户端除了制定env外，还可以制定host、post等参数。</li><li>vis作为一个客户端对象，可以使用如下常见的画图函数。<ul><li>line：类似MATLAB中的plot操作，用于记录某些标量的变化，例如损失、标准率等。</li><li>image：可视化图片，可以是输入的图片，也可以是GAN生成的图片，还可以是卷积核的信息。</li><li>text：用于记录日志等文字信息，支持HTML格式</li><li>histgram：可视化分布，主要是查看数据、参数的分布。</li><li>scatter：绘制散点图。</li><li>bar：绘制柱状图</li><li>pie：绘制饼状图</li><li>更多参考github主页</li></ul></li></ul><p>visdom同时支持pytorch的tensor和numpy的ndarray两种数据结构，但不支持python的int、float等类型。上述操作的参数一般不同，但有两个参数是绝大多数操作都具备。</p><ul><li>win：用于指定pane的名字，如果不指定，visdom将自动分配一个新的pane。如果两次操作指定的win名字一样，新的操作将覆盖当前pane的内容，因此建议每次操作都指定win</li><li>opts：用来可视化配置，接受一个字典，常见的option包括title、xlabel、ylabel、width等，主要用于设置pane的显示格式。</li></ul><p>之前提到过，每次操作会覆盖之前的数据，但我们在训练网络的过程中往往需要不断更新数值，这时就需要指定参数update=’append’来避免覆盖之前的数值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="comment"># append追加数据</span></span><br><span class="line"><span class="keyword">for</span> ii <span class="keyword">in</span> range(<span class="number">0</span>, <span class="number">10</span>):</span><br><span class="line">    x = t.Tensor([ii])</span><br><span class="line">    y = <span class="number">2</span> * x</span><br><span class="line">    time.sleep(<span class="number">0.5</span>)</span><br><span class="line">    vis.line(X = x, Y= y, win = <span class="string">'polynomial'</span>,name = <span class="string">'Trace1'</span>, update = <span class="string">'append'</span> <span class="keyword">if</span> ii &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="literal">None</span>)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 新增一条线</span></span><br><span class="line"><span class="keyword">for</span> ii <span class="keyword">in</span> np.linspace(<span class="number">0</span>, <span class="number">5</span>, <span class="number">50</span>):</span><br><span class="line">    x = t.Tensor([ii])</span><br><span class="line">    y = x ** <span class="number">2</span></span><br><span class="line">    time.sleep(<span class="number">0.1</span>)</span><br><span class="line">    vis.line(X = x, Y= y, win = <span class="string">'polynomial'</span>,name = <span class="string">'Trace2'</span>, update = <span class="string">'append'</span>)</span><br></pre></td></tr></table></figure><p>images的画图功能可分为如下两类。</p><ul><li>image接受一个二维或三维向量，H<em>W 或 3</em>H*W ，前者是黑白图像，后者是彩色图像。</li><li>images接受一个四维向量N<em>C</em>H*W，C可以是1或3，分别代表黑白和彩色图像。可实现类似torchvision中make_grid的功能，将多张图片拼接在一起。images也可以接受一个二维或三维的向量，此时它所实现的功能与image一致。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可视化一张随机的黑白照片</span></span><br><span class="line">vis.image(t.randn(<span class="number">64</span>, <span class="number">64</span>).numpy())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化一张随机的彩色图片</span></span><br><span class="line">vis.image(t.randn(<span class="number">3</span>, <span class="number">64</span>, <span class="number">64</span>).numpy(), win = <span class="string">'random2'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化36张随机的彩色图片，每行6张</span></span><br><span class="line">vis.images(t.randn(<span class="number">36</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">64</span>).numpy(), nrow = <span class="number">6</span>, win = <span class="string">'random3'</span>,opts = &#123;<span class="string">'title'</span>:<span class="string">'random_imgs'</span>&#125;)</span><br></pre></td></tr></table></figure><pre><code>&apos;random3&apos;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">x = dataset[<span class="number">0</span>][<span class="number">0</span>].unsqueeze(<span class="number">0</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">36</span>):</span><br><span class="line">    x = t.cat((x, dataset[i][<span class="number">0</span>].unsqueeze(<span class="number">0</span>)), dim = <span class="number">0</span>)</span><br><span class="line">vis.images(x, nrow = <span class="number">6</span>, win = <span class="string">'MNIST'</span>,opts = &#123;<span class="string">'title'</span>:<span class="string">'MNIST数据集'</span>&#125;)</span><br></pre></td></tr></table></figure><pre><code>&apos;MNIST&apos;</code></pre><p>vis.text用于可视化文本，它支持所有的html标签，同时也遵循着html的语法标签。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vis.text(<span class="string">u'''&lt;h1&gt;Hello visdom&lt;/h1&gt;&lt;br&gt;visdom是Facebook专门为&lt;b&gt;pytorch&lt;b/&gt;开发一个可视化工具，'''</span>,win = <span class="string">'visdom'</span>,opts = &#123;<span class="string">'title'</span>:<span class="string">u'visdom简介'</span>&#125;)</span><br></pre></td></tr></table></figure><pre><code>&apos;visdom&apos;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vis.text(<span class="string">'ss'</span>, win = <span class="string">'visdom'</span>, append = <span class="literal">True</span>, opts = &#123;<span class="string">'title'</span>:<span class="string">'平方'</span>&#125;)</span><br></pre></td></tr></table></figure><pre><code>&apos;visdom&apos;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 文本更新</span></span><br><span class="line">vis.text(<span class="string">'&lt;b&gt;平方:&lt;/b&gt;'</span>, win = <span class="string">'pingfang'</span>,opts = &#123;<span class="string">'title'</span>:<span class="string">'pingfang'</span>&#125;)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">20</span>):</span><br><span class="line">    time.sleep(<span class="number">0.5</span>)</span><br><span class="line">    vis.text(<span class="string">'&lt;b&gt;[info]&#123;&#125;^2=&#123;&#125;&lt;/b&gt;'</span>.format(i, i **<span class="number">2</span>), append = <span class="literal">True</span>,win = <span class="string">'pingfang'</span>)</span><br></pre></td></tr></table></figure><h4 id="GPU"><a href="#GPU" class="headerlink" title="GPU"></a>GPU</h4><p>略</p><h4 id="持久化"><a href="#持久化" class="headerlink" title="持久化"></a>持久化</h4><p>在pytorch中，以下对象可以持久化到硬盘，并能通过相应的方法加载到内存中。</p><ul><li>Tensor</li><li>Variable</li><li>nn.Module</li><li>Optimizer</li></ul><p>本质上，上述信息最终都是保存成Tensor。Tensor的保存和加载十分简单，使用t.save和t.load即可完成相应的功能。在save/load时可指定使用pickle模块，在load时还可将GPU tensor映射到CPU或其他GPU上。</p><p>我们可以通过t.save(obj, file_name)等方法保存任意可序列化的对象，然后通过obj = t.load(file_name)方法加载保存的数据。</p><p>对Module和Optimizer对象，这里建议保存对应的state_dict，而不是直接保存整个Module/Optimizer对象。Optimizer对象保存的是参数即动量信息，通过加载之前的动量信息，能够有效减少模型震荡。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">a = t.Tensor(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="keyword">if</span> t.cuda.is_available():</span><br><span class="line">    a = a.cuda(<span class="number">1</span>)<span class="comment"># 把a转为GPU1上的tensor</span></span><br><span class="line">    t.save(a, <span class="string">'a.pth'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#加载为b，存储于GPU1上（因为保存时就在GPU1）</span></span><br><span class="line">    b = t.load(<span class="string">'a.pth'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 加载为c，存储于CPU</span></span><br><span class="line">    c = t.load(<span class="string">'a.pth'</span>, map_location = <span class="keyword">lambda</span> storage, loc:storage)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 加载为d，存储于GPU0</span></span><br><span class="line">    d = t.load(<span class="string">'a.pth'</span>, map_location = &#123;<span class="string">'cuda:1'</span>:<span class="string">'cuda:0'</span>&#125;)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision.models <span class="keyword">import</span> resnet34</span><br><span class="line"></span><br><span class="line">model = resnet34()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># model的state_dict是一个字典</span></span><br><span class="line">list(model.state_dict().keys())[:<span class="number">8</span>]</span><br></pre></td></tr></table></figure><pre><code>[&apos;conv1.weight&apos;, &apos;bn1.weight&apos;, &apos;bn1.bias&apos;, &apos;bn1.running_mean&apos;, &apos;bn1.running_var&apos;, &apos;bn1.num_batches_tracked&apos;, &apos;layer1.0.conv1.weight&apos;, &apos;layer1.0.bn1.weight&apos;]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># module对象的保存与加载</span></span><br><span class="line">t.save(model.state_dict(), <span class="string">'resnet32.pth'</span>)</span><br><span class="line">model.load_state_dict(t.load(<span class="string">'resnet32.pth'</span>))</span><br></pre></td></tr></table></figure><pre><code>&lt;All keys matched successfully&gt;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = t.optim.Adam(model.parameters(), lr = <span class="number">0.1</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t.save(optimizer.state_dict(),<span class="string">'optimizer.pth'</span>)</span><br><span class="line">optimizer.load_state_dict(t.load(<span class="string">'optimizer.pth'</span>))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">all_data = dict(</span><br><span class="line">    optimizer = optimizer.state_dict(),</span><br><span class="line">    model = model.state_dict(),</span><br><span class="line">    info = <span class="string">u'模型和优化器的所有参数'</span></span><br><span class="line">)</span><br><span class="line">t.save(all_data, <span class="string">'all_data.pth'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">all_data = t.load(<span class="string">'all_data.pth'</span>)</span><br><span class="line">all_data.keys()</span><br></pre></td></tr></table></figure><pre><code>dict_keys([&apos;optimizer&apos;, &apos;model&apos;, &apos;info&apos;])</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;pytorch中常见的工具&quot;&gt;&lt;a href=&quot;#pytorch中常见的工具&quot; class=&quot;headerlink&quot; title=&quot;pytorch中常见的工具&quot;&gt;&lt;/a&gt;pytorch中常见的工具&lt;/h3&gt;&lt;p&gt;在训练神经网络的过程中需要用到很多工具、其中最重要的
      
    
    </summary>
    
      <category term="笔记" scheme="https://zinw623.github.io/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="pytorch" scheme="https://zinw623.github.io/categories/%E7%AC%94%E8%AE%B0/pytorch/"/>
    
    
      <category term="pytorch" scheme="https://zinw623.github.io/tags/pytorch/"/>
    
      <category term="笔记" scheme="https://zinw623.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>【notes】pytorch学习笔记3-神经网络工具箱nn</title>
    <link href="https://zinw623.github.io/2019/08/22/nn.Module/"/>
    <id>https://zinw623.github.io/2019/08/22/nn.Module/</id>
    <published>2019-08-22T09:30:45.000Z</published>
    <updated>2019-08-23T11:38:08.172Z</updated>
    
    <content type="html"><![CDATA[<h3 id="神经网络工具箱nn"><a href="#神经网络工具箱nn" class="headerlink" title="神经网络工具箱nn"></a>神经网络工具箱nn</h3><p>autograd实现了自动微分系统，然而对于深度学习来说过于底层。nn模块是构建与autograd之上的神经网络模块。除了nn之外，我们还会介绍神经网络中常用的工具，比如优化器optim,初始化init等。</p><h4 id="nn-Module"><a href="#nn-Module" class="headerlink" title="nn.Module"></a>nn.Module</h4><p>torch.nn的核心数据结构是Module，它是一个抽象的概念，既可以表示神经网络中的某个层（layer），也可以表示一个包含很多层的神经网络。在实际使用中，最常见的做法是继承nn.Module，撰写自己的网络、层。</p><p>全连接层，又名仿射层，输出$\boldsymbol{x}$和输入$\boldsymbol{x}$满足$\boldsymbol{y=Wx+b}$，$\boldsymbol{W}$ 和 $\boldsymbol{b}$ 是可学习函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable <span class="keyword">as</span> V</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Linear</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_features, out_features)</span>:</span></span><br><span class="line">        </span><br><span class="line">        super(Linear, self).__init__() <span class="comment">#等价于nn.Module.__init__(self)</span></span><br><span class="line">        self.w = nn.Parameter(t.randn(in_features, out_features))</span><br><span class="line">        self.b = nn.Parameter(t.randn(out_features))</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = x.mm(self.w)</span><br><span class="line">        <span class="keyword">return</span> x + self.b.expand_as(x)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">layer = Linear(<span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line">input = V(t.randn(<span class="number">2</span>, <span class="number">4</span>))</span><br><span class="line">output = layer(input)</span><br><span class="line">output</span><br></pre></td></tr></table></figure><pre><code>tensor([[ 2.0269,  5.1465,  1.5603],        [-0.6868, -0.8096, -0.6427]], grad_fn=&lt;AddBackward0&gt;)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> name, parameter <span class="keyword">in</span> layer.named_parameters():</span><br><span class="line">    </span><br><span class="line">    print(name, parameter)</span><br></pre></td></tr></table></figure><pre><code>w Parameter containing:tensor([[-0.0121, -0.2593, -0.5310],        [ 0.2982, -0.2846, -0.0437],        [ 0.6220,  1.7351,  0.8025],        [ 1.0544,  2.3325,  0.6561]], requires_grad=True)b Parameter containing:tensor([0.2586, 2.3734, 0.5372], requires_grad=True)</code></pre><p>但需要注意一下几点：</p><ul><li>自定义层Linear必须继承nn.Module，并且在其构造函数中需调用nn.Module的构造函数，即super(Linear, self).<strong>init</strong>()或nn.Module.__init__(self)。</li><li>在构造函数__init__中必须自己定义可学习的参数，并封装成Parameter，如在本例中我们把w和b封装成Parameter。Parameter是一种特殊的Variable，但其默认需要求导（requires_grad=True）。</li><li>forward函数实现前向传播过程，其输入可以是一个或多个variable，对x的任何操作也必须是variable支持的操作。</li><li>无需写反向传播函数，因其前向传播都是对variable进行操作，nn.Module能够利用autograd自动实现反向传播，这一点比Function简单许多。</li><li>使用时，直观上可将layer看成数学概念中的函数，调用layer(input)即可得到input对应的结果。它等价于layers.__call__(input)，在__call__函数中，主要调用的是layer.forward(X)，另外还对钩子做了一些处理。所以在实际使用中应尽量使用layer(x)而不是使用layer.forward(x)。</li></ul><p>Module能够自动检测到自己的parameter，并将其作为学习参数。除了parameter，Module还包含子Module，主Module能够递归查找子Module中的parameter。</p><p>下面看一个稍微复杂的网络：多层感知机。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Perceptron</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_features, hidden_features, out_features)</span>:</span></span><br><span class="line">        nn.Module.__init__(self)</span><br><span class="line">        self.layer1 = Linear(in_features, hidden_features)</span><br><span class="line">        self.layer2 = Linear(hidden_features, out_features)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.layer1(x)</span><br><span class="line">        x = t.sigmoid(x)</span><br><span class="line">        <span class="keyword">return</span> self.layer2(x)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">perceptron = Perceptron(<span class="number">3</span>, <span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> perceptron.named_parameters():</span><br><span class="line">    print(name, param.size())</span><br></pre></td></tr></table></figure><pre><code>layer1.w torch.Size([3, 4])layer1.b torch.Size([4])layer2.w torch.Size([4, 1])layer2.b torch.Size([1])</code></pre><p>注意一下两个知识点。</p><ul><li>构造函数__init__中，可利用前面自定义的Linear层（module）作为当前module对象的一个字module，它的可学习参数，也会成为当前module的可学习参数。</li><li>在前向传播函数中，我们有意识地将输出变量都命名为x，是为了能让python回收一些中间层的输出，从而节省内存。但并不是所有的中间结果都会被回收，有些variable虽然名字被覆盖，但其在反向传播时仍需要用到，此时python的内存回收模块将通过检查引用计数，不会回收这一部分内存。</li></ul><p>module中parameter的全局命名规范如下。</p><ul><li>parameter直接命名。例如self.param_name = nn.Parameter(t.randn(3,4))，命名为param_name。</li><li>子module中的parameter，会在其名字之前加上当前module的名字，就是sub_module.param_name。</li></ul><p>为了方便用户使用，pytorch实现了神经网络中绝大多数的layer，这些layer都继承了nn.Module，封装了可学习参数parameter，并实现了forward函数，且专门针对GPU运算进行了CuDNN优化。具体内容可参考<a href="http://pytorch.org/docs/nn.html" target="_blank" rel="noopener">官方文档</a>或在IPython/Jupyter中使用nn.layer。</p><p>阅读文档注意：</p><ul><li>构造函数的参数，如nn.Linear(in_features, out_features, bias)，需关注着三个参数的作用</li><li>属性、可学习参数和子module。如nn.Linear中有weight和bias两个可学习参数，不包含子module</li><li>输入输出的形状，如nn.Linear的输入形状是(N, input_features)，输出为(N, output_features)，N是batch_size。<br>若想输入一个数据需要调用unsqueeze(0)函数将数据伪装成batch_size = 1的batch</li></ul><h4 id="常用的神经网络层"><a href="#常用的神经网络层" class="headerlink" title="常用的神经网络层"></a>常用的神经网络层</h4><h5 id="图像相关层"><a href="#图像相关层" class="headerlink" title="图像相关层"></a>图像相关层</h5><p>图像相关层主要包括卷积层（Conv）、池化层（Pool）等，这些层在实际使用中可分为一维、二维和三维，池化层又分为平均池化（AvgPool）、最大值池化（MaxPool）、自适应池化（AdaptiveAvgPool）等。卷积层除了常用的前向卷积外，还有逆卷积（TransposeConv）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable <span class="keyword">as</span> V</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> torchvision.transforms <span class="keyword">import</span> ToTensor, ToPILImage</span><br><span class="line">to_tensor = ToTensor()</span><br><span class="line">to_pil = ToPILImage()</span><br><span class="line">curry = Image.open(<span class="string">'curry'</span>)</span><br><span class="line">curry</span><br></pre></td></tr></table></figure><img width="50%" data-src="\uploads\nn.module\output_15_0.png"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输入一个batch，batch_size = 1</span></span><br><span class="line">input = to_tensor(curry).unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 锐化卷积核</span></span><br><span class="line">kernel = t.ones(<span class="number">3</span>, <span class="number">3</span>) / <span class="number">-9</span></span><br><span class="line">kernel[<span class="number">1</span>][<span class="number">1</span>] = <span class="number">1</span></span><br><span class="line">conv = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, (<span class="number">3</span>, <span class="number">3</span>), <span class="number">1</span>, bias = <span class="literal">False</span>)</span><br><span class="line">conv.weight.data = kernel.view(<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">out = conv(V(input))</span><br><span class="line">to_pil(out.data.squeeze(<span class="number">0</span>))</span><br></pre></td></tr></table></figure><img width="50%" data-src="\uploads\nn.module\output_16_0.png"><p>Shape:</p><ul><li>Input: $(N, C_{in}, H_{in}, W_{in})$</li><li>Output: $(N, C_{out}, H_{out}, W_{out})$ where<br>$H_{out} = \left\lfloor\frac{H_{in}  + 2 \times \text{padding}[0] - \text{dilation}[0]\times (\text{kernel_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor$</li></ul><p>$W_{out} = \left\lfloor\frac{W_{in}  + 2 \times \text{padding}[1] - \text{dilation}[1]\times (\text{kernel_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor$</p><p>图像的卷积操作还有各种变体，有关各种变体的介绍可以参照此处的介绍<a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md" target="_blank" rel="noopener">https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md</a></p><p>池化层可以看作是一种特殊的卷积层，用来下采样。但池化层没有可学习参数，其weight是固定的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pool = nn.AvgPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">list(pool.parameters())</span><br></pre></td></tr></table></figure><pre><code>[]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">out = pool(V(input))</span><br><span class="line">to_pil(out.data.squeeze(<span class="number">0</span>))</span><br></pre></td></tr></table></figure><img width="50%" data-src="\uploads\nn.module\output_21_0.png"><p>除了卷积层和池化层，深度学习中还将常用到一下几个层</p><ul><li>Linear：全连接层</li><li>BatchNorm：批规范化层，分为1D、2D和3D。除了标准的BatchNorm之外，还有在风格迁移中常用到的InstanceNorm层。</li><li>Dropout：dropout层，用来防止过拟合，同样分为1D、2D和3D。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输入batch_size=2，维度3</span></span><br><span class="line">input = V(t.randn(<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">linear = nn.Linear(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">h = linear(input);h</span><br></pre></td></tr></table></figure><pre><code>tensor([[-0.4360,  0.3433, -0.1978, -0.3128],        [-0.9655,  0.6278,  0.2510,  0.1256]], grad_fn=&lt;AddmmBackward&gt;)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 4 channel,初始化标准差为4，均值为0</span></span><br><span class="line">bn = nn.BatchNorm1d(<span class="number">4</span>)</span><br><span class="line">bn.weight.data = t.ones(<span class="number">4</span>) * <span class="number">4</span></span><br><span class="line">bn.bias.data = t.zeros(<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">bn_out = bn(h)</span><br><span class="line"><span class="comment"># 注意输出的均值和方差</span></span><br><span class="line"><span class="comment"># 方差是标准差的平方，计算无偏方差分母会减1</span></span><br><span class="line"><span class="comment"># 使用unbiased=False，分母不减1</span></span><br><span class="line">bn_out.mean(<span class="number">0</span>), bn_out.var(<span class="number">0</span>, unbiased = <span class="literal">False</span>)</span><br></pre></td></tr></table></figure><pre><code>(tensor([-1.1921e-07,  0.0000e+00,  0.0000e+00,  0.0000e+00],        grad_fn=&lt;MeanBackward1&gt;), tensor([15.9977, 15.9921, 15.9968, 15.9967], grad_fn=&lt;VarBackward1&gt;))</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 每个元素以0.5的概率舍弃</span></span><br><span class="line">dropout = nn.Dropout(<span class="number">0.5</span>)</span><br><span class="line">o = dropout(bn_out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 有一半的概率会变成0</span></span><br><span class="line">o</span><br></pre></td></tr></table></figure><pre><code>tensor([[7.9994, -0.0000, -0.0000, -0.0000],        [-0.0000, 7.9980, 7.9992, 7.9992]], grad_fn=&lt;MulBackward0&gt;)</code></pre><p>以上很多例子中都对module的属性直接操作，其大多数是可学习参数，一般会随着学习的进行而不断改变。实际使用中除非需要使用特殊的初始化，否则尽量不要直接改变参数。</p><h5 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h5><p>pytorch实现了常见的激活函数。其他具体的接口信息可参见<a href="http://pytorch.org/docs/nn.html#non-linear-activations" target="_blank" rel="noopener">官方文档</a>，这些激活函数可以作为独立的layer使用。这里介绍最常用的激活函数ReLU，其数学表达式为：<br>$$ReLU(x)=max(0,x)$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">relu = nn.ReLU(inplace = <span class="literal">True</span>)</span><br><span class="line">input = V(t.randn(<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">print(input)</span><br><span class="line">output = relu(input)</span><br><span class="line">print(output) <span class="comment"># 小于0的都被截断为0</span></span><br><span class="line">id(input) == id(output)</span><br></pre></td></tr></table></figure><pre><code>tensor([[ 0.5049,  0.6093, -0.1565],        [-0.9114, -0.9594,  1.0539]])tensor([[0.5049, 0.6093, 0.0000],        [0.0000, 0.0000, 1.0539]])True</code></pre><p>ReLU函数有个inplace参数，如果设为True，如果设为True，它会把输出直接覆盖到输入中，这样可以节省内存/显存。之所以可以覆盖是因为在计算ReLU的反向传播时，只需根据输出就能够推算出反向传播的梯度。但是只有少数的autograd操作支持inplace操作（如variable.sigmoid_()），除非你明确知道自己在做什么，否则一般不要使用inplace操作。</p><p>在以上例子里，都是将每一层的输出直接作为下一层的输入，这种网络成为前馈传播网络。对于此种网络，如果每次都写复杂的forward函数会有些麻烦，在此就有两种简化方式，ModuleList和Sequential。其中Sequential是一个特殊的Module，它包含几个子module，前向传播时会将输入一层接一层第传递下去。ModuleList也是一个特殊的Module，可以包含几个子Module，可以像用list一样使用它，但不能直接把输入传给ModuleList。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Sequential的三种写法</span></span><br><span class="line"></span><br><span class="line">net1 = nn.Sequential()</span><br><span class="line">net1.add_module(<span class="string">'conv'</span>, nn.Conv2d(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>))     <span class="comment"># 输入为(N, C_&#123;in&#125;, H_&#123;in&#125;, W_&#123;in&#125;)，参数为</span></span><br><span class="line">net1.add_module(<span class="string">'batchnorm'</span>, nn.BatchNorm2d(<span class="number">3</span>)) <span class="comment"># 3为(N, C, H, W)中的C</span></span><br><span class="line">net1.add_module(<span class="string">'activation_layer'</span>, nn.ReLU())</span><br><span class="line"></span><br><span class="line">net2 = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>),</span><br><span class="line">    nn.BatchNorm2d(<span class="number">3</span>),</span><br><span class="line">    nn.ReLU()</span><br><span class="line">    )</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line">net3 = nn.Sequential(OrderedDict([</span><br><span class="line">    (<span class="string">'conv1'</span>, nn.Conv2d(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>)),</span><br><span class="line">    (<span class="string">'bn1'</span>, nn.BatchNorm2d(<span class="number">3</span>)),</span><br><span class="line">    (<span class="string">'relu1'</span>, nn.ReLU()),</span><br><span class="line">]))</span><br><span class="line"></span><br><span class="line">print(<span class="string">'net1'</span>, net1)</span><br><span class="line">print(<span class="string">'net2'</span>, net2)</span><br><span class="line">print(<span class="string">'net3'</span>, net3)</span><br></pre></td></tr></table></figure><pre><code>net1 Sequential(  (conv): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))  (batchnorm): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  (activation_layer): ReLU())net2 Sequential(  (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))  (1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  (2): ReLU())net3 Sequential(  (conv1): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))  (bn1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  (relu1): ReLU())</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可根据名字或序号取出子module</span></span><br><span class="line">net1.conv, net2[<span class="number">0</span>], net3.conv1</span><br></pre></td></tr></table></figure><pre><code>(Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1)), Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1)), Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1)))</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">input = V(t.rand(<span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">net1(input), net2(input), net3(input), net3.relu1(net2[<span class="number">1</span>](net1.conv(input)))</span><br></pre></td></tr></table></figure><pre><code>(tensor([[[[0.0000, 1.4727],           [0.1600, 0.0000]],          [[0.0000, 0.0000],           [0.7015, 1.1069]],          [[1.7189, 0.0000],           [0.0000, 0.0000]]]], grad_fn=&lt;ReluBackward0&gt;), tensor([[[[0.0000, 1.6957],           [0.0000, 0.0000]],          [[1.2454, 0.6350],           [0.0000, 0.0000]],          [[1.0204, 0.4811],           [0.1430, 0.0000]]]], grad_fn=&lt;ReluBackward0&gt;), tensor([[[[0.0000, 1.5585],           [0.1751, 0.0000]],          [[0.0000, 1.4177],           [0.1846, 0.0000]],          [[0.0000, 0.0000],           [1.3537, 0.2417]]]], grad_fn=&lt;ReluBackward0&gt;), tensor([[[[0.0000, 1.4727],           [0.1600, 0.0000]],          [[0.0000, 0.0000],           [0.7015, 1.1069]],          [[1.7189, 0.0000],           [0.0000, 0.0000]]]], grad_fn=&lt;ReluBackward0&gt;))</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">modulelist = nn.ModuleList([nn.Linear(<span class="number">3</span>, <span class="number">4</span>), nn.ReLU(), nn.Linear(<span class="number">4</span>, <span class="number">2</span>)])</span><br><span class="line">input = V(t.rand(<span class="number">1</span>, <span class="number">3</span>))</span><br><span class="line"><span class="keyword">for</span> model <span class="keyword">in</span> modulelist:</span><br><span class="line">    input = model(input)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面会报错，因为modellist没有实现forward方法</span></span><br><span class="line"><span class="comment"># output = modellist(input)</span></span><br></pre></td></tr></table></figure><p>为何不直接使用python中自带的list，而非要多次一举呢？<br>这是因为ModuleList是Module的子类，当在Module中使用它时，就能自动识别为子module。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyModule</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(MyModule, self).__init__()</span><br><span class="line">        self.list = [nn.Linear(<span class="number">3</span>,<span class="number">4</span>), nn.ReLU()] <span class="comment"># 直接用list</span></span><br><span class="line">        self.module_list = nn.ModuleList([nn.Conv2d(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>), nn.ReLU()]) <span class="comment"># 用nn.ModuleList</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    </span><br><span class="line">model = MyModule()</span><br><span class="line">model</span><br></pre></td></tr></table></figure><pre><code>MyModule(  (module_list): ModuleList(    (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))    (1): ReLU()  ))</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">    </span><br><span class="line">    print(name, param.size())</span><br></pre></td></tr></table></figure><pre><code>module_list.0.weight torch.Size([3, 3, 3, 3])module_list.0.bias torch.Size([3])</code></pre><p>可见，list中的子module并不能被主module识别，而ModuleList中的子module能够被主module识别。</p><p>除ModuleList之外还有ParameterList，它是一个可以包含多个parameter的类list对象。在实际应用中，使用方式和ModuleList类似。在构造函数__init__中用到list、tuple、dict等对象，一定要思考是否应该用ModuleList或ParameterList代替。</p><h5 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h5><p>在深度学习中会用到各种各样的损失函数，这些损失函数可看作是一种特殊的layer，pytorch也将这些损失函数实现为nn.Module的子类。然而在实际使用中通常将这些损失函数专门提取出来，作为独立的一部分。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#batch_size = 3, 计算对应每个类别的分数</span></span><br><span class="line">score = V(t.randn(<span class="number">3</span>, <span class="number">10</span>))  <span class="comment"># (N, C) N是batch_size，C是class的个数</span></span><br><span class="line"><span class="comment"># 三个样本分别属于1， 0， 1类，label必须是LongTensor</span></span><br><span class="line">label = V(t.Tensor([<span class="number">1</span>, <span class="number">0</span>, <span class="number">9</span>])).long()</span><br><span class="line"></span><br><span class="line"><span class="comment"># loss与普通的layer无差异</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">loss = criterion(score, label)</span><br><span class="line">loss</span><br></pre></td></tr></table></figure><pre><code>tensor(2.8392)</code></pre><h4 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h4><p>pytorch将深度学习中常用的优化方法全部封装到torch.optim中，其设计十分灵活，能够很方便地扩展自定义的优化方法。<br>所有的优化方法都是继承基类optim.Optimizer，并实现了自己的优化步骤。下面就以最基本的优化方法————随机梯度下降法（SGD）举例说明。这里需要重点掌握：</p><ul><li>优化方法的基本使用方法</li><li>如何对模型的不同部分设置不同的学习率</li><li>如何调整学习率</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先定义一个LeNet网络</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.features = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        self.classifier = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">120</span>, <span class="number">84</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">84</span>, <span class="number">10</span>),</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.features(x)</span><br><span class="line">        x = x.view(<span class="number">-1</span>, <span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>)</span><br><span class="line">        x = self.classifier(x)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">net = Net()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line">optimizer = optim.SGD(params = net.parameters(), lr = <span class="number">1</span>)</span><br><span class="line">optimizer.zero_grad() <span class="comment"># 梯度清零，等价于net.zero_grad()</span></span><br><span class="line"></span><br><span class="line">input = V(t.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>))</span><br><span class="line">output = net(input)</span><br><span class="line">output.backward(output) <span class="comment"># fake backward</span></span><br><span class="line"></span><br><span class="line">optimizer.step() <span class="comment"># 执行优化</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 为不同子网络参数设置不同的学习率，在finetune中经常用到</span></span><br><span class="line"><span class="comment"># 如果对某个参数不指定学习率，就是用默认学习率</span></span><br><span class="line">optimizer = optim.SGD([&#123;<span class="string">'params'</span>:net.features.parameters()&#125;,&#123;<span class="string">'params'</span>:net.classifier.parameters(), <span class="string">'lr'</span>:<span class="number">1e-2</span>&#125;], lr = <span class="number">1e-5</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  只为两个全连接层设置较大的学习率，其余层的学习率较小</span></span><br><span class="line">special_layers = nn.ModuleList([net.classifier[<span class="number">0</span>],net.classifier[<span class="number">2</span>]])</span><br><span class="line">special_layers_params = list(map(id, special_layers.parameters()))</span><br><span class="line">base_params = filter(<span class="keyword">lambda</span> p: id(p) <span class="keyword">not</span> <span class="keyword">in</span> special_layers_params, net.parameters())</span><br><span class="line"></span><br><span class="line">optimizer = t.optim.SGD([</span><br><span class="line">    &#123;<span class="string">'params'</span>:base_params&#125;,</span><br><span class="line">    &#123;<span class="string">'params'</span>:special_layers.parameters(), <span class="string">'lr'</span>: <span class="number">0.01</span>&#125;</span><br><span class="line">], lr = <span class="number">0.001</span>)</span><br></pre></td></tr></table></figure><p>调整学习率主要有两种做法。一种是修改optimmizer.param_groups中对应的学习率。另一种是新建优化器（更简单也是更推荐的做法），由于optimizer十分轻量级，构建开销很小，故可以构建新的optimizer。但是新建优化器会重新初始化动量等状态信息，这对使用动量的优化器来说（如带momentum的sgd），可能会造成损失函数在收敛过程中出震荡。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 调整学习率，新建一个optimizer</span></span><br><span class="line">old_lr = <span class="number">0.1</span></span><br><span class="line">optimizer = optim.SGD([</span><br><span class="line">    &#123;<span class="string">'params'</span>: net.features.parameters()&#125;,</span><br><span class="line">    &#123;<span class="string">'params'</span>: net.classifier.parameters(), <span class="string">'lr'</span>:old_lr *<span class="number">0.1</span>&#125;</span><br><span class="line">],lr = <span class="number">1e-5</span>)</span><br></pre></td></tr></table></figure><h4 id="nn-functional"><a href="#nn-functional" class="headerlink" title="nn.functional"></a>nn.functional</h4><p>nn中还有一个常用的模块：nn.functional。nn中的大多数layer在functional中都有一个与之相对应的函数。<br>nn.functional中的函数和nn.Module的主要区别在于，用nn.Module实现的layers是一个特殊的类，都是由class Layer(nn.Module)定义，会自动提取可学习的参数；而nn.functional中的函数更像是纯函数，由def function(input)定义。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">input = V(t.randn(<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">model = nn.Linear(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">output1 = model(input)</span><br><span class="line">output2 = nn.functional.linear(input, model.weight, model.bias)</span><br><span class="line">output1 == output2</span><br></pre></td></tr></table></figure><pre><code>tensor([[True, True, True, True],        [True, True, True, True]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">b = nn.functional.relu(input)</span><br><span class="line">b2 = nn.ReLU()(input)</span><br><span class="line">b == b2</span><br></pre></td></tr></table></figure><pre><code>tensor([[True, True, True],        [True, True, True]])</code></pre><p>应该什么时候使用nn.Module，什么时候使用nn.functional？<br>如果模型有可学习的参数，最好用nn.Module，否则既可以使用nn.functional也可以使用nn.Module，二者在性能上没有太大差异，具体的使用方式取决于个人喜好。<br>但建议还是使用nn.Dropout而不是nn.functional.dropout，因为dropout在训练和测试两个阶段的行为有所差别，使用nn.Module对象能够通过model.eval操作以一区分。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv1(x)), <span class="number">2</span>)</span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv2(x)), <span class="number">2</span>)</span><br><span class="line">        x = x.view(<span class="number">-1</span>, <span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">net = Net()</span><br><span class="line">x = t.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line">net(x)</span><br></pre></td></tr></table></figure><pre><code>tensor([[-0.0539,  0.0145,  0.0214,  0.0474, -0.0516,  0.0890,  0.0539,  0.0805,          0.0785, -0.1043]], grad_fn=&lt;AddmmBackward&gt;)</code></pre><p>不具备可学习参数的层（激活层、池化层等），将它们用函数代替，这样可以不用放置在构造函数__init__中。有可学习参数的模块，也可以用functional代替，只不过实现起来较繁琐，需要手动定义参数parameter。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyLinear</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(MyLinear, self).__init__()</span><br><span class="line">        self.weight = nn.Parameter(t.randn(<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">        self.bias = nn.Parameter(t.zeros(<span class="number">3</span>))</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,input)</span>:</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> F.linear(input, self.weight, self.bias)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = t.randn(<span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">linear = MyLinear()</span><br><span class="line">linear(x)</span><br></pre></td></tr></table></figure><pre><code>tensor([[-0.0678,  2.5530,  0.8512]], grad_fn=&lt;AddmmBackward&gt;)</code></pre><h4 id="初始化策略"><a href="#初始化策略" class="headerlink" title="初始化策略"></a>初始化策略</h4><p>在深度学习中参数的初始化十分重要，良好的初始化能使模型收敛更快，并达到更高水平，而糟糕的初始化可能使模型迅速崩溃。pytorch中nn.Module的模块参数都采取了较合理的初始化策略，因此一般不用我们考虑。当然我们可以用自定义的初始化代替系统的默认初始化。自定义初始化尤为重要，因为t.Tensor()返回的是内存中的随机数，很可能会有极大值，这在实际训练网络中会造成溢出或者梯度消失。pytorch中的nn.init模块专门为初始化设计，实现了常用的初始化策略。如果某种初始化策略nn.init不提供，用户也可以自己直接初始化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 利用nn.init初始化</span></span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> init</span><br><span class="line">linear = nn.Linear(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">t.manual_seed(<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 等价于linear.weight.data.normal_(0, std)</span></span><br><span class="line">init.xavier_normal_(linear.weight)</span><br></pre></td></tr></table></figure><pre><code>Parameter containing:tensor([[ 0.3535,  0.1427,  0.0330],        [ 0.3321, -0.2416, -0.0888],        [-0.8140,  0.2040, -0.5493],        [-0.3010, -0.4769, -0.0311]], requires_grad=True)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line">t.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">std = math.sqrt(<span class="number">2</span>)/math.sqrt(<span class="number">7.</span>)</span><br><span class="line">linear.weight.data.normal_(<span class="number">0</span>, std)</span><br></pre></td></tr></table></figure><pre><code>tensor([[ 0.3535,  0.1427,  0.0330],        [ 0.3321, -0.2416, -0.0888],        [-0.8140,  0.2040, -0.5493],        [-0.3010, -0.4769, -0.0311]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对模型的所有参数进行初始化</span></span><br><span class="line"><span class="keyword">for</span> name, params <span class="keyword">in</span> net.named_parameters():</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> name.find(<span class="string">'linear'</span>) != <span class="number">-1</span>:</span><br><span class="line">        print(params[<span class="number">0</span>]) <span class="comment"># weight</span></span><br><span class="line">        print(params[<span class="number">1</span>]) <span class="comment"># bias</span></span><br><span class="line">    <span class="keyword">elif</span> name.find(<span class="string">'conv'</span>) != <span class="number">-1</span>:</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">elif</span> name.find(<span class="string">'norm'</span>) != <span class="number">-1</span>:</span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><h4 id="nn-Module深入分析"><a href="#nn-Module深入分析" class="headerlink" title="nn.Module深入分析"></a>nn.Module深入分析</h4><p>如果想深入地理解nn.Module，研究其原理是很有必要的。首先来看看nn.Module基类的构造函数的源代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">    self._parameters = OrderedDict()</span><br><span class="line">    self._modules = OrderedDict()</span><br><span class="line">    self._buffers = OrderedDict()</span><br><span class="line">    self._backward_hooks = OrderedDict()</span><br><span class="line">    self._forward_hooks = OrderedDict()</span><br><span class="line">    self.training = <span class="literal">True</span></span><br></pre></td></tr></table></figure><p>其中每个属性的解释如下：</p><ul><li>_parameters：字典。保存用户直接设置的parameter，self.param1 = nn.Parameter(t.randn(3, 3))会被检测到，在字典中加入一个key为param，value为对应parameter的item，而self.submodule = nn.Linear(3, 4)中的parameter则不会存于此。</li><li>_modules：子module。通过self.submodule = nn.Linear(3, 4)指定的子module会保存于此。</li><li>_buffers：缓存。如batchnorm使用momentum机制，每次前向传播需用到上一次前向传播的结果。</li><li>_backward_hooks与_forward_hooks：钩子技术，用来提取中间变量，类似variable的hook</li><li>training：BatchNorm与Dropout层在训练阶段和测试阶段中采取的策略不同，通过判断training值决定前向传播策略。</li></ul><p>上述几个属性中，_parameters、_modules和_buffers这三个字典中的键值，都可以通过self.key方式获得，效果等价于self._parameters[‘key’]</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        <span class="comment"># 等价于self.register_parameter('param1', nn.Parameter(t.randn(3, 3)))</span></span><br><span class="line">        self.param1 = nn.Parameter(t.rand(<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">        self.submodel1 = nn.Linear(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">foward</span><span class="params">(self, input)</span>:</span></span><br><span class="line">        x = self.param1 * input</span><br><span class="line">        x = self.submodel1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">net = Net()</span><br><span class="line">net</span><br></pre></td></tr></table></figure><pre><code>Net(  (submodel1): Linear(in_features=3, out_features=4, bias=True))</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net._modules</span><br></pre></td></tr></table></figure><pre><code>OrderedDict([(&apos;submodel1&apos;, Linear(in_features=3, out_features=4, bias=True))])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net._parameters</span><br></pre></td></tr></table></figure><pre><code>OrderedDict([(&apos;param1&apos;, Parameter containing:              tensor([[0.3398, 0.5239, 0.7981],                      [0.7718, 0.0112, 0.8100],                      [0.6397, 0.9743, 0.8300]], requires_grad=True))])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net.param1 == net._parameters[<span class="string">'param1'</span>]</span><br></pre></td></tr></table></figure><pre><code>tensor([[True, True, True],        [True, True, True],        [True, True, True]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> net.named_parameters():</span><br><span class="line">    print(name, param.size())</span><br></pre></td></tr></table></figure><pre><code>param1 torch.Size([3, 3])submodel1.weight torch.Size([4, 3])submodel1.bias torch.Size([4])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> name, submodel <span class="keyword">in</span> net.named_modules():</span><br><span class="line">    print(name, submodel)</span><br></pre></td></tr></table></figure><pre><code> Net(  (submodel1): Linear(in_features=3, out_features=4, bias=True))submodel1 Linear(in_features=3, out_features=4, bias=True)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">bn = nn.BatchNorm1d(<span class="number">2</span>)</span><br><span class="line">input = V(t.rand(<span class="number">3</span>,<span class="number">2</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">output = bn(input)</span><br><span class="line">bn._buffers</span><br></pre></td></tr></table></figure><pre><code>OrderedDict([(&apos;running_mean&apos;, tensor([0.0362, 0.0596])),             (&apos;running_var&apos;, tensor([0.9009, 0.9262])),             (&apos;num_batches_tracked&apos;, tensor(1))])</code></pre><p>nn.Module在实际使用中可能层层嵌套，一个module包含若干个子module，每一个子module又包含了更多的子module。为了方便用户访问各个子module，nn.Module实现了很多方法，如函数children可以查看直接子module，函数modules可以查看所有的子module（包括当前module）。与之相对应的还有函数named_children和named_modules，其能够在返回module列表的同时返回它们的名字。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">input = V(t.arange(<span class="number">0</span>, <span class="number">12</span>).view(<span class="number">3</span>, <span class="number">4</span>).float())</span><br><span class="line">model = nn.Dropout()</span><br><span class="line">model(input)</span><br></pre></td></tr></table></figure><pre><code>tensor([[ 0.,  0.,  0.,  0.],        [ 0.,  0., 12.,  0.],        [ 0., 18.,  0.,  0.]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model.training = <span class="literal">False</span></span><br><span class="line">model(input)</span><br></pre></td></tr></table></figure><pre><code>tensor([[ 0.,  1.,  2.,  3.],        [ 4.,  5.,  6.,  7.],        [ 8.,  9., 10., 11.]])</code></pre><p>对batchnorm、dropout、instancenorm等在训练和测试阶段行为差距较大的层，如果在测试时不将其training值设为False，则可能会有很大影响，这在实际使用中千万注意。虽然可通过直接设置training属性将子module设为train和eval模式，但是这种方式比较繁琐。推荐的做法是调用model.train()函数，它会将当前module及其子module中的所有training属性都设为True。model.eval()函数会把training属性都设为False。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(net.training, net.submodel1.training)</span><br><span class="line">net.eval()</span><br><span class="line">net.training, net.submodel1.training</span><br></pre></td></tr></table></figure><pre><code>True True(False, False)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">list(net.named_modules())</span><br></pre></td></tr></table></figure><pre><code>[(&apos;&apos;, Net(    (submodel1): Linear(in_features=3, out_features=4, bias=True)  )), (&apos;submodel1&apos;, Linear(in_features=3, out_features=4, bias=True))]</code></pre><p>register_forward_hook和register_backward_hook函数的功能类似于variable的register_hook，可在module前向传播或反向传播时注册钩子。每次前向传播执行结束后会执行钩子函数（hook）。前向传播的钩子函数具有如下形式：hook(module, input, output) -&gt; None，而反向传播则具有如下形式：hook(module, grad_input, grad_ouput) -&gt; Tensor or None。钩子函数不应修改输入和输出，并且在使用后应及时删除，以避免每次都运行钩子增加运行负载。钩子函数主要用在获取某些中间结果的情景，如中间某一层的输出或某一层的梯度。这些结果本应写在forward函数中，但如果在forward函数中加上这些处理，可能会使处理逻辑比较复杂，这时使用钩子技术就更合适。下面考虑一种场景：有一个预训练的模型，需要提取模型的某一层（不是最后一层）的输出作为特征进行分类，希望不修改其原有的模型定义文件，这时就可以利用钩子函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">model = VGG()</span><br><span class="line">features = t.Tensor()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hook</span><span class="params">(module, input, output)</span>:</span></span><br><span class="line">    features.copy_(output.data)</span><br><span class="line">    </span><br><span class="line">handle = model.layer8.register_forward_hook(hook)</span><br><span class="line">_ = model(input)</span><br><span class="line"><span class="comment"># 用完hook后删除</span></span><br><span class="line">handle.remove()</span><br></pre></td></tr></table></figure><p>nn.Module对象在构造函数中的行为看起来有些诡异，想要理解就需要看两个魔法方法__getattr__和__setattr__。在python中有两个常用的builtin方法：getattr和setattr。getattr(obj, ‘attr1’)等价于obj.attr，setattr(obj, ‘name’, value)等价于obj.name = value。</p><ul><li>result = obj.name会调用builtin函数getattr(obj, ‘name’)，如果该属性找不到，会调用obj.__getattr__(‘name’)</li><li>obj.name = value会调用builtin函数setattr(obj, ‘name’, value)，如果obj对象实现了__setattr__方法，setattr会直接调用obj.__setattr__(‘name’, value)。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">person</span><span class="params">()</span>:</span></span><br><span class="line">    dict = &#123;<span class="string">'name'</span>:<span class="string">'xxx'</span>,<span class="string">'sex'</span>:<span class="string">'boy'</span>,<span class="string">'age'</span>:<span class="number">18</span>&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getattr__</span><span class="params">(self,name)</span>:</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> self.dict[name]</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__setattr__</span><span class="params">(self, name , value)</span>:</span></span><br><span class="line">        </span><br><span class="line">        self.dict[name] = value</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> value</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">one = person()</span><br><span class="line">one.name, one.sex, one.age</span><br></pre></td></tr></table></figure><pre><code>(&apos;xxx&apos;, &apos;boy&apos;, 18)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">one.name = <span class="string">'吴彦祖'</span>;one.name</span><br></pre></td></tr></table></figure><pre><code>&apos;吴彦祖&apos;</code></pre><p>nn.Module实现了自定义的__setattr__函数，当执行module.name=value时，会在__setattr__中判断value是否为Parameter或nn.Module对象，如果是则将这些对象加到_parameters和_modules两个字典中；如果是其他类型的对象，如Variable、list、dict等，则调用默认的操作，将这个值保存在__dict__中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">module = nn.Module()</span><br><span class="line">module.param = nn.Parameter(t.ones(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">module._parameters,module.param</span><br></pre></td></tr></table></figure><pre><code>(OrderedDict([(&apos;param&apos;, Parameter containing:               tensor([[1., 1.],                       [1., 1.]], requires_grad=True))]), Parameter containing: tensor([[1., 1.],         [1., 1.]], requires_grad=True))</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">submodule1 = nn.Linear(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">submodule2 = nn.Linear(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">module_list = [submodule1, submodule2]</span><br><span class="line"><span class="comment"># 对于list对象，调用builtin函数，保存在__dict__中</span></span><br><span class="line">module.submodules = module_list</span><br><span class="line">print(<span class="string">'_modules:'</span>,module._modules)</span><br><span class="line">print(<span class="string">"__dict__['submodules']:"</span>,module.__dict__.get(<span class="string">'submodules'</span>))</span><br></pre></td></tr></table></figure><pre><code>_modules: OrderedDict()__dict__[&apos;submodules&apos;]: [Linear(in_features=2, out_features=2, bias=True), Linear(in_features=2, out_features=2, bias=True)]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">module.submodules = nn.ModuleList(module_list)</span><br><span class="line">print(<span class="string">'_modules:'</span>,module._modules)</span><br><span class="line">print(<span class="string">"__dict__['submodules']:"</span>,module.__dict__.get(<span class="string">'submodules'</span>))</span><br></pre></td></tr></table></figure><pre><code>_modules: OrderedDict([(&apos;submodules&apos;, ModuleList(  (0): Linear(in_features=2, out_features=2, bias=True)  (1): Linear(in_features=2, out_features=2, bias=True)))])__dict__[&apos;submodules&apos;]: None</code></pre><p>因_modules和_parameters中的item未保存在__dict__中，所以默认的getattr方法无法获取它，因而nn.Module实现了自定义的__getattr__方法。如果默认的getattr无法处理，就调用自定义的__getattr__方法，尝试从_modules、_parameters和_buffers三个字典中获取。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">getattr(module, <span class="string">'training'</span>) <span class="comment"># 等价于module.training</span></span><br><span class="line"><span class="comment">#error</span></span><br><span class="line"><span class="comment">#module.__getattr__('training')</span></span><br></pre></td></tr></table></figure><pre><code>True</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">module.attr1 = <span class="number">2</span></span><br><span class="line">getattr(module, <span class="string">'attr1'</span>)</span><br><span class="line"><span class="comment"># 报错</span></span><br><span class="line"><span class="comment"># module.__getattr__('attr1')</span></span><br></pre></td></tr></table></figure><pre><code>2</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">getattr(module, <span class="string">'param'</span>)</span><br></pre></td></tr></table></figure><pre><code>Parameter containing:tensor([[1., 1.],        [1., 1.]], requires_grad=True)</code></pre><p>在pytorch中保存模型十分简单，所有的module对象都具有state_dict()函数，返回当前Module所有的状态数据。将这些状态数据保存后，下次使用模型时即可利用model.load_state_dict()函数将状态加载进来。优化器（optimizer）也有类似机制，不过一般并不需要保存优化器的运行状态。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 保存模型</span></span><br><span class="line">t.save(net.state_dict(), <span class="string">'net.pth'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载已保存的模型</span></span><br><span class="line">net2 = Net()</span><br><span class="line">net2.load_state_dict(t.load(<span class="string">'net.pth'</span>))</span><br></pre></td></tr></table></figure><pre><code>&lt;All keys matched successfully&gt;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">t.save(net, <span class="string">'net_all.pth'</span>)</span><br><span class="line">net2 = t.load(<span class="string">'net_all.pth'</span>)</span><br><span class="line">net2</span><br></pre></td></tr></table></figure><pre><code>/usr/local/lib/python3.6/dist-packages/torch/serialization.py:256: UserWarning: Couldn&apos;t retrieve source code for container of type Net. It won&apos;t be checked for correctness upon loading.  &quot;type &quot; + obj.__name__ + &quot;. It won&apos;t be checked &quot;Net(  (submodel1): Linear(in_features=3, out_features=4, bias=True))</code></pre><p>将Module放在GPU上运行也十分简单，只需一下两步。</p><ul><li>model = model.cuda():将模型的所有参数转存到GPU</li><li>input.cuda():将输入数据放置到GPU上。</li></ul><p>至于如何在多个GPU上并行计算，pytorch也提供了两个函数，可实现简单高效的并行GPU计算。</p><ul><li>nn.parallel.data_parallel(module, inputs, device_ids = None, output_device = None, dim = 0, module_kwargs = None)</li><li>class torch.nn.DataParallel(module, device_ids = None, output_device = None, dim = 0)</li></ul><p>可见二者的参数十分相似，通过device_ids参数可以指定在哪些GPU上进行优化，output_device指定输出到哪个GPU上。唯一的不同在于前者直接利用多GPU并行计算得出结果，后者则返回一个新的module，能够自动在多GPU上进行并行加速。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># method1</span></span><br><span class="line">new_net = nn.DataParallel(net, device_ids = [<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">output = new_net(input)</span><br><span class="line"></span><br><span class="line"><span class="comment"># method 2</span></span><br><span class="line">output = nn.parallel.data_parallel(net, input, device_ids = [<span class="number">0</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure><p>DataParallel并行的方式，是将输入一个batch的数据均分成多份，分别送到对应的GPU进行计算，然后将各个GPU得到的梯度相加。与Module相关的所有数据也会以浅复制的方式复制多份。</p><h4 id="nn和aautograd的关系"><a href="#nn和aautograd的关系" class="headerlink" title="nn和aautograd的关系"></a>nn和aautograd的关系</h4><p>nn.Module利用的是autograd技术，其主要工作是实现前向传播。在forward函数中，nn.Module对输入的Variable进行的各种操作，本质上都用了autograd技术。这里需要对比autograd.Function和nn.Module之间的区别。</p><ul><li>autograd.Function利用Tensor对autograd技术的扩展，为autograd实现了新的运算op，不仅要实现前向传播还要手动实现反向传播。</li><li>nn.Module利用了autograd技术，对nn的功能进行扩展，实现了深度学习中更多的层。只需实现前向传播功能，autograd即会自动实现反向传播。</li><li>nn.functional是一些autograd操作的集合，是经过封装的函数。</li></ul><p>作为两种扩充pytorch接口的方法，我们在实际作用中应该如何选择？如果某一个操作在autograd中尚未支持，那么需要利用Function手动实现对应的前后传播和反向传播。如果某些时候利用autograd接口比较复杂，则可以利用Function将多个操作聚合，实现优化，比直接利用autograd低级别的操作要快。如果只是想在深度学习中怎加某一层，使用nn.Module进行封装则更简单高效。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;神经网络工具箱nn&quot;&gt;&lt;a href=&quot;#神经网络工具箱nn&quot; class=&quot;headerlink&quot; title=&quot;神经网络工具箱nn&quot;&gt;&lt;/a&gt;神经网络工具箱nn&lt;/h3&gt;&lt;p&gt;autograd实现了自动微分系统，然而对于深度学习来说过于底层。nn模块是构建与a
      
    
    </summary>
    
      <category term="笔记" scheme="https://zinw623.github.io/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="pytorch" scheme="https://zinw623.github.io/categories/%E7%AC%94%E8%AE%B0/pytorch/"/>
    
    
      <category term="pytorch" scheme="https://zinw623.github.io/tags/pytorch/"/>
    
      <category term="笔记" scheme="https://zinw623.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>【notes】pytorch学习笔记2-autograd部分</title>
    <link href="https://zinw623.github.io/2019/08/20/autograd/"/>
    <id>https://zinw623.github.io/2019/08/20/autograd/</id>
    <published>2019-08-20T09:30:45.000Z</published>
    <updated>2019-08-20T09:24:42.211Z</updated>
    
    <content type="html"><![CDATA[<h3 id="autograd"><a href="#autograd" class="headerlink" title="autograd"></a>autograd</h3><p>torch.autograd是为方便用户使用，专门开发的一套自动求导引擎，它能够根据输入和前向传播过程自动构建计算图，并执行反向传播。</p><p>计算图是现代深度学习框架的核心，它为自动求导算法——反向传播提供了理论支持。</p><h4 id="Variable"><a href="#Variable" class="headerlink" title="Variable"></a>Variable</h4><p>pytorch在autograd模块中实现计算图相关功能，autograd中的核心数据结构是Variable。Variable封装了tensor，并记录对tensor的操作记录用来构建计算图。Variale的数据结构如图：</p><pre class="mermaid" style="text-align: center;">                        graph LR;A[autograd.Variable] --> B(data);A[autograd.Variable] --> C(grad);A[autograd.Variable] --> D(grad_fn);          </pre><p>Variable的构造函数需要传入tensor，同时有两个可选参数。</p><ul><li>require_grad(bool)：是否需要对该variable进行求导。</li><li>volatile(bool)：意为“挥发”，设置为True，构建在该variable之上的图都不会求导，转为推理阶段设计。</li></ul><p>Variable支持大部分tensor支持的函数，但其不支持部分inplace函数，因为这些函数会修改tensor自身，而在反向传播中，variable需要缓存原来的tensor来计算梯度。如果想要计算各个Variable的梯度，只需调用根节点variable的backward方法，autograd会自动沿着计算图反向传播，计算每一个叶子节点的梯度。</p><ul><li>grad_variables：形状与variable一致，对于y.backward()，grad_variables相当于链式法则$\frac{\partial{z}}{\partial{x}}=\frac{\partial{z}}{\partial{y}}\times\frac{\partial{y}}{\partial{x}}$中的$\frac{\partial{z}}{\partial{y}}$。grad_variables也可以是tensor或序列。</li></ul><ul><li>retain_graph：反向传播需要缓存一些中间结果，反向传播之后，这些缓存就被清空，可通过指定这个参数不清空缓存，用来多次反向传播。</li></ul><ul><li>create_graph：对反向传播过程再次构建计算图，可通过backward of backward实现求高阶导数。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable <span class="keyword">as</span> V</span><br><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从tensor中创建variable，指定需要求导</span></span><br><span class="line"></span><br><span class="line">a = V(t.ones(<span class="number">3</span>, <span class="number">4</span>), requires_grad = <span class="literal">True</span>);a</span><br></pre></td></tr></table></figure><pre><code>tensor([[1., 1., 1., 1.],        [1., 1., 1., 1.],        [1., 1., 1., 1.]], requires_grad=True)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">b = V(t.zeros(<span class="number">3</span>, <span class="number">4</span>));b</span><br></pre></td></tr></table></figure><pre><code>tensor([[0., 0., 0., 0.],        [0., 0., 0., 0.],        [0., 0., 0., 0.]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 函数的使用和tensor一致</span></span><br><span class="line"><span class="comment"># 也可写成c = a + b</span></span><br><span class="line">c = a.add(b)</span><br><span class="line">c</span><br></pre></td></tr></table></figure><pre><code>tensor([[1., 1., 1., 1.],        [1., 1., 1., 1.],        [1., 1., 1., 1.]], grad_fn=&lt;AddBackward0&gt;)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">d = c.sum()</span><br><span class="line">d.backward() <span class="comment"># 反向传播</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 注意二者的区别</span></span><br><span class="line"><span class="comment"># 前者在取data后变为tensor，从tensor计算sum得到float</span></span><br><span class="line"><span class="comment"># 后者计算sum后仍然是Variable</span></span><br><span class="line">c.data.sum(), c.sum()</span><br></pre></td></tr></table></figure><pre><code>(tensor(12.), tensor(12., grad_fn=&lt;SumBackward0&gt;))</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a.grad</span><br></pre></td></tr></table></figure><pre><code>tensor([[1., 1., 1., 1.],        [1., 1., 1., 1.],        [1., 1., 1., 1.]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 此处虽然没有指定c需要求导，但c依赖于a，而a需要求导</span></span><br><span class="line"><span class="comment"># 因此c的requires_grad属性会自动设为True</span></span><br><span class="line">a.requires_grad, b.requires_grad, c.requires_grad</span><br></pre></td></tr></table></figure><pre><code>(True, False, True)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># c.grad是None，c不是叶子节点，他的梯度是用来计算a的梯度</span></span><br><span class="line"><span class="comment"># 虽然c.requires_grad = True，但其梯度计算完之后即被释放</span></span><br><span class="line">c.grad <span class="keyword">is</span> <span class="literal">None</span></span><br></pre></td></tr></table></figure><pre><code>True</code></pre><p>接下来看看autograd计算导数和我们手动推导的导数的区别。<br>$$y=x^2e^x$$</p><p>它的导函数是：<br>$$\frac{\partial{y}}{\partial{x}}=2xe^x+x^2e^x$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">    y = x**<span class="number">2</span> * t.exp(x)</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradf</span><span class="params">(x)</span>:</span></span><br><span class="line">    dx = <span class="number">2</span>*x*t.exp(x) + x**<span class="number">2</span>*t.exp(x)</span><br><span class="line">    <span class="keyword">return</span> dx</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = V(t.randn(<span class="number">3</span>, <span class="number">4</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">y = f(x)</span><br><span class="line">y</span><br></pre></td></tr></table></figure><pre><code>tensor([[1.3949e-01, 2.7201e-01, 4.9848e-01, 2.2968e+00],        [3.2033e-01, 3.3618e-01, 2.3554e-02, 1.0507e+01],        [3.9416e+01, 3.5322e+00, 9.6847e-02, 1.2743e+01]],       grad_fn=&lt;MulBackward0&gt;)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y.backward(t.ones(y.size())) <span class="comment"># grad_variables形状与y一致</span></span><br><span class="line">x.grad</span><br></pre></td></tr></table></figure><pre><code>tensor([[ 1.0154, -0.4398, -0.1755,  7.1583],        [-0.4095,  1.7961,  0.3532, 24.3531],        [76.1412, 10.0143, -0.4190, 28.6505]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># autograd的计算结果与利用公式手动计算的结果一致</span></span><br><span class="line">gradf(x)</span><br></pre></td></tr></table></figure><pre><code>tensor([[ 1.0154, -0.4398, -0.1755,  7.1583],        [-0.4095,  1.7961,  0.3532, 24.3531],        [76.1412, 10.0143, -0.4190, 28.6505]], grad_fn=&lt;AddBackward0&gt;)</code></pre><h4 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h4><p>pytorch中autograd的底层采用了计算图，计算图是一种特殊的有向无环图（DAG），用于记录算子与变量之间的关系。一般用矩形表示算子，椭圆形表示变量。如表达式$\boldsymbol{z}=\boldsymbol{wx}+\boldsymbol{b}$可分解为$\boldsymbol{y}=\boldsymbol{wx}$和$\boldsymbol{z}=\boldsymbol{y}+\boldsymbol{b}$，其计算图如图所示，图中的MUL和ADD都是算子，$\boldsymbol{w}$、$\boldsymbol{x}$、$\boldsymbol{b}$为变量。</p><pre class="mermaid" style="text-align: center;">                        graph LR;A((W)) --> C[MUL];B((X)) --> C[MUL];C[MUL] --> D((y));E((b)) --> F[Add];D((y)) --> F[Add];F[Add] --> G((z));          </pre><p>如上有向无环图，$\boldsymbol{X}$和$\boldsymbol{b}$是叶子节点，这些节点通常有用户自己创建，不依赖于其他变量。$\boldsymbol{z}$称为根节点，是计算图的最终目标。利用链式法则很容易求各个叶子节点的梯度。</p><p>$$ \frac{\partial{z}}{\partial{b}}=1,\frac{\partial{z}}{\partial{y}}=1\\frac{\partial{y}}{\partial{w}}=x,\frac{\partial{y}}{\partial{x}}=w\\frac{\partial{z}}{\partial{x}}=\frac{\partial{z}}{\partial{y}}\frac{\partial{y}}{\partial{x}}=1<em>w\\frac{\partial{z}}{\partial{w}}=\frac{\partial{z}}{\partial{y}}\frac{\partial{y}}{\partial{w}}=1</em>x $$</p><p>有了计算图上述链式求导可自动利用计算图的反向传播自动完成，其过程如图所示：</p><pre class="mermaid" style="text-align: center;">                        graph LR;A((dz)) --> B[addBackward];B[addBackward] --> C((dy));B[addBackward] --> D((db));C((dy)) --> E[mulBackward];E[mulBackward] --> F((dX));E[mulBackward] --> G((dW));          </pre><p>图中记录了操作function，每个变量在图中的位置可通过其grad_fn属性在图中的位置推测得到。在反向传播的过程中，autograd沿着这个图从当前变量（根节点z）溯源，可以利用链式求导法则计算所有叶子节点的梯度。</p><p>每一个前向传播操作的函数都有与之对应的反向传播函数用来计算输入的各个variable的梯度，这些函数的函数名通常以Backward结尾。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = V(t.ones(<span class="number">1</span>))</span><br><span class="line">b = V(t.rand(<span class="number">1</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">w = V(t.rand(<span class="number">1</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">y = w * x <span class="comment"># 等价于y = w.mul(x)</span></span><br><span class="line">z = y + b <span class="comment"># 等价于z = y.add(b)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.requires_grad, b.requires_grad, w.requires_grad</span><br></pre></td></tr></table></figure><pre><code>(False, True, True)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 虽然未指定y.requires_grad为True，但由于y依赖于需要求导的w</span></span><br><span class="line"><span class="comment"># 故而y.requires_grad为True</span></span><br><span class="line">y.requires_grad</span><br></pre></td></tr></table></figure><pre><code>True</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.is_leaf, w.is_leaf, b.is_leaf</span><br></pre></td></tr></table></figure><pre><code>(True, True, True)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y.is_leaf, z.is_leaf</span><br></pre></td></tr></table></figure><pre><code>(False, False)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># grad_fn可以查看这个variable的反向传播函数</span></span><br><span class="line"><span class="comment"># z是add函数的输出，所以它的反向传播函数是AddBackward</span></span><br><span class="line">z.grad_fn</span><br></pre></td></tr></table></figure><pre><code>&lt;AddBackward0 at 0x7f073e390e80&gt;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#next_function保存grad_fn的输入，grad_fn的输入是一个tuple</span></span><br><span class="line"><span class="comment"># 第一个是y，它是乘法（mul）的输出，所以对应的反向传播函数y.grad_fn是MulBackward</span></span><br><span class="line"><span class="comment"># 第二个是b，它是叶子节点，由用户创建，grad_fn为None，但是有</span></span><br><span class="line">z.grad_fn.next_functions</span><br></pre></td></tr></table></figure><pre><code>((&lt;MulBackward0 at 0x7f073e390828&gt;, 0), (&lt;AccumulateGrad at 0x7f073e390f28&gt;, 0))</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># variable的grad_fn对应着图中的function</span></span><br><span class="line">z.grad_fn.next_functions[<span class="number">0</span>][<span class="number">0</span>] == y.grad_fn</span><br></pre></td></tr></table></figure><pre><code>True</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第一个是w，叶子节点，需要求导，梯度是累加的</span></span><br><span class="line"><span class="comment"># 第二个是x，叶子节点，不需要求导，所以为None</span></span><br><span class="line">y.grad_fn.next_functions</span><br></pre></td></tr></table></figure><pre><code>((&lt;AccumulateGrad at 0x7f073e390470&gt;, 0), (None, 0))</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 叶子节点的grad_fn是None</span></span><br><span class="line">w.grad_fn, x.grad_fn</span><br></pre></td></tr></table></figure><pre><code>(None, None)</code></pre><p>计算$\boldsymbol{w}$的梯度时需要用到$\boldsymbol{x}$的数值（$\frac{\partial{y}}{\partial{w}}=x$），这些数值在前向过程中会保存成buffer，在计算完梯度之后会自动清空。为了能够多次反向传播需要指定retain_graph来保留这些buffer。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y.grad_fn.saved_variables</span><br></pre></td></tr></table></figure><pre><code>---------------------------------------------------------------------------AttributeError                            Traceback (most recent call last)&lt;ipython-input-30-284a59926bb7&gt; in &lt;module&gt;----&gt; 1 y.grad_fn.saved_variablesAttributeError: &apos;MulBackward0&apos; object has no attribute &apos;saved_variables&apos;</code></pre><p>原因确实是版本问题，PyTorch0.3 中把许多python的操作转移到了C++中，saved_variables 现在是一个c++的对象，无法通过python访问。<a href="https://github.com/chenyuntc/pytorch-book/issues/7" target="_blank" rel="noopener">https://github.com/chenyuntc/pytorch-book/issues/7</a></p><p>可以查看这里进行学习<a href="https://github.com/chenyuntc/pytorch-book/blob/0.3/chapter3-Tensor" target="_blank" rel="noopener">https://github.com/chenyuntc/pytorch-book/blob/0.3/chapter3-Tensor</a>和autograd/Autograd.ipynb,省掉上面的操作</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用retain_graph保存buffer</span></span><br><span class="line">z.backward(retain_graph = <span class="literal">True</span>)</span><br><span class="line">w.grad</span><br></pre></td></tr></table></figure><pre><code>tensor([1.])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 多次反向传播，梯度累加，这也就是w中AccumulateGrad标识的含义</span></span><br><span class="line">z.backward()</span><br><span class="line">w.grad</span><br></pre></td></tr></table></figure><pre><code>tensor([2.])</code></pre><p>pytorch使用的是动态图，它的计算图在每次前向传播时都是从头开始构建的，所以它能够使用python的控制语句（如for、if等），根据需求创建计算图。这一点在自然语言处理领域中很有用，它意为你不需要事先构建所有可能用到的图的路径，图在运行时才构建。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">abs</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> x.data[<span class="number">0</span>] &gt; <span class="number">0</span>: <span class="keyword">return</span> x</span><br><span class="line">    <span class="keyword">else</span>: <span class="keyword">return</span> -x</span><br><span class="line">x = V(t.ones(<span class="number">1</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">y = abs(x)</span><br><span class="line">y.backward()</span><br><span class="line">x.grad</span><br></pre></td></tr></table></figure><pre><code>tensor([1.])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = V(<span class="number">-1</span> * t.ones(<span class="number">1</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">y = abs(x)</span><br><span class="line">y.backward()</span><br><span class="line">print(x.grad)</span><br></pre></td></tr></table></figure><pre><code>tensor([-1.])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">    result = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> ii <span class="keyword">in</span> x:</span><br><span class="line">        <span class="keyword">if</span> ii.data &gt; <span class="number">0</span>: result = ii * result</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line">x = V(t.arange(<span class="number">-2</span>, <span class="number">4</span>).float(), requires_grad = <span class="literal">True</span>)</span><br><span class="line">y = f(x) <span class="comment"># y = x[3] * x[4] * x[5]</span></span><br><span class="line">y.backward()</span><br><span class="line">x.grad</span><br></pre></td></tr></table></figure><pre><code>tensor([0., 0., 0., 6., 3., 2.])</code></pre><p>变量的requires_grad属性默认为False，如果某一个节点requires_grad被设置为True，那么所有依赖它的节点requires_grad都是True。</p><p>with torch.no_grad()内的variable均为不会求导，其优先级高于requires_grad。函数可以用装饰器@torch.no_grad()。可实现一定程度的速度提升，并节省约一半显存，因为其不需要分配空间保存梯度。</p><p>详细内容可见：<a href="https://pytorch.org/docs/master/autograd.html#locally-disable-grad" target="_blank" rel="noopener">https://pytorch.org/docs/master/autograd.html#locally-disable-grad</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = t.tensor([<span class="number">1.</span>], requires_grad = <span class="literal">True</span>)</span><br><span class="line"><span class="keyword">with</span> t.no_grad():</span><br><span class="line">    y = x * <span class="number">2</span></span><br><span class="line">y.requires_grad</span><br></pre></td></tr></table></figure><pre><code>False</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@t.no_grad()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">doubler</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x * <span class="number">2</span></span><br><span class="line">z = doubler(x)</span><br><span class="line">z.requires_grad</span><br></pre></td></tr></table></figure><pre><code>False</code></pre><p>在反向传播过程中非叶子节点的导数计算完之后即被清空。若想查看这些变量的梯度，有以下两种方法：</p><ul><li>使用autograd.grad函数</li><li>使用hook</li></ul><p>推荐使用hook方法，但在实际使用中应尽量避免修改grad的值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = V(t.ones(<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">w = V(t.rand(<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">y = x * w</span><br><span class="line"><span class="comment"># y依赖于w，而w.requires_grad = True</span></span><br><span class="line">z = y.sum()</span><br><span class="line">x.requires_grad, w.requires_grad, y.requires_grad</span><br></pre></td></tr></table></figure><pre><code>(True, True, True)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 非叶子节点grad计算完之后自动清空，y.grad是None</span></span><br><span class="line">z.backward()</span><br><span class="line">x.grad, w.grad, y.grad</span><br></pre></td></tr></table></figure><pre><code>(tensor([0.1283, 0.8326, 0.6539]), tensor([1., 1., 1.]), None)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第一种方法：使用grad获取中间变量的梯度</span></span><br><span class="line">x = V(t.ones(<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">w = V(t.rand(<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">y = x * w</span><br><span class="line"><span class="comment"># y依赖于w，而w.requires_grad = True</span></span><br><span class="line">z = y.sum()</span><br><span class="line"><span class="comment"># z对y的梯度，隐式调用backward()</span></span><br><span class="line">t.autograd.grad(z, y)</span><br></pre></td></tr></table></figure><pre><code>(tensor([1., 1., 1.]),)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第二种方法：使用hook</span></span><br><span class="line"><span class="comment"># hook是一个函数，输入是梯度，不应该有返回值</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">variable_hook</span><span class="params">(grad)</span>:</span></span><br><span class="line">    print(<span class="string">'y的梯度：\r\n'</span>,grad)</span><br><span class="line">    </span><br><span class="line">x = V(t.ones(<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">w = V(t.rand(<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">y = x * w</span><br><span class="line"><span class="comment"># 注册hook</span></span><br><span class="line">hook_handle = y.register_hook(variable_hook)</span><br><span class="line">z = y.sum()</span><br><span class="line">z.backward()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 除非你每次都要用hook，否则用完之后记得移除hook</span></span><br><span class="line">hook_handle.remove()</span><br></pre></td></tr></table></figure><pre><code>y的梯度： tensor([1., 1., 1.])</code></pre><p>最后再来看看variable中grad属性和backward函数grad_variables参数的含义。</p><ul><li>variables $\boldsymbol{x}$ 的梯度是目标函数$f(x)$对$\boldsymbol{x}$的梯度，形状与$\boldsymbol{x}$一致。</li><li>y.backward(grad_variables)中grad_variables相当于链式法则中的$\frac{\partial{z}}{\partial{x}}=\frac{\partial{z}}{\partial{y}}\frac{\partial{y}}{\partial{x}}$中的$\frac{\partial{z}}{\partial{y}}$。z是目标函数，一般是个标量，故而$\frac{\partial{z}}{\partial{y}}$的形状与$\boldsymbol{y}$的形状一致。z.backward()等价于y.backward(grad_y)。而z.backward()省略了grad_variables参数，是因为z是个标量，而$\frac{\partial{z}}{\partial{z}}=1$</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = V(t.arange(<span class="number">0</span>, <span class="number">3</span>).float(), requires_grad = <span class="literal">True</span>)</span><br><span class="line">y = x**<span class="number">2</span> + x*<span class="number">2</span></span><br><span class="line">z = y.sum()</span><br><span class="line">z.backward()</span><br><span class="line">x.grad</span><br></pre></td></tr></table></figure><pre><code>tensor([2., 4., 6.])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = V(t.arange(<span class="number">0</span>, <span class="number">3</span>).float(), requires_grad = <span class="literal">True</span>)</span><br><span class="line">y = x**<span class="number">2</span> + x*<span class="number">2</span></span><br><span class="line">z = y.sum()</span><br><span class="line">y_grad_variables = V(t.Tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])) </span><br><span class="line">y.backward(y_grad_variables)</span><br><span class="line">x.grad</span><br></pre></td></tr></table></figure><pre><code>tensor([2., 4., 6.])</code></pre><p>值得注意的是，只有对variable的操作才能使用autograd，如果variable的data直接进行操作，将无法使用反向传播。除了参数初始化，一般我们不会直接修改variable.data的值。</p><p>在pytorch中计算图的特点总结如下：</p><ul><li>autograd根据用户对variable的操作构建计算图。对variable的操作抽象为Function。</li><li>由用户创建的节点称为叶子节点，叶子节点的grad_fn为None。叶子节点中需要求导的variable，具有AccumulateGrad标识，因其梯度是累加的。</li><li>variable默认是不需要求导的，即requires_grad属性默认为False。如果某一个节点requires_grad被设置为True，那么所有依赖它的节点requires_grad都为True。</li><li>with torch.no_grad和@torch.no_grad()的作用下的节点都不会求导，优先级比requires_grad高。</li><li>多次反向传播时，梯度是累加的。反向传播的中间缓存会被清空，为进行多次反向传播需指定retain_graph=True来保存这些缓存。</li><li>非叶子节点的梯度计算完后即被清空，可以使用autograd.grad或hook技术获取非叶子节点梯度的值。</li><li>variable的grad与data形状一致，应避免直接修改variable.data，因为对data的直接操作无法利用autograd进行反向传播。</li><li>反向传播函数backward的参数grad_variables可以看成链式求导的中间结果，如果是标量，可以省略，默认为1。</li><li>pytorch采用动态图设计，可以很方便地查看中间层的输出，动态地设计计算图结构。</li></ul><h4 id="扩展autograd"><a href="#扩展autograd" class="headerlink" title="扩展autograd"></a>扩展autograd</h4><p>目前绝大多数函数都可以使用autograd实现反向求导，但如果需要自己写一个复杂的函数，不支持自动反向求导怎么办？答案是写一个Function，实现它的前向传播和反向传播代码，Function对应于计算图中的矩形，它接受参数，计算并返回结果。下面给出了一个例子。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Function</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiplyAdd</span><span class="params">(Function)</span>:</span></span><br><span class="line">    </span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(ctx, w, x, b)</span>:</span></span><br><span class="line">        print(<span class="string">'type in forward'</span>, type(x))</span><br><span class="line">        ctx.save_for_backward(w, x)<span class="comment">#存储用来反向传播的参数</span></span><br><span class="line">        output = w*x +b</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(ctx, grad_output)</span>:</span></span><br><span class="line">        w, x = ctx.saved_tensors <span class="comment"># 老版本是saved_variables</span></span><br><span class="line">        print(<span class="string">'type in backward'</span>,type(x))</span><br><span class="line">        grad_w = grad_output * x</span><br><span class="line">        grad_x = grad_output * w</span><br><span class="line">        grad_b = grad_output * <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> grad_w, grad_x, grad_b</span><br></pre></td></tr></table></figure><p>分析如下：</p><ul><li>自定义的Function需要继承autograd.Function，没有构造函数<strong>init</strong>，forward和backward函数都是静态方法</li><li>forward函数的输入和输出都是Tensor，backward函数的输入和输出都是Variable</li><li>backward函数的输出和forward函数的输入一一对应，backward函数的输入和forward函数的输出一一对应</li><li>backward函数的grad_output参数即t.autograd.backward中的grad_variables</li><li>如果某一个输入不需要求导，直接返回None，如forward中的输入参数x_requires_grad显然无法对它求导，直接返回None即可</li><li>反向传播可能需要利用前向传播的某些中间结果，需要进行保存，否则前向传播结束后这些对象即被释放</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">x = V(t.ones(<span class="number">1</span>))</span><br><span class="line">w = V(t.rand(<span class="number">1</span>),requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = V(t.rand(<span class="number">1</span>),requires_grad=<span class="literal">True</span>)</span><br><span class="line">print(<span class="string">'开始前向传播'</span>)</span><br><span class="line">z = MultiplyAdd.apply(w, x, b)</span><br><span class="line">print(<span class="string">'开始反向传播'</span>)</span><br><span class="line">z.backward()</span><br><span class="line"></span><br><span class="line"><span class="comment"># x不需要求导，中间过程还是会计算它的导数，但随后被清空</span></span><br><span class="line">x.grad, w.grad, b.grad</span><br></pre></td></tr></table></figure><pre><code>开始前向传播type in forward &lt;class &apos;torch.Tensor&apos;&gt;开始反向传播type in backward &lt;class &apos;torch.Tensor&apos;&gt;(None, tensor([1.]), tensor([1.]))</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">x = V(t.ones(<span class="number">1</span>))</span><br><span class="line">w = V(t.rand(<span class="number">1</span>),requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = V(t.rand(<span class="number">1</span>),requires_grad=<span class="literal">True</span>)</span><br><span class="line">print(<span class="string">'开始前向传播'</span>)</span><br><span class="line">z = MultiplyAdd.apply(w, x, b)</span><br><span class="line">print(<span class="string">'开始反向传播'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用MultiplyAdd.backward</span></span><br><span class="line"><span class="comment"># 会自动输出grad_w, grad_x, grad_b</span></span><br><span class="line">z.grad_fn.apply(V(t.ones(<span class="number">1</span>)))</span><br></pre></td></tr></table></figure><pre><code>开始前向传播type in forward &lt;class &apos;torch.Tensor&apos;&gt;开始反向传播type in backward &lt;class &apos;torch.Tensor&apos;&gt;(tensor([1.]), tensor([0.5986], grad_fn=&lt;MulBackward0&gt;), tensor([1.]))</code></pre><p>在backward函数里之所以也要对variable进行操作是为了能计算梯度的梯度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = V(t.Tensor([<span class="number">5</span>]), requires_grad = <span class="literal">True</span>)</span><br><span class="line">y = x ** <span class="number">2</span></span><br><span class="line">grad_x = t.autograd.grad(y, x, create_graph = <span class="literal">True</span>)</span><br><span class="line">grad_x</span><br></pre></td></tr></table></figure><pre><code>(tensor([10.], grad_fn=&lt;MulBackward0&gt;),)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grad_grad_x = t.autograd.grad(grad_x[<span class="number">0</span>],x);grad_grad_x</span><br></pre></td></tr></table></figure><pre><code>(tensor([2.]),)</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;autograd&quot;&gt;&lt;a href=&quot;#autograd&quot; class=&quot;headerlink&quot; title=&quot;autograd&quot;&gt;&lt;/a&gt;autograd&lt;/h3&gt;&lt;p&gt;torch.autograd是为方便用户使用，专门开发的一套自动求导引擎，它能够根据输入和前
      
    
    </summary>
    
      <category term="笔记" scheme="https://zinw623.github.io/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="pytorch" scheme="https://zinw623.github.io/categories/%E7%AC%94%E8%AE%B0/pytorch/"/>
    
    
      <category term="pytorch" scheme="https://zinw623.github.io/tags/pytorch/"/>
    
      <category term="笔记" scheme="https://zinw623.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>【notes】pytorch学习笔记1-Tensor部分</title>
    <link href="https://zinw623.github.io/2019/08/19/pytorch-1/"/>
    <id>https://zinw623.github.io/2019/08/19/pytorch-1/</id>
    <published>2019-08-19T10:30:45.000Z</published>
    <updated>2019-08-19T08:49:53.865Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Tensor和autograd"><a href="#Tensor和autograd" class="headerlink" title="Tensor和autograd"></a>Tensor和autograd</h2><blockquote><p>每个深度学习框架的设计核心是张量和计算图，在pytorch里体现为张量系统（Tensor）和自动微分系统（atutograd）。</p></blockquote><h3 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h3><ul><li>中文译为张量，可以简单看作一个数组。</li><li>与numpy里的ndarrays类似，但tensor支持GPU加速。</li></ul><h4 id="基础操作"><a href="#基础操作" class="headerlink" title="基础操作"></a>基础操作</h4><p>接口角度：</p><ol><li>torch.function </li><li>tensor.function</li></ol><p>存储角度：</p><ol><li>不会修改自身数据，如a.add(b),返回一个值为加法结果的新的tensor。</li><li>会修改自身数据，如a.add_(b)，加法的值储存在a中了。</li></ol><h5 id="创建Tensor"><a href="#创建Tensor" class="headerlink" title="创建Tensor"></a>创建Tensor</h5><p>在pytorch中常见的新建tensor的方法：</p><table><thead><tr><th align="left">类别</th><th align="left">特点</th><th align="left">函数</th><th align="left">功能</th></tr></thead><tbody><tr><td align="left">第一类：基础方法</td><td align="left">最灵活</td><td align="left">Tensor(*sizes)</td><td align="left">基础构造函数</td></tr><tr><td align="left">第二类：根据sizes建立</td><td align="left">常数型</td><td align="left">ones(*sizes)</td><td align="left">全1Tensor</td></tr><tr><td align="left"></td><td align="left">常数型</td><td align="left">zeros(*sizes)</td><td align="left">全0Tensor</td></tr><tr><td align="left"></td><td align="left">常数型</td><td align="left">eyes(*sizes)</td><td align="left">对角线为1，其他为0</td></tr><tr><td align="left"></td><td align="left">概率分布型</td><td align="left">rand/randn(*sizes)</td><td align="left">均匀/标准分布</td></tr><tr><td align="left">第三类：在一定范围内建立</td><td align="left">等差数列型</td><td align="left">arange(s,e,step)</td><td align="left">从s到e，步长为step</td></tr><tr><td align="left"></td><td align="left">等差数列型</td><td align="left">linspace(s,e,steps)</td><td align="left">从s到e，均匀切分成steps份</td></tr><tr><td align="left"></td><td align="left">概率分布型</td><td align="left">normal(mean,std)/uniform(from,to)</td><td align="left">正态分布/均匀分布</td></tr><tr><td align="left"></td><td align="left">概率分布型</td><td align="left">randperm(m)</td><td align="left">随机分布</td></tr></tbody></table><ul><li>其中使用Tensor函数新建tensor是最复杂多变的，它既可以接受一个list，并根据list的数据新建tensor，也可根据指定的形状新建tensor，还能传入其他的tensor。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 引入必要的包</span></span><br><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable <span class="keyword">as</span> V</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 指定tensor的形状</span></span><br><span class="line">a = t.Tensor(<span class="number">2</span>, <span class="number">3</span>);a</span><br></pre></td></tr></table></figure><pre><code>tensor([[7.2443e+22, 4.2016e+30, 9.9708e+17],        [7.2296e+31, 5.6015e-02, 4.4721e+21]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用list的数据创建tensor</span></span><br><span class="line">b = t.Tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]]);b</span><br></pre></td></tr></table></figure><pre><code>tensor([[1., 2., 3.],        [4., 5., 6.]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">b.tolist(),type(b.tolist()) <span class="comment"># 把tensor转为list</span></span><br></pre></td></tr></table></figure><pre><code>([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], list)</code></pre><p>tensor.size()返回torch.Size()对象，它是tuple的子类，但其使用方式与tuple略有不同。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">b_size = b.size();b_size</span><br></pre></td></tr></table></figure><pre><code>torch.Size([2, 3])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">b.numel() <span class="comment"># numelements前五个字母，b中元素总个数，等价于b.nelement()</span></span><br></pre></td></tr></table></figure><pre><code>6</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个和b形状一样的tensor</span></span><br><span class="line">c = t.Tensor(b_size)</span><br><span class="line"><span class="comment"># 创建一个元素为2和3的tensor</span></span><br><span class="line">d = t.Tensor((<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">c, d   <span class="comment"># 输出结果不同，明显看出torch.Size()对象和tuple的不同</span></span><br></pre></td></tr></table></figure><pre><code>(tensor([[5.8959e-35, 4.5636e-41, 1.0257e-36],         [0.0000e+00, 5.0000e+00, 6.0000e+00]]), tensor([2., 3.]))</code></pre><p>tensor.shape等价于tensor.size()</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">c.shape</span><br></pre></td></tr></table></figure><pre><code>torch.Size([2, 3])</code></pre><p><strong>* 需要注意：t.Tensor(*sizes)创建tensor时，系统不会马上分配空间，只会计算内存是否够用，使用到tensor时才会分配，而其他方法是创建后会立马分配空间。 *</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t.ones(<span class="number">2</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure><pre><code>tensor([[1., 1., 1.],        [1., 1., 1.]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t.zeros(<span class="number">2</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure><pre><code>tensor([[0., 0., 0.],        [0., 0., 0.]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t.linspace(<span class="number">1</span>, <span class="number">10</span> ,<span class="number">3</span>)</span><br></pre></td></tr></table></figure><pre><code>tensor([ 1.0000,  5.5000, 10.0000])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure><pre><code>tensor([[-0.4864,  0.5022, -0.4059],        [ 0.4138,  1.1588, -1.1650]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t.randperm</span><br></pre></td></tr></table></figure><pre><code>&lt;function _VariableFunctions.randperm&gt;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 0到n-1随机排列后的数列</span></span><br><span class="line">n = <span class="number">10</span></span><br><span class="line">t.randperm(n)</span><br></pre></td></tr></table></figure><pre><code>tensor([2, 5, 8, 3, 4, 1, 0, 7, 9, 6])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t.eye(<span class="number">2</span>, <span class="number">3</span>) <span class="comment"># 不要求行列数一致</span></span><br></pre></td></tr></table></figure><pre><code>tensor([[1., 0., 0.],        [0., 1., 0.]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t.normal(t.Tensor([<span class="number">0</span>]),t.Tensor([<span class="number">1</span>]))</span><br></pre></td></tr></table></figure><pre><code>tensor([-0.5517])</code></pre><h5 id="常用Tensor操作"><a href="#常用Tensor操作" class="headerlink" title="常用Tensor操作"></a>常用Tensor操作</h5><ul><li><p>tensor.view方法可以改变tensor的形状，但要保证前后元素总数一致。前后保持数据一致，返回的新tensor与源tensor共享内存。</p></li><li><p>在实际应用中可能经常需要增加或减少某个维度，这是squeeze和unsqueeze两个函数排上用场。</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = t.arange(<span class="number">0</span>, <span class="number">6</span>)</span><br><span class="line">a.view(<span class="number">2</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure><pre><code>tensor([[0, 1, 2],        [3, 4, 5]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b = a.view(<span class="number">-1</span>, <span class="number">3</span>) <span class="comment"># 当某一维为-1时，会自动计算它的大小</span></span><br><span class="line">b</span><br></pre></td></tr></table></figure><pre><code>tensor([[0, 1, 2],        [3, 4, 5]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">b.shape, b.unsqueeze(<span class="number">1</span>).shape <span class="comment"># 注意形状，在第1维上增加“1”</span></span><br></pre></td></tr></table></figure><pre><code>(torch.Size([2, 3]), torch.Size([2, 1, 3]))</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">b.unsqueeze(<span class="number">-2</span>) <span class="comment"># -2表示倒数第二个维度</span></span><br></pre></td></tr></table></figure><pre><code>tensor([[[0, 1, 2]],        [[3, 4, 5]]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">c = b.view(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">c, c.squeeze(<span class="number">0</span>) <span class="comment"># 压缩第0维的1</span></span><br></pre></td></tr></table></figure><pre><code>(tensor([[[[[0, 1, 2],            [3, 4, 5]]]]]), tensor([[[[0, 1, 2],           [3, 4, 5]]]]))</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">c.squeeze() <span class="comment"># 压缩所有的“1”的维度</span></span><br></pre></td></tr></table></figure><pre><code>tensor([[0, 1, 2],        [3, 4, 5]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a[<span class="number">1</span>] = <span class="number">100</span></span><br><span class="line">b <span class="comment"># a和b共享内存，修改了a，b也变了</span></span><br></pre></td></tr></table></figure><pre><code>tensor([[  0, 100,   2],        [  3,   4,   5]])</code></pre><p>resize是另一种改变size的方法，和view不同的地方是resize可以改变尺寸，可以有不同数量的元素。如果新尺寸超过了旧尺寸，会自动分配空间，如果新尺寸小于旧尺寸，之前的数据依旧会保存。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b.resize_(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">b</span><br></pre></td></tr></table></figure><pre><code>tensor([[  0, 100,   2]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b.resize_(<span class="number">3</span>, <span class="number">3</span>) <span class="comment"># 旧的数据依旧被保存，多出的数据会分配新空间。</span></span><br><span class="line">b</span><br></pre></td></tr></table></figure><pre><code>tensor([[                  0,                 100,                   2],        [                  3,                   4,                   5],        [7881702260482471202, 8319104481852400229, 7075192647680159593]])</code></pre><h5 id="索引操作"><a href="#索引操作" class="headerlink" title="索引操作"></a>索引操作</h5><p>Tensor支持和numpy.ndarray类似的索引操作，语法上也类似。</p><p><strong><em>如无特殊说明，索引出来的结果与原tensor共享内存</em></strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a = t.randn(<span class="number">3</span>,<span class="number">4</span>);a</span><br></pre></td></tr></table></figure><pre><code>tensor([[ 0.8865, -0.8832, -1.0883, -0.2804],        [-0.9056,  0.0635,  0.5528, -0.0222],        [ 1.4919, -1.0480, -1.7623,  0.8558]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a[<span class="number">0</span>] <span class="comment"># 第0行</span></span><br></pre></td></tr></table></figure><pre><code>tensor([ 0.8865, -0.8832, -1.0883, -0.2804])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a[:, <span class="number">0</span>] <span class="comment"># 第0列</span></span><br></pre></td></tr></table></figure><pre><code>tensor([ 0.8865, -0.9056,  1.4919])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a[<span class="number">0</span>][<span class="number">2</span>] <span class="comment"># 第0行第2个元素，等价于a[0,2]</span></span><br></pre></td></tr></table></figure><pre><code>tensor(-1.0883)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a[<span class="number">0</span>, <span class="number">-1</span>] <span class="comment"># 第0行最后一个元素</span></span><br></pre></td></tr></table></figure><pre><code>tensor(-0.2804)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a[:<span class="number">2</span>] <span class="comment"># 前两行</span></span><br></pre></td></tr></table></figure><pre><code>tensor([[ 0.8865, -0.8832, -1.0883, -0.2804],        [-0.9056,  0.0635,  0.5528, -0.0222]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a[:<span class="number">2</span>, <span class="number">0</span>:<span class="number">2</span>] <span class="comment"># 前两行，第0,1列</span></span><br></pre></td></tr></table></figure><pre><code>tensor([[ 0.8865, -0.8832],        [-0.9056,  0.0635]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a[<span class="number">0</span>:<span class="number">1</span>, :<span class="number">2</span>].shape, a[<span class="number">0</span>, :<span class="number">2</span>].shape <span class="comment"># 注意两者的区别是形状不同，但是值是一样的</span></span><br></pre></td></tr></table></figure><pre><code>(torch.Size([1, 2]), torch.Size([2]))</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a[a &gt; <span class="number">1</span>] <span class="comment"># 等价于a.masked_select(a&gt;1)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 选择结果与原tensor不共享内存空间</span></span><br></pre></td></tr></table></figure><pre><code>tensor([1.4919])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a[t.LongTensor([<span class="number">0</span>,<span class="number">1</span>])] <span class="comment"># 第0行和第1行</span></span><br></pre></td></tr></table></figure><pre><code>tensor([[ 0.8865, -0.8832, -1.0883, -0.2804],        [-0.9056,  0.0635,  0.5528, -0.0222]])</code></pre><p>常用的选择函数：</p><table><thead><tr><th align="left">函数</th><th align="left">功能</th></tr></thead><tbody><tr><td align="left">index_select(input, dim, index)</td><td align="left">在指定维度dim上选取，例如选取某行某列</td></tr><tr><td align="left">masked_select(input, mask)</td><td align="left">例子如上，a[a &gt; 0],使用ByteTensor进行选取</td></tr><tr><td align="left">non_zero(input)</td><td align="left">非0元素的下标</td></tr><tr><td align="left">gather(input, dim, index)</td><td align="left">根据index，在dim维度上选取数据，输出的size与index一样</td></tr></tbody></table><p>gather是一个比较复杂的操作，对于一个二维的tensor，输出的每个元素如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">out[i][j] = input[index[i][j]][j] <span class="comment"># dim = 0</span></span><br><span class="line">out[i][j] = input[i][index[i][j]] <span class="comment"># dim = 1</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a = t.arange(<span class="number">0</span>, <span class="number">16</span>).view(<span class="number">4</span>, <span class="number">4</span>);a</span><br></pre></td></tr></table></figure><pre><code>tensor([[ 0,  1,  2,  3],        [ 4,  5,  6,  7],        [ 8,  9, 10, 11],        [12, 13, 14, 15]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 选取对角线上的元素</span></span><br><span class="line">index = t.LongTensor([[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]])</span><br><span class="line">a.gather(<span class="number">0</span>, index)</span><br></pre></td></tr></table></figure><pre><code>tensor([[ 0,  5, 10, 15]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 选取反对角线上的元素</span></span><br><span class="line">index = t.LongTensor([[<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>]]).t()</span><br><span class="line">a.gather(<span class="number">1</span>,index)</span><br></pre></td></tr></table></figure><pre><code>tensor([[ 3],        [ 6],        [ 9],        [12]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 选取反对角线上的元素，注意与上面不同</span></span><br><span class="line">index = t.LongTensor([[<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>]])</span><br><span class="line">a.gather(<span class="number">0</span>, index)</span><br></pre></td></tr></table></figure><pre><code>tensor([[12,  9,  6,  3]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 选取两个对角线上的元素</span></span><br><span class="line">index = t.LongTensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>]]).t()</span><br><span class="line">b = a.gather(<span class="number">1</span>, index);b</span><br></pre></td></tr></table></figure><pre><code>tensor([[ 0,  3],        [ 5,  6],        [10,  9],        [15, 12]])</code></pre><p>gather的逆操作是scatter_, gather把数据从input中按index取出，而scatter_是把取出的数据再放回去。注意scatter_函数是inplace操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">out = input.gather(dim, index)</span><br><span class="line">--&gt;近似逆操作</span><br><span class="line">out = Tensor()</span><br><span class="line">out.scatter_(dim, index)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 把两个对角线元素放回到指定位置</span></span><br><span class="line"></span><br><span class="line">c = t.zeros(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">c.scatter_(<span class="number">1</span>, index, b.float())</span><br></pre></td></tr></table></figure><pre><code>tensor([[ 0.,  0.,  0.,  3.],        [ 0.,  5.,  6.,  0.],        [ 0.,  9., 10.,  0.],        [12.,  0.,  0., 15.]])</code></pre><h5 id="高级索引"><a href="#高级索引" class="headerlink" title="高级索引"></a>高级索引</h5><p>高级索引可以看成是普通索引的扩展，但是高级索引操作的结果一般不和原Tensor共享内存。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x = t.arange(<span class="number">0</span>, <span class="number">27</span>).view(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>);x</span><br></pre></td></tr></table></figure><pre><code>tensor([[[ 0,  1,  2],         [ 3,  4,  5],         [ 6,  7,  8]],        [[ 9, 10, 11],         [12, 13, 14],         [15, 16, 17]],        [[18, 19, 20],         [21, 22, 23],         [24, 25, 26]]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x[[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">1</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">0</span>]] <span class="comment"># 元素的个数是列表的长度  元素为x[1,1,2]和x[2,2,0]</span></span><br></pre></td></tr></table></figure><pre><code>tensor([14, 24])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x[[<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">1</span>]] <span class="comment"># 元素为最长列表的长度 x[2,0,1] x[1,0,1] x[0,0,1]</span></span><br></pre></td></tr></table></figure><pre><code>tensor([19, 10,  1])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x[[<span class="number">0</span>,<span class="number">2</span>],...] <span class="comment"># x[0] x[2]</span></span><br></pre></td></tr></table></figure><pre><code>tensor([[[ 0,  1,  2],         [ 3,  4,  5],         [ 6,  7,  8]],        [[18, 19, 20],         [21, 22, 23],         [24, 25, 26]]])</code></pre><h5 id="Tensor类型"><a href="#Tensor类型" class="headerlink" title="Tensor类型"></a>Tensor类型</h5><p>默认的Tensor类型为FloatTensor，可通过t.get_default_tensor_type修改默认类型（如果默认类型为GPU tensor，在所有操作都在GPU上进行）。</p><p>HalfTensor是专门为GPU版本设计的，同样的元素个数，显存占用只有FloatTensor的一半，所以可以极大地缓解GPU显存不足的问题，但是由于HalfTensor所能表示的数值大小和精度有限，所以可能出现溢出等问题。</p><table><thead><tr><th align="left">数据类型</th><th align="left">CPU tensor</th><th align="left">GPU tensor</th></tr></thead><tbody><tr><td align="left">32bit浮点</td><td align="left">torch.FloatTensor</td><td align="left">torch.cuda.FloatTensor</td></tr><tr><td align="left">64bit浮点</td><td align="left">torch.DoubleTensor</td><td align="left">torch.cuda.DoubleTensor</td></tr><tr><td align="left">16半精度浮点</td><td align="left">N/A</td><td align="left">torch.cuda.HalfTensor</td></tr><tr><td align="left">8bit无符号整型（0~255）</td><td align="left">torch.ByteTensor</td><td align="left">torch.cuda.ByteTensor</td></tr><tr><td align="left">8bit有符号整型（-128~127）</td><td align="left">torch.CharTensor</td><td align="left">torch.cuda.CharTensor</td></tr><tr><td align="left">16bit有符号整型</td><td align="left">torch.ShortTensor</td><td align="left">torch.cuda.ShortTensor</td></tr><tr><td align="left">32bit有符号整型</td><td align="left">torch.IntTensor</td><td align="left">torch.cuda.IntTensor</td></tr><tr><td align="left">64bit有符号整型</td><td align="left">torch.LongTensor</td><td align="left">torch.cuda.LongTensor</td></tr></tbody></table><p>各数据类型之间可以互相转换，type(new_type)是通用的做法，同时还有float、long、half等快捷方法。CPU tensor与GPUtensor之间的互相装换通过tensor.cuda和tensor.cpu的方法实现。Tensor还有一个new方法，用法与t.Tensor一样，会调用该tensor对应类型的构造函数，生成与当前tensor类型一致的tensor。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置默认tensor类型, 注意参数是字符串</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># t.set_default_tensor_type('torch.IntTensor') 会报错</span></span><br><span class="line"><span class="comment"># TypeError: only floating-point types are supported as the default type</span></span><br><span class="line"><span class="comment"># t.get_default_dtype() 返回 torch.float32</span></span><br><span class="line"><span class="comment"># t.set_default_dtype(t.int) 报错 TypeError: only floating-point types are supported as the default type</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a = t.Tensor(<span class="number">2</span>, <span class="number">3</span>);a</span><br></pre></td></tr></table></figure><pre><code>tensor([[1.8609e+34, 1.8179e+31, 1.8524e+28],        [9.6647e+35, 2.0076e+29, 7.3185e+28]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">b = a.int();b</span><br></pre></td></tr></table></figure><pre><code>tensor([[-2147483648, -2147483648, -2147483648],        [-2147483648, -2147483648, -2147483648]], dtype=torch.int32)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">c = a.type_as(b);c</span><br></pre></td></tr></table></figure><pre><code>tensor([[-2147483648, -2147483648, -2147483648],        [-2147483648, -2147483648, -2147483648]], dtype=torch.int32)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">d = b.new(<span class="number">2</span>, <span class="number">3</span>);d</span><br></pre></td></tr></table></figure><pre><code>tensor([[         0,  775041082,  960062260],        [1697986359,  926101553,  895706424]], dtype=torch.int32)</code></pre><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看函数new的源码</span></span><br><span class="line"></span><br><span class="line">a.new??</span><br></pre></td></tr></table></figure><h5 id="逐元素操作"><a href="#逐元素操作" class="headerlink" title="逐元素操作"></a>逐元素操作</h5><p>这部分操作会对tensor的每个元素进行操作，输入和输出的形状相同。</p><table><thead><tr><th align="left">函数</th><th align="left">功能</th></tr></thead><tbody><tr><td align="left">abs/sqrt/div/exp/fmod/log/pow..</td><td align="left">绝对值/平方根/除法/指数/求余/对数/求幂</td></tr><tr><td align="left">cos/sin/asin/atan2/cosh</td><td align="left">三角函数</td></tr><tr><td align="left">ceil/round/floor/trunc</td><td align="left">上取整/四舍五入/下取整/只保留整数部分</td></tr><tr><td align="left">clamp(input,min,max)</td><td align="left">超过min和max部分截断</td></tr><tr><td align="left">sigmod/tanh…</td><td align="left">激活函数</td></tr></tbody></table><p>对于很多基本的运算，比如加减乘除求余等运算pytorch都实现了运算符重载，可以直接使用运算符。<br>其中camp(x, min, max)的输出满足一个分段函数：</p><p>$$<br>y_i=<br>\begin{cases}<br>min, &amp; {x_i &lt; min}\\<br>x_i, &amp; {min \leq x_i \leq max}\\<br>max, &amp; {x_i &gt; max}<br>\end{cases}<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = t.arange(<span class="number">0</span>, <span class="number">6</span>).view(<span class="number">2</span>, <span class="number">3</span>).float() <span class="comment"># 注意要转换一下类型，否则会报错</span></span><br><span class="line">t.cos(a)</span><br></pre></td></tr></table></figure><pre><code>tensor([[ 1.0000,  0.5403, -0.4161],        [-0.9900, -0.6536,  0.2837]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a % <span class="number">3</span> <span class="comment"># 等价于t.fmod(a, 5)</span></span><br></pre></td></tr></table></figure><pre><code>tensor([[0., 1., 2.],        [0., 1., 2.]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a ** <span class="number">2</span><span class="comment"># 等价于t.power(a, 2)</span></span><br></pre></td></tr></table></figure><pre><code>tensor([[ 0.,  1.,  4.],        [ 9., 16., 25.]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># a中每个元素与3相比取较大的那一个</span></span><br><span class="line"></span><br><span class="line">print(a)</span><br><span class="line">t.clamp(a, min = <span class="number">3</span>)</span><br></pre></td></tr></table></figure><pre><code>tensor([[0., 1., 2.],        [3., 4., 5.]])tensor([[3., 3., 3.],        [3., 4., 5.]])</code></pre><h5 id="归并操作"><a href="#归并操作" class="headerlink" title="归并操作"></a>归并操作</h5><p>这类操作会使输入形状小于输出形状，并可以沿着某一维度进行制定操作。</p><table><thead><tr><th align="left">函数</th><th align="left">功能</th></tr></thead><tbody><tr><td align="left">mean/sum/median/mode</td><td align="left">均值/和/中位数/众数</td></tr><tr><td align="left">norm/dist</td><td align="left">范数/距离</td></tr><tr><td align="left">std/var</td><td align="left">标准差/方差</td></tr><tr><td align="left">cumsum/cumprod</td><td align="left">累加/累乘</td></tr></tbody></table><p>几乎每个函数都有一个dim参数，用来制定在那个维度上执行。<br>假设输入的形状是(m, n, k):</p><ul><li>如果指定dim = 0，输出的形状为(1, n, k)或者(n, k)</li><li>如果指定dim = 1，输出的形状为(m, 1, k)或者(m, k)</li><li>如果指定dim = 2，输出的形状为(m, n, 1)或者(m, n)</li></ul><p>也就是dim指定哪个维度，那个维度就会变成1，size中是否有1取决于keepdim，keepdim=True会保留1，keepdim默认为False。但是并非总是这样，比如cumsum。</p><p>归并运算就是对其他维度取值相同且该维度取值不同元素进行操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b = t.ones(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">b.sum(dim = <span class="number">0</span>, keepdim = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure><pre><code>tensor([[2., 2., 2.]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">b.sum(dim = <span class="number">0</span>) <span class="comment">#keepdim = False</span></span><br></pre></td></tr></table></figure><pre><code>tensor([2., 2., 2.])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">b.sum(dim = <span class="number">1</span>)</span><br></pre></td></tr></table></figure><pre><code>tensor([3., 3.])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = t.arange(<span class="number">0</span>, <span class="number">6</span>).view(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">print(a)</span><br><span class="line">a.cumsum(dim = <span class="number">1</span>) <span class="comment">#沿着行累加</span></span><br></pre></td></tr></table></figure><pre><code>tensor([[0, 1, 2],        [3, 4, 5]])tensor([[ 0,  1,  3],        [ 3,  7, 12]])</code></pre><p>cumsum可以理解为以dim这个维度上索引取值相同的看作一个整体，比如dim=0每一行就是一个整体，cumsum运算相当于dim这个维度上取值为n的值加上取值为n-1的值（这个n-1已经进行过前面的运算，不是初始的值）。</p><h5 id="比较"><a href="#比较" class="headerlink" title="比较"></a>比较</h5><p>比较函数有的是逐元素操作，有的是归并操作。</p><table><thead><tr><th align="left">函数</th><th align="left">功能</th></tr></thead><tbody><tr><td align="left">gt/lt/ge/le/eq/ne</td><td align="left">大于/小于/大于等于/小于等于/等于/不等</td></tr><tr><td align="left">topk</td><td align="left">最大的k个数</td></tr><tr><td align="left">sort</td><td align="left">排序</td></tr><tr><td align="left">max/min</td><td align="left">比较两个tensor的最大值或最小值</td></tr></tbody></table><p>表中第一行的比较操作已经重载，已经可以使用a&gt;=b, a&gt;b, a!=b和a==b，其返回结果为一个ByteTensor,可以用来选取元素(高级索引)。</p><p>max和min两个操作比较特殊，以max为例：</p><ul><li>t.max(tensor):返回tensor中最大的一个数。</li><li>t.max(tensor,dim)：指定维上最大的一个数，返回tensor和下标。</li><li>t.max(tensor1,tensor2)：比较两个tensor中较大的元素。</li></ul><p>tensor和一个数的比较可以用clamp函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a = t.linspace(<span class="number">0</span>, <span class="number">15</span>, <span class="number">6</span>).view(<span class="number">2</span>, <span class="number">3</span>);a</span><br></pre></td></tr></table></figure><pre><code>tensor([[ 0.,  3.,  6.],        [ 9., 12., 15.]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">b = t.linspace(<span class="number">15</span>, <span class="number">0</span>, <span class="number">6</span>).view(<span class="number">2</span>, <span class="number">3</span>);b</span><br></pre></td></tr></table></figure><pre><code>tensor([[15., 12.,  9.],        [ 6.,  3.,  0.]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a &gt; b</span><br></pre></td></tr></table></figure><pre><code>tensor([[False, False, False],        [ True,  True,  True]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a[a &gt; b]</span><br></pre></td></tr></table></figure><pre><code>tensor([ 9., 12., 15.])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t.max(a)</span><br></pre></td></tr></table></figure><pre><code>tensor(15.)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t.max(a, dim = <span class="number">1</span>)</span><br></pre></td></tr></table></figure><pre><code>torch.return_types.max(values=tensor([ 6., 15.]),indices=tensor([2, 2]))</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t.max(a, b)</span><br></pre></td></tr></table></figure><pre><code>tensor([[15., 12.,  9.],        [ 9., 12., 15.]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 比较a和10较大的元素</span></span><br><span class="line"></span><br><span class="line">t.clamp(a, min=<span class="number">10</span>)</span><br></pre></td></tr></table></figure><pre><code>tensor([[10., 10., 10.],        [10., 12., 15.]])</code></pre><h5 id="线性代数"><a href="#线性代数" class="headerlink" title="线性代数"></a>线性代数</h5><p>pytorch的线性函数封装了Blas和Lapack。</p><table><thead><tr><th align="left">函数</th><th align="left">功能</th></tr></thead><tbody><tr><td align="left">trace</td><td align="left">对角线元素（矩阵的迹）</td></tr><tr><td align="left">diag</td><td align="left">对角线元素</td></tr><tr><td align="left">triu/tril</td><td align="left">矩阵的上三角/下三角，可以指定偏移量</td></tr><tr><td align="left">mm/bmm</td><td align="left">矩阵乘法，batch的矩阵乘法</td></tr><tr><td align="left">addmm/addbmm/addmv</td><td align="left">矩阵运算</td></tr><tr><td align="left">t</td><td align="left">转置</td></tr><tr><td align="left">dot/cross</td><td align="left">内积/外积</td></tr><tr><td align="left">inverse</td><td align="left">求逆矩阵</td></tr><tr><td align="left">svd</td><td align="left">奇异值分解</td></tr></tbody></table><p>需要注意矩阵装置会导致储存空间不连续，需调用它的.contiguous方法将其转为连续。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b = a.t()</span><br><span class="line">b.is_contiguous(),b</span><br></pre></td></tr></table></figure><pre><code>(False, tensor([[ 0.,  9.],         [ 3., 12.],         [ 6., 15.]]))</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">b.contiguous()</span><br></pre></td></tr></table></figure><pre><code>tensor([[ 0.,  9.],        [ 3., 12.],        [ 6., 15.]])</code></pre><h4 id="Tensor和Numpy"><a href="#Tensor和Numpy" class="headerlink" title="Tensor和Numpy"></a>Tensor和Numpy</h4><p>tensor和numpy数组之间具有很高的相似性，彼此之间相互操作也十分高效。需要注意，numpy和tensor共享内存。当遇到tensor不支持的操作时，可先转成Numpy数组，处理后再装回tensor，其转换开销很小。</p><p>广播法则是科学运算中经常使用的一个技巧，它在快速执行向量化的同时不会占用额外的内存、显存。Numpy的广播法则定义如下：</p><ul><li>让所有输入数组都向shape最长的数组看齐，shape中不足的部分通过在前面加1补齐。</li><li>两个数组要么在某一个维度的长度一致，要么其中一个为1，否则不能计算。</li><li>当输入数组的某个维度的长度为1时，计算时沿着此维度复制扩充成一样的形状。</li></ul><p>pytorch当前已经支持了自动广播法则，但建议可以手动通过函数实现广播法则，更直观不易出错。</p><ul><li>unsqueeze或者view：为数据的某一维的形状补1，实现法则1。</li><li>expand或者expand_as，重复数组，实现法则3；该操作不会复制数组，所以不会占用额外的空间。</li></ul><p><strong>注意</strong>:repeat实现有expand类似，但是repeat会把相同数据复制多份，因此会占用额外空间。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = t.ones(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">b = t.zeros(<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 自动广播法则</span></span><br><span class="line"><span class="comment"># 第一步：a是二维，b是三维，所以先在较小的a前面补1，</span></span><br><span class="line"><span class="comment"># 即：a.unsqueeze(0), a的形状变成(1, 3, 2), b的形状是(2, 3, 1),</span></span><br><span class="line"><span class="comment"># 第二步：a和b在第一维和第三维的形状不一样，其中一个为1</span></span><br><span class="line"><span class="comment"># 可以利用广播法则扩展，两个形状都变成了(2, 3, 2)</span></span><br><span class="line"></span><br><span class="line">(a + b).shape,a + b</span><br></pre></td></tr></table></figure><pre><code>(torch.Size([2, 3, 2]), tensor([[[1., 1.],          [1., 1.],          [1., 1.]],         [[1., 1.],          [1., 1.],          [1., 1.]]]))</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a.unsqueeze(<span class="number">0</span>).expand(<span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>) + b.expand(<span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure><pre><code>tensor([[[1., 1.],         [1., 1.],         [1., 1.]],        [[1., 1.],         [1., 1.],         [1., 1.]]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.ones([<span class="number">2</span>, <span class="number">3</span>], dtype = np.float32)</span><br><span class="line">a</span><br></pre></td></tr></table></figure><pre><code>array([[1., 1., 1.],       [1., 1., 1.]], dtype=float32)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">b = t.from_numpy(a);b</span><br></pre></td></tr></table></figure><pre><code>tensor([[1., 1., 1.],        [1., 1., 1.]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b = t.Tensor(a) <span class="comment"># 也可以直接讲numpy对象传入Tensor，这种情况下若numpy类型不是Float32会新建。</span></span><br><span class="line">b</span><br></pre></td></tr></table></figure><pre><code>tensor([[1., 1., 1.],        [1., 1., 1.]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">c = b.numpy() <span class="comment"># a, b, c三个对象共享内存</span></span><br><span class="line">c</span><br></pre></td></tr></table></figure><pre><code>array([[1., 1., 1.],       [1., 1., 1.]], dtype=float32)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># expand不会占用额外空间，只会在需要时才扩充，可极大地节省内存。</span></span><br><span class="line">e = t.Tensor(a).unsqueeze(<span class="number">0</span>).expand(<span class="number">1000000000000</span>,  <span class="number">2</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure><h4 id="内部结构"><a href="#内部结构" class="headerlink" title="内部结构"></a>内部结构</h4><p>tensor分为头信息区（Tensor）和存储区（Storage），信息区主要保存着tensor的形状（size），步长（stride）、数据类型（type）等信息，而真正的数据则保存成连续的数组。</p><pre class="mermaid" style="text-align: center;">                        graph LR;A[Tensor A: *size *stride * dimention...] --> C[Storage:*data *size ...];B[Tensor B: *size *stride * dimention....] --> C[Storage:*data *size ...];          </pre><p>一般来说，一个tensor有着与之对应的storage，storage是在data之上封装的接口，便于使用。不同的tensor的头信息一般不同，但却可能使用相同的storage。下面我们来看两个例子。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = t.arange(<span class="number">0</span>, <span class="number">6</span>)</span><br><span class="line">a.storage()</span><br></pre></td></tr></table></figure><pre><code> 0 1 2 3 4 5[torch.LongStorage of size 6]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b = a.view(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">b.storage()</span><br></pre></td></tr></table></figure><pre><code> 0 1 2 3 4 5[torch.LongStorage of size 6]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一个对象的id值可以看作它在内存中的地址</span></span><br><span class="line"><span class="comment"># a和b storage的内存地址一样，即是同一个storage</span></span><br><span class="line">id(b.storage()) == id(a.storage())</span><br></pre></td></tr></table></figure><pre><code>True</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># a改变，b也随之改变，因为它们共享storage</span></span><br><span class="line"></span><br><span class="line">a[<span class="number">1</span>] = <span class="number">100</span></span><br><span class="line">b</span><br></pre></td></tr></table></figure><pre><code>tensor([[  0, 100,   2],        [  3,   4,   5]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">c = a[<span class="number">2</span>:]</span><br><span class="line">c.storage()</span><br></pre></td></tr></table></figure><pre><code> 0 100 2 3 4 5[torch.LongStorage of size 6]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">c.data_ptr(), a.data_ptr(), c.dtype <span class="comment"># data_ptr返回tensor的首元素的内存地址</span></span><br><span class="line"><span class="comment"># 可以看出相差16，这是因为2x8相差两个元素，每个元素占8个字节</span></span><br></pre></td></tr></table></figure><pre><code>(61509136, 61509120, torch.int64)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">c[<span class="number">0</span>] = <span class="number">-100</span> <span class="comment"># c[0]的内存地址对应a[2]内存地址</span></span><br><span class="line">a</span><br></pre></td></tr></table></figure><pre><code>tensor([   0,  100, -100,    3,    4,    5])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">d = t.Tensor(c.float().storage())</span><br><span class="line">d[<span class="number">0</span>] = <span class="number">6666</span></span><br><span class="line">b</span><br></pre></td></tr></table></figure><pre><code>tensor([[   0,  100, -100],        [   3,    4,    5]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下面4个共享storage</span></span><br><span class="line"></span><br><span class="line">id(a.storage()) == id(b.storage()) == id(c.storage()) == id(d.storage())</span><br></pre></td></tr></table></figure><pre><code>True</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a.storage_offset(), c.storage_offset(), a[<span class="number">3</span>:].storage_offset()</span><br></pre></td></tr></table></figure><pre><code>(0, 2, 3)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">e = b[::<span class="number">2</span>, ::<span class="number">2</span>] <span class="comment"># 隔2行/列取一个元素</span></span><br><span class="line">id(e.storage()) == id(a.storage())</span><br></pre></td></tr></table></figure><pre><code>True</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">b.stride(), e.stride()</span><br></pre></td></tr></table></figure><pre><code>((3, 1), (6, 2))</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">e.is_contiguous()</span><br></pre></td></tr></table></figure><pre><code>False</code></pre><ul><li><p>可见绝大多数操作并不修改tensor的数据，只是修改头信息。这样更节省内存，同时提升了处理的速度。但是，有些操作会导致tensor不连续，这时需调用tensor.contiguous方法将他们变成连续数据，该方法复制数据到新的内存，不再与原来的数据共享storage。</p></li><li><p>另外高级索引一般不共享内存，而普通索引共享storage。</p></li></ul><h3 id="其他有关Tensor的话题"><a href="#其他有关Tensor的话题" class="headerlink" title="其他有关Tensor的话题"></a>其他有关Tensor的话题</h3><h4 id="持久化"><a href="#持久化" class="headerlink" title="持久化"></a>持久化</h4><p>tensor的保存和加载十分简单，使用t.save和t.load即可完成相应功能。在save/load时可以指定使用的pickle模块，在load时还可以将GPU tensor映射到CPU或其他GPU上。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> t.cuda.is_available():</span><br><span class="line">    a = a.cuda(<span class="number">1</span>)</span><br><span class="line">    t.save(a, <span class="string">'a.pth'</span>)</span><br><span class="line">    <span class="comment"># 加载为b，储存于GPU1上（因为保存时就在GPU1上）</span></span><br><span class="line">    b = t.load(<span class="string">'a.pth'</span>)</span><br><span class="line">    <span class="comment"># 加载为c，储存在CPU</span></span><br><span class="line">    c = t.load(<span class="string">'a.pth'</span>, map_location = <span class="keyword">lambda</span> storage,loc:storage)</span><br><span class="line">    <span class="comment"># 加载为d，储存于GPU0上</span></span><br><span class="line">    d = t.load(<span class="string">'a.pth'</span>, map_location = &#123;<span class="string">'cuda:1'</span>:<span class="string">'cuda:0'</span>&#125;)</span><br></pre></td></tr></table></figure><h4 id="向量化"><a href="#向量化" class="headerlink" title="向量化"></a>向量化</h4><p>向量化计算是一种特殊的并行计算方式，通常是对不同的数据执行同样的一个或一批指令。向量化可极大第提高科学运算的效率。Python有许多操作很低效，尤其是for循环。在科学计算中要极力避免使用Python原生的for循环，尽量使用向量化的数值计算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">for_loop_add</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    result = []</span><br><span class="line">    <span class="keyword">for</span> i, j <span class="keyword">in</span> zip(x, y):</span><br><span class="line">        result.append(i + j)</span><br><span class="line">    <span class="keyword">return</span> t.Tensor(result)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = t.zeros(<span class="number">100</span>)</span><br><span class="line">y = t.ones(<span class="number">100</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">%timeit -n <span class="number">10</span> for_loop_add(x, y)</span><br><span class="line">%timeit -n <span class="number">10</span> x + y</span><br></pre></td></tr></table></figure><pre><code>729 µs ± 414 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)The slowest run took 4.81 times longer than the fastest. This could mean that an intermediate result is being cached.3.5 µs ± 2.69 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)</code></pre><p>可见有好几百倍的速度差距，因此在实际使用中应尽量调用内建函数，这些函数底层由C/C++实现，能通过执行底层优化实现高效计算。</p><p>此为还需要注意几点：</p><ul><li>大多数t.function都有一个参数out，这时产生的结果将保存在out指定的tensor之中</li><li>t.set_num_threads可以设置pytorch进行CPU多线程并行计算时所占用的线程数，来限制pytorch所占用的CPU数目。</li><li>t.set_printoptions可以用来设置打印tensor时的数值精度和格式。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a = t.randn(<span class="number">2</span>, <span class="number">3</span>); a</span><br></pre></td></tr></table></figure><pre><code>tensor([[-0.1227, -0.0569, -0.6876],        [ 1.6025,  0.6995,  0.1694]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t.set_printoptions(precision = <span class="number">10</span>);a</span><br></pre></td></tr></table></figure><pre><code>tensor([[-0.1226951405, -0.0568769276, -0.6875813603],        [ 1.6024936438,  0.6995284557,  0.1693879962]])</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Tensor和autograd&quot;&gt;&lt;a href=&quot;#Tensor和autograd&quot; class=&quot;headerlink&quot; title=&quot;Tensor和autograd&quot;&gt;&lt;/a&gt;Tensor和autograd&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;每个深度学
      
    
    </summary>
    
      <category term="笔记" scheme="https://zinw623.github.io/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="pytorch" scheme="https://zinw623.github.io/categories/%E7%AC%94%E8%AE%B0/pytorch/"/>
    
    
      <category term="pytorch" scheme="https://zinw623.github.io/tags/pytorch/"/>
    
      <category term="笔记" scheme="https://zinw623.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>[thinkload]读书-疑问</title>
    <link href="https://zinw623.github.io/2019/08/13/deeplearning-book/"/>
    <id>https://zinw623.github.io/2019/08/13/deeplearning-book/</id>
    <published>2019-08-13T02:18:13.000Z</published>
    <updated>2019-08-13T03:02:00.205Z</updated>
    
    <content type="html"><![CDATA[<h4 id="2019-8-13"><a href="#2019-8-13" class="headerlink" title="2019-8-13"></a>2019-8-13</h4><ul><li><p>为什么要求算法最小化cost function却要以准确率错误率来衡量它？从cost到准确率丢失了衡量差距到底有多大的信息，这两个并不等价啊！</p></li><li><p>如何衡量一个被选择的函数在某一目的上达到了多少效果？或者起到了多少作用？</p></li><li><p>把输入端之前看成也连接了一个网络，这个网络是一种输出数据的网络，两个网络连接在一起看成一个大网络，而这种网络的生成目的应当是不同于后面我们人类设计网络的目的。但是输出的结果仍能一定程度上的符合人类需求，是不是说网络的每个部分可以存在不同的目标？</p></li><li><p>有一个迷宫一样的屋子，同样也是一个挑战，找出从一楼走到天台的最短路径，有一万个人来接受挑战每个走到屋顶的人都被告知自己是否是走的最短路径，每个人都是独立的，有自己的判断标准，当他再次上楼时如何分析他们行为的变化？</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;2019-8-13&quot;&gt;&lt;a href=&quot;#2019-8-13&quot; class=&quot;headerlink&quot; title=&quot;2019-8-13&quot;&gt;&lt;/a&gt;2019-8-13&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;为什么要求算法最小化cost function却要以准确率错误率来
      
    
    </summary>
    
      <category term="thinkload" scheme="https://zinw623.github.io/categories/thinkload/"/>
    
      <category term="花书" scheme="https://zinw623.github.io/categories/thinkload/%E8%8A%B1%E4%B9%A6/"/>
    
    
      <category term="thinkload" scheme="https://zinw623.github.io/tags/thinkload/"/>
    
      <category term="花书" scheme="https://zinw623.github.io/tags/%E8%8A%B1%E4%B9%A6/"/>
    
  </entry>
  
  <entry>
    <title>【notes】docker学习笔记6-docker容器数据管理</title>
    <link href="https://zinw623.github.io/2019/08/09/docker-6/"/>
    <id>https://zinw623.github.io/2019/08/09/docker-6/</id>
    <published>2019-08-09T09:06:17.000Z</published>
    <updated>2019-08-09T12:14:33.803Z</updated>
    
    <content type="html"><![CDATA[<h3 id="docker容器的数据管理简介"><a href="#docker容器的数据管理简介" class="headerlink" title="docker容器的数据管理简介"></a>docker容器的数据管理简介</h3><ul><li>docker容器的数据卷</li><li>docker的数据卷容器</li><li>docker数据卷的备份和还原</li></ul><h3 id="docker容器的数据卷"><a href="#docker容器的数据卷" class="headerlink" title="docker容器的数据卷"></a>docker容器的数据卷</h3><h4 id="什么是数据卷-Data-Volume"><a href="#什么是数据卷-Data-Volume" class="headerlink" title="什么是数据卷(Data Volume)"></a>什么是数据卷(Data Volume)</h4><p>docker的生存周期是与运行的程序相一致的，而我们需要数据持久化，docker容器之间也需要共享一些数据</p><ul><li><p>数据卷是经过特殊设计的目录，可以绕过联合文件系统（UFS)，为一个或多个容器提供访问。</p></li><li><p>数据卷设计的目的，在于数据持久化，它完全对独立于容器的生存周期，因此docker不会在容器删除时删除其挂载的数据卷，也不会存在类似的垃圾收集机制，对容器引用的数据卷进行处理。</p></li><li><p>数据卷架构：</p><p>  <img alt data-src="/uploads/docker/5.PNG"></p><ul><li>docker数据卷独立于docker，独立运docker的生存周期。</li><li>docker数据卷位于docker的宿主机中文件系统。</li><li>docker数据卷既可以是目录也可是文件。</li><li>docker数据卷与宿主机进行数据共享。</li><li>同一目录或文件可以支持多个容器</li></ul></li><li><p>数据卷的特点</p><ul><li>数据卷在容器启动时初始化，如果容器使用的镜像在挂载点包含了数据，这些数据会拷贝到新初始化的数据卷中。</li><li>数据卷可以在容器之间共享和重用</li><li>可以对数据卷里的内容直接进行修改</li><li>数据卷的变化不会影响镜像的更新</li><li>卷会一直存在，即使挂载数据卷的容器已经被删除</li></ul></li></ul><h4 id="数据卷的使用"><a href="#数据卷的使用" class="headerlink" title="数据卷的使用"></a>数据卷的使用</h4><ul><li>为容器添加数据卷</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo docker run -v ~/container_data:/data -it ubuntu /bin/bash</span><br></pre></td></tr></table></figure><p>在本机系统的目录:在容器中映射的目录名</p><p>注：这种方式（bind mount)已不推荐使用，应使用volume方式</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">docker volume create my_volume <span class="comment"># 创建卷</span></span><br><span class="line">docker volume ls  <span class="comment"># 卷列表</span></span><br><span class="line">docker volume inspect my_volume <span class="comment">#卷信息</span></span><br><span class="line">docker volume rm my_volume <span class="comment"># 删除卷</span></span><br><span class="line"></span><br><span class="line">docker run -v [卷名]:[容器目录]:[选项列表] -it ubuntu /bin/bash</span><br></pre></td></tr></table></figure><p>详情：<a href="https://deepzz.com/post/the-docker-volumes-basic.html" target="_blank" rel="noopener">https://deepzz.com/post/the-docker-volumes-basic.html</a></p><ul><li>为数据卷添加访问权限</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo docker run -v [卷名]:[容器目录]:ro(访问权限) -it ubuntu /bin/bash</span><br></pre></td></tr></table></figure><ul><li>使用dockerfile构建包含数据卷的镜像<br>dockerfile指令：<br>  VOLUME [“/data1”, “/data2”]</li></ul><p>不能映射到本地目录，并且运行同一镜像的不同容器所创建的数据卷也是不一样的。</p><h3 id="docker的数据卷容器"><a href="#docker的数据卷容器" class="headerlink" title="docker的数据卷容器"></a>docker的数据卷容器</h3><p>什么是数据卷容器：<br>    命名的容器挂载数据卷，其他容器通过挂载这个容器实现数据共享，挂载数据卷的容器，就叫做数据卷容器</p><p>图示：</p><p><img alt data-src="/uploads/docker/6.PNG"></p><ul><li>挂载数据卷容器的方法</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --volumes-from [CONTAINER NAME]</span><br></pre></td></tr></table></figure><p>如果数据卷容器删除（即使同时删除挂载的数据卷）后，挂载该数据卷容器的容器的数据目录仍存在且有效。</p><p>数据卷容器的作用仅仅是将数据卷挂载的配置传递到挂载了数据卷容器的新容器中。</p><h3 id="docker数据卷的备份与还原"><a href="#docker数据卷的备份与还原" class="headerlink" title="docker数据卷的备份与还原"></a>docker数据卷的备份与还原</h3><ul><li>数据备份方法</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --volumes-from [container name] -v $(<span class="built_in">pwd</span>):/backup ubuntu tar cvf /backup/backup.tar [container data volume]</span><br></pre></td></tr></table></figure><p><img alt data-src="/uploads/docker/7.PNG"></p><ul><li>数据还原方法</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --volumes-from [container name] -v $(<span class="built_in">pwd</span>):/backup ubuntu tar xvf /backup/backup.tar [container data volume]</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;docker容器的数据管理简介&quot;&gt;&lt;a href=&quot;#docker容器的数据管理简介&quot; class=&quot;headerlink&quot; title=&quot;docker容器的数据管理简介&quot;&gt;&lt;/a&gt;docker容器的数据管理简介&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;docker容器的数据卷
      
    
    </summary>
    
      <category term="笔记" scheme="https://zinw623.github.io/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="docker" scheme="https://zinw623.github.io/categories/%E7%AC%94%E8%AE%B0/docker/"/>
    
    
      <category term="docker" scheme="https://zinw623.github.io/tags/docker/"/>
    
      <category term="笔记" scheme="https://zinw623.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>【notes】docker学习笔记5-dockerfile</title>
    <link href="https://zinw623.github.io/2019/08/08/docker-5/"/>
    <id>https://zinw623.github.io/2019/08/08/docker-5/</id>
    <published>2019-08-08T03:27:42.000Z</published>
    <updated>2019-08-13T04:38:51.262Z</updated>
    
    <content type="html"><![CDATA[<h3 id="dockerfile指令"><a href="#dockerfile指令" class="headerlink" title="dockerfile指令"></a>dockerfile指令</h3><h4 id="指令格式"><a href="#指令格式" class="headerlink" title="指令格式"></a>指令格式</h4><p>注释： # Comment<br>指令： INSTRUCTION argument</p><h5 id="FROM"><a href="#FROM" class="headerlink" title="FROM"></a>FROM</h5><ul><li>FROM &lt;image&gt;</li><li>FROM &lt;image&gt;:&lt;tag&gt;</li><li>必须已经存在的镜像，也就是基础镜像</li><li>必须是第一条非注释指令</li></ul><h5 id="MAINTAINER"><a href="#MAINTAINER" class="headerlink" title="MAINTAINER"></a>MAINTAINER</h5><ul><li>MAINTAINER &lt;name&gt;<br>  指定镜像的作者信息，包含镜像的所有者和练习方式</li></ul><h5 id="RUN"><a href="#RUN" class="headerlink" title="RUN"></a>RUN</h5><p><strong>构建构成运行的</strong></p><ul><li>RUN &lt;command&gt;  (shell模式)<br>  /bin/sh -c command</li><li>RUN [“executable”, “param1”, “param2”]  (exec模式)<br>  RUN[“/bin/bash”, “-c”, “echo hello”]</li></ul><h5 id="EXPOSE"><a href="#EXPOSE" class="headerlink" title="EXPOSE"></a>EXPOSE</h5><ul><li>EXPOSE &lt;port&gt; [&lt;port&gt;…]<br>  指定运行该镜像的容器使用的端口，但只是告诉docker会使用特定的端口号，出于安全考虑不会自动打开，在容器运行时仍需要手动指定端口映射。<h5 id="CMD-ENTRYPOINT"><a href="#CMD-ENTRYPOINT" class="headerlink" title="CMD ENTRYPOINT"></a>CMD ENTRYPOINT</h5></li></ul><p><strong>指定容器启动时运行的命令</strong></p><ul><li>CMD [“executable”, “param1”, “param2”] (exec模式)</li><li>CMD command param1 param2 (shell模式)</li><li>CMD [“params1”, “params2”] (作为ENTRYPOINT指令的默认参数)</li></ul><p>在docker run时如果指定命令的话dockerfile里的cmd命令会被覆盖掉。</p><ul><li>ENTRYPOINT [“executable”, “param1”, “param2”] (exec模式)</li><li>ENTRYPOINT command param1 param2 (shell模式)</li></ul><p>默认不会被覆盖，如果需要覆盖需要指定docker run –entrypoint 覆盖</p><h5 id="ADD-COPY-VOLUME"><a href="#ADD-COPY-VOLUME" class="headerlink" title="ADD COPY VOLUME"></a>ADD COPY VOLUME</h5><p><strong>设置镜像的目录和文件</strong></p><ul><li><p>ADD &lt;src&gt;…&lt;dest&gt;</p></li><li><p>ADD [“&lt;src”…”<dest>“] (适用于文件路径中有空格)</dest></p></li><li><p>COPY &lt;src&gt;…&lt;dest&gt;</p></li><li><p>COPY  [“&lt;src”…”<dest>“] (适用于文件路径中有空格)</dest></p></li></ul><p>可以使文件地址（构建目录的相对地址），也可以是远程url（推荐使用curl获取文件内容）</p><ul><li><p>ADD vs. COPY<br>  ADD包含类似tar的解压功能<br>  如果单纯复制文件，docker推荐使用COPY</p></li><li><p>VOLUME [“/data”]<br>  添加卷  </p></li></ul><h5 id="WORKDIR-ENV-USER"><a href="#WORKDIR-ENV-USER" class="headerlink" title="WORKDIR ENV USER"></a>WORKDIR ENV USER</h5><p><strong>构建及容器运行时的环境设置</strong></p><ul><li>WORKDIR /path/to/workdir  (设置工作目录，通常使用绝对路径，否则会一直传递下去)</li></ul><p>e.g:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">WORKDIR /a</span><br><span class="line">WORKDIR b</span><br><span class="line">WORKDIR C</span><br><span class="line">RUN <span class="built_in">pwd</span>  <span class="comment"># 结果为 /a/b/c</span></span><br></pre></td></tr></table></figure><ul><li>ENV &lt;key&gt;&lt;value&gt;</li><li>ENV &lt;key&gt;=&lt;value&gt;</li></ul><p>设置环境变量</p><ul><li>USER daemon</li></ul><p>USER user           USER uid<br>USER user:group     USER uid:gid<br>USER user:gid       USER uid:group</p><p>指定运行的用户，若不指定则默认root用户。</p><h5 id="ONBUILD"><a href="#ONBUILD" class="headerlink" title="ONBUILD"></a>ONBUILD</h5><ul><li>ONBUILD [INSTRUCTION]</li><li>镜像触发器</li><li>当一个镜像被其他镜像作为基础镜像时执行</li><li>会在构建过程中插入指令 </li></ul><h4 id="dockerfile构建过程"><a href="#dockerfile构建过程" class="headerlink" title="dockerfile构建过程"></a>dockerfile构建过程</h4><ul><li>从基础镜像运行一个容器</li><li>执行一条指令，对容器作出修改</li><li>执行类似docker commit的操作， 提交一个新的镜像层（中间层镜像）</li><li>再基于刚提交的镜像运行一个新的容器</li><li>执行dockerfile中的下一条指令，直至所有指令执行完毕</li></ul><ul><li><p>中间层镜像进行调试  注：dockerfile会删除中间层镜像容器但不会删除中间层镜像</p></li><li><p>构建缓存，构建时会建立缓存，因此第二次执行构建命令会很快，是因为使用了缓存。</p></li><li><p>不使用缓存</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker build --no-cache</span><br></pre></td></tr></table></figure></li><li><p>另一种方法通过更改缓存刷新时间</p></li></ul><blockquote><p>FROM Ubuntu:14:04<br>MAINTAINER dormancypress <a href="mailto:user@mail.com" target="_blank" rel="noopener">user@mail.com</a><br>ENV REFRESH_DATE 2019-08-08<br>RUN apt-get update<br>RUN apt-get install -y nginx<br>EXPOSE 80</p></blockquote><p>修改REFRESH_DATE时间</p><ul><li>查看镜像构建过程</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">history</span> [image]</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;dockerfile指令&quot;&gt;&lt;a href=&quot;#dockerfile指令&quot; class=&quot;headerlink&quot; title=&quot;dockerfile指令&quot;&gt;&lt;/a&gt;dockerfile指令&lt;/h3&gt;&lt;h4 id=&quot;指令格式&quot;&gt;&lt;a href=&quot;#指令格式&quot; cla
      
    
    </summary>
    
      <category term="笔记" scheme="https://zinw623.github.io/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="docker" scheme="https://zinw623.github.io/categories/%E7%AC%94%E8%AE%B0/docker/"/>
    
    
      <category term="docker" scheme="https://zinw623.github.io/tags/docker/"/>
    
      <category term="笔记" scheme="https://zinw623.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>【notes】docker学习笔记4-docker客户端与守护进程</title>
    <link href="https://zinw623.github.io/2019/08/07/docker-4/"/>
    <id>https://zinw623.github.io/2019/08/07/docker-4/</id>
    <published>2019-08-07T10:31:41.000Z</published>
    <updated>2019-08-08T03:07:56.995Z</updated>
    
    <content type="html"><![CDATA[<h3 id="docker的C-S模式"><a href="#docker的C-S模式" class="headerlink" title="docker的C/S模式"></a>docker的C/S模式</h3><p><img alt data-src="/uploads/docker/4.PNG"></p><h4 id="客户端与守护进程通信的接口"><a href="#客户端与守护进程通信的接口" class="headerlink" title="客户端与守护进程通信的接口"></a>客户端与守护进程通信的接口</h4><ol><li>命令行接口</li><li>remote API：<ul><li>RESTful风格API</li><li>STDIN STDOUT STDERR</li><li>语言参考：<a href="https://docs.docker.com/reference/api/docker_remote_api" target="_blank" rel="noopener">https://docs.docker.com/reference/api/docker_remote_api</a></li></ul></li></ol><h4 id="连接方式"><a href="#连接方式" class="headerlink" title="连接方式"></a>连接方式</h4><ul><li>unix:///var/run/docker.sock  默认方式</li><li>tcp://host:port</li><li>fd://socketfd</li></ul><h4 id="利用socket进行通信"><a href="#利用socket进行通信" class="headerlink" title="利用socket进行通信"></a>利用socket进行通信</h4><ul><li>查看正在运行的守护进程</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ps -ef | grep docker</span><br></pre></td></tr></table></figure><ul><li>连接socket进行通信</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nc -U /var/run/docker.sock</span><br><span class="line">GET /info HTTP/1.1</span><br></pre></td></tr></table></figure><p>以上都是在本地的访问，docker也支持远程访问。</p><h3 id="docker守护进程的配置和操作"><a href="#docker守护进程的配置和操作" class="headerlink" title="docker守护进程的配置和操作"></a>docker守护进程的配置和操作</h3><ul><li>查看守护进程</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ps -ef | grep docker</span><br><span class="line">sudo status docker</span><br></pre></td></tr></table></figure><ul><li>守护进程的启动、停止和重启</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo service docker start</span><br><span class="line">sudo service docker stop</span><br><span class="line">sudo service docker restart</span><br></pre></td></tr></table></figure><ul><li><p>docker的启动选项</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker -d [OPTIONS]  <span class="comment">#所以守护形式运行</span></span><br></pre></td></tr></table></figure></li><li><p>运行相关:<br>  -D, –debug = false<br>  -e, –exec-driver = “native”<br>  -g, –graph = “/var/lib/docker”<br>  –icc = true<br>  -l, –log-level = “info”<br>  –label = []<br>  -p, –pidfile = “/var/run/docker.pid”</p></li><li><p>docker服务器连接相关：<br>  -G, –group = “docker”<br>  -H, –host = []<br>  –tls = false<br>  –tlscacert = “/home/sven/.docker/ca.pem”<br>  –tlscert = “/home/sven/.docker/cert.pem”<br>  –tlskey = “/home/sven/.docker/key.pem”<br>  –tlsverify = false</p></li><li><p>RemotAPI相关：<br>  –api-enable-cors = false</p></li><li><p>Registry相关：<br>  –insecure-registry = []<br>  –registry-mirror = []</p></li><li><p>网络设置相关：<br>  -b, –bridge = “”<br>  –bip = “”<br>  –fixed-cidr = “”<br>  –fixed-cidr-v6 = “”<br>  –dns = []<br>  –dns-search = []<br>  –ip = 0.0.0.0<br>  –ip-forward = true<br>  –ip-masq = true<br>  –iptables = true<br>  –ipv6 = false<br>  –mtu = 0</p></li><li><p>启动配置文件  /etc/default/docker<br>  注：ubuntu 16.04及以上版本使用：</p><pre><code>修改/lib/systemd/system/docker.service中的ExecStart加载配置：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">service docker restart</span><br><span class="line">docker info</span><br></pre></td></tr></table></figure></code></pre></li></ul><h3 id="docker的远程访问"><a href="#docker的远程访问" class="headerlink" title="docker的远程访问"></a>docker的远程访问</h3><ul><li>第二台安装docker的服务器</li><li>保证Client API与Server API版本一致</li></ul><h4 id="修改docker守护进程启动选项"><a href="#修改docker守护进程启动选项" class="headerlink" title="修改docker守护进程启动选项"></a>修改docker守护进程启动选项</h4><ul><li><p>修改服务器端配置<br>  -H  tcp://host:post</p><pre><code>unix:///path/to/socket fd://* or fd//socketfd</code></pre><p>  守护进程默认配置：</p><pre><code>-H unix:///var/run/docker.sock注：我的默认的是 fd://</code></pre><p>  改为 tcp:</p><pre><code>tcp://0.0.0.0:2375</code></pre>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl http://ip:2375/info</span><br></pre></td></tr></table></figure></li><li><p>修改客户端配置<br>  -H  tcp://host:post</p><pre><code>unix:///path/to/socket fd://* or fd//socketfd</code></pre><p>  默认配置：</p><pre><code>-H unix:///var/run/docker.sock</code></pre><ol><li>docker -H tcp//ip:2375 # 太麻烦</li><li>export DOCKET_HOST=”tcp://ip:2357” # 使用环境变量<br> export DOCKET_HOST=”tcp://ip:2357”  # 使用本地</li></ol></li></ul><ul><li>怎样在设置了远程连接的服务器也支持本机连接？<br>答：给-H再增加一个方式，-H可以设置多个值。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;docker的C-S模式&quot;&gt;&lt;a href=&quot;#docker的C-S模式&quot; class=&quot;headerlink&quot; title=&quot;docker的C/S模式&quot;&gt;&lt;/a&gt;docker的C/S模式&lt;/h3&gt;&lt;p&gt;&lt;img alt data-src=&quot;/uploads/do
      
    
    </summary>
    
      <category term="笔记" scheme="https://zinw623.github.io/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="docker" scheme="https://zinw623.github.io/categories/%E7%AC%94%E8%AE%B0/docker/"/>
    
    
      <category term="docker" scheme="https://zinw623.github.io/tags/docker/"/>
    
      <category term="笔记" scheme="https://zinw623.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>【notes】docker学习笔记3-docker镜像</title>
    <link href="https://zinw623.github.io/2019/08/06/docker-3/"/>
    <id>https://zinw623.github.io/2019/08/06/docker-3/</id>
    <published>2019-08-06T13:17:53.000Z</published>
    <updated>2019-08-06T16:06:44.693Z</updated>
    
    <content type="html"><![CDATA[<h3 id="查看和删除镜像"><a href="#查看和删除镜像" class="headerlink" title="查看和删除镜像"></a>查看和删除镜像</h3><ul><li>镜像的存储位置：/var/lib/docker</li></ul><h4 id="列出镜像"><a href="#列出镜像" class="headerlink" title="列出镜像"></a>列出镜像</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker images [OPSIONS] [REPOSITORY]</span><br></pre></td></tr></table></figure><blockquote><p>   -a, –all = false   # 显示所有镜像，默认并不显示中间层镜像（没有标签名的镜像）<br>   -f, –filter = []   # 过滤条件<br>   –no-trunc = false  # 不使用截断的形式来显示数据(默认使用截断显示EID，比文件名短)<br>   -q, –quiet = false # 只显示EID</p></blockquote><h4 id="镜像标签和仓库"><a href="#镜像标签和仓库" class="headerlink" title="镜像标签和仓库"></a>镜像标签和仓库</h4><ul><li><p>镜像仓库<br>  区分：</p><ul><li><p>REPOSITORY 仓库</p></li><li><p>REGISTRY 仓库</p><p>REGISTRY里会有很多REPOSITORY仓库，每个REPOSITORY里会有一个个独立的镜像。</p></li></ul></li><li><p>标签 TAG</p><ul><li><p>镜像的名字 = 镜像仓库名 : 镜像标签    –对应–&gt;   镜像ID </p><p>ubuntu:latest, ubuntu:14.04, …..</p></li><li><p>如果没有指定标签，默认为latest。</p></li><li><p>同一仓库的不同标签可以对应同一镜像ID，也就是说可以根据需求给同一镜像文件打上不同的标签。</p></li><li><p>没有标签名的镜像称作中间层镜像。</p></li></ul></li></ul><h4 id="查看镜像"><a href="#查看镜像" class="headerlink" title="查看镜像"></a>查看镜像</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker inspect [OPTIONS] CONTIANER|IMAGE [CONTAINER|IMAGE...]</span><br></pre></td></tr></table></figure><blockquote><p>   -f, –format=””</p></blockquote><h4 id="删除镜像"><a href="#删除镜像" class="headerlink" title="删除镜像"></a>删除镜像</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker rmi [OPTIONS] IMAGE [IMAGE...]</span><br></pre></td></tr></table></figure><blockquote><p>   -f, –force = false  强制删除<br>   –no-prune = false  不删除未打标签的父镜像<br>   对应多个标签的镜像文件可以直接用ID选定所有标签</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker rmi ID</span><br></pre></td></tr></table></figure><h3 id="获取和推送镜像"><a href="#获取和推送镜像" class="headerlink" title="获取和推送镜像"></a>获取和推送镜像</h3><h4 id="查找镜像"><a href="#查找镜像" class="headerlink" title="查找镜像"></a>查找镜像</h4><ul><li><p>Docker Hub<br>  <a href="https://registry.hub.docker.com" target="_blank" rel="noopener">https://registry.hub.docker.com</a></p></li><li><p>docker search</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker search [OPTIONS] TERM</span><br></pre></td></tr></table></figure><blockquote><p>   –automated = false<br>   –no-trunc = false<br>   -s, stars = 0   只显示最少多少stars的<br>   最多返回25个结果</p></blockquote><h4 id="拉取镜像"><a href="#拉取镜像" class="headerlink" title="拉取镜像"></a>拉取镜像</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull [OPTIONS] NAME [:TAG]</span><br></pre></td></tr></table></figure><blockquote><p>   -a, –all-tags = false 下载仓库中所有被标记的镜像</p></blockquote><h4 id="推送镜像"><a href="#推送镜像" class="headerlink" title="推送镜像"></a>推送镜像</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker push username/IMAGE</span><br></pre></td></tr></table></figure><h3 id="构建镜像"><a href="#构建镜像" class="headerlink" title="构建镜像"></a>构建镜像</h3><ul><li>保存对容器的修改，并再次使用</li><li>自定义镜像的能力</li><li>以软件的形式打包并分发服务及其运行环境</li></ul><h4 id="docker-commit"><a href="#docker-commit" class="headerlink" title="docker commit"></a>docker commit</h4><p>通过容器构建</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker commit [OPTIONS] CONTAINER [REPOSITORY[:TAG]]</span><br></pre></td></tr></table></figure><blockquote><p>   -a, –author=”” Author<br>       e.g., “John Hannibal Smith <a href="mailto:hannibal@a-team.com" target="_blank" rel="noopener">hannibal@a-team.com</a>“<br>   -m, –message=””    记录构建的信息<br>   -p, –pause = true  不暂停容器的运行</p></blockquote><h4 id="docker-build"><a href="#docker-build" class="headerlink" title="docker build"></a>docker build</h4><p>通过Dockerfile文件构建</p><p>dockerfile:</p><blockquote><p> <strong>#First Dockerfile</strong><br>FROM ubuntu:14.04<br>MAINTAINER dormancypress “<a href="mailto:dormancypress@outlook.com" target="_blank" rel="noopener">dormancypress@outlook.com</a><br>RUN apt-get update<br>RUN apt-get install -y nginx<br>EXPOSE 80</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker build [OPTIONS] PATH|URL|-</span><br></pre></td></tr></table></figure><blockquote><p>   –force-rm = false<br>   –no-cache = false<br>   –pull=false<br>   -q,–quiet = false<br>   –rm = true<br>   -t, –tag=””</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;查看和删除镜像&quot;&gt;&lt;a href=&quot;#查看和删除镜像&quot; class=&quot;headerlink&quot; title=&quot;查看和删除镜像&quot;&gt;&lt;/a&gt;查看和删除镜像&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;镜像的存储位置：/var/lib/docker&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;列
      
    
    </summary>
    
      <category term="笔记" scheme="https://zinw623.github.io/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="docker" scheme="https://zinw623.github.io/categories/%E7%AC%94%E8%AE%B0/docker/"/>
    
    
      <category term="docker" scheme="https://zinw623.github.io/tags/docker/"/>
    
      <category term="笔记" scheme="https://zinw623.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>【notes】docker学习笔记2-docker容器</title>
    <link href="https://zinw623.github.io/2019/08/06/docker-2/"/>
    <id>https://zinw623.github.io/2019/08/06/docker-2/</id>
    <published>2019-08-06T07:21:42.000Z</published>
    <updated>2019-08-06T15:45:01.235Z</updated>
    
    <content type="html"><![CDATA[<h3 id="容器的基本操作"><a href="#容器的基本操作" class="headerlink" title="容器的基本操作"></a>容器的基本操作</h3><h4 id="启动容器"><a href="#启动容器" class="headerlink" title="启动容器"></a>启动容器</h4><ul><li><p>仅一次命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run IMAGE [COMMAND] [ARG]</span><br></pre></td></tr></table></figure></li><li><p>启动交互式容器</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -i -t IMAGE /bin/bash</span><br></pre></td></tr></table></figure></li></ul><p>-i –interactive=ture | false 默认是false，为容器始终打开标准输入<br>-t –tty=true | false 默认是false，分配一个终端</p><ul><li><p>自定义容器名字</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --name=自定义名 -i -t IMAGE /bin/bash</span><br></pre></td></tr></table></figure></li><li><p>重新启动已建立的容器<br>docker start [-i] 容器名</p></li></ul><h4 id="查看容器"><a href="#查看容器" class="headerlink" title="查看容器"></a>查看容器</h4><ul><li><p>不加参数是正在运行的容器，-a是所有容器，-l是最新创建的一个容器。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker ps [-a] [-l]</span><br></pre></td></tr></table></figure></li><li><p>查看容器参数</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker inspect [ID] or [name]</span><br></pre></td></tr></table></figure></li></ul><h4 id="删除容器"><a href="#删除容器" class="headerlink" title="删除容器"></a>删除容器</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker rm 容器名</span><br></pre></td></tr></table></figure><h3 id="守护式容器"><a href="#守护式容器" class="headerlink" title="守护式容器"></a>守护式容器</h3><p>什么是守护式容器：</p><ul><li>能够长期运行</li><li>没有交互式会话</li></ul><h4 id="以守护形式运行容器："><a href="#以守护形式运行容器：" class="headerlink" title="以守护形式运行容器："></a>以守护形式运行容器：</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker run -i -t IMAGE /bin/bash</span><br><span class="line">Ctrl + P Ctrl + Q</span><br></pre></td></tr></table></figure><h4 id="附加到运行中的容器"><a href="#附加到运行中的容器" class="headerlink" title="附加到运行中的容器"></a>附加到运行中的容器</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker attach 容器名</span><br></pre></td></tr></table></figure><h4 id="启动守护式容器"><a href="#启动守护式容器" class="headerlink" title="启动守护式容器"></a>启动守护式容器</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d 镜像名 [COMMAND] [ARG...]</span><br></pre></td></tr></table></figure><h4 id="得知容器运行情况"><a href="#得知容器运行情况" class="headerlink" title="得知容器运行情况"></a>得知容器运行情况</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker logs [-f] [-t] [--tail] 容器名</span><br></pre></td></tr></table></figure><p>-f –follows=true | false 默认为false  一直跟踪日志变化并返回结果<br>-t –timestamps=true | false 默认为false  结果加上时间戳<br>–tail= “all”  多少数量的日志</p><h4 id="查看运行中容器进程"><a href="#查看运行中容器进程" class="headerlink" title="查看运行中容器进程"></a>查看运行中容器进程</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker top 容器名</span><br></pre></td></tr></table></figure><h4 id="在运行中的容器内启动新进程"><a href="#在运行中的容器内启动新进程" class="headerlink" title="在运行中的容器内启动新进程"></a>在运行中的容器内启动新进程</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> [-d] [-i] [-t] 容器名 [COMMAND] [ARG...]</span><br></pre></td></tr></table></figure><h4 id="停止守护式容器"><a href="#停止守护式容器" class="headerlink" title="停止守护式容器"></a>停止守护式容器</h4><ul><li><p>发送指令等待停止</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker stop 容器名</span><br></pre></td></tr></table></figure></li><li><p>直接停止容器</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">kill</span> 容器名</span><br></pre></td></tr></table></figure></li></ul><p>###在容器中部署静态网站</p><h4 id="设置容器的端口映射"><a href="#设置容器的端口映射" class="headerlink" title="设置容器的端口映射"></a>设置容器的端口映射</h4><p>run [-P]</p><ul><li><p>-P , –publish-all = true | false 默认为false  为容器暴露的所有端口设置映射</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -P -t -i ubuntu /bin/bash</span><br></pre></td></tr></table></figure></li><li><p>-p , 指定端口</p><ul><li><p>容器端口</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -p 80 -i -t ubuntu /bin/bash</span><br></pre></td></tr></table></figure></li><li><p>宿主机端口:容器端口</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -p 8080:80 -i -t ubuntu /bin/bash</span><br></pre></td></tr></table></figure></li><li><p>ip::容器端口</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -p 0.0.0.0:80 -i -t ubuntu /bin/bash</span><br></pre></td></tr></table></figure></li><li><p>ip:宿主机端口:容器端口</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -p 0.0.0.0:8080:80 -i -t ubuntu /bin/bash</span><br></pre></td></tr></table></figure></li></ul></li></ul><h4 id="Nginx部署"><a href="#Nginx部署" class="headerlink" title="Nginx部署"></a>Nginx部署</h4><ul><li><p>创建映射80端口的交互式容器</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -p 80 --name web -it ubuntu /bin/bash</span><br></pre></td></tr></table></figure></li><li><p>安装Nginx</p></li><li><p>安装文本编辑器vim</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">apt-get update</span><br><span class="line">apt-get upgrade</span><br><span class="line">apt-get install nginx -y</span><br><span class="line">apt-get install vim -y</span><br></pre></td></tr></table></figure></li><li><p>创建静态页面</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /var/www/html</span><br><span class="line">vim index.html</span><br></pre></td></tr></table></figure></li><li><p>修改Nginx配置文件</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/nginx/sites-enabled/default</span><br></pre></td></tr></table></figure></li><li><p>运行Nginx</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">nginx</span><br><span class="line">ps -ef</span><br><span class="line">Ctrl P Ctrl Q</span><br></pre></td></tr></table></figure></li><li><p>验证网站访问</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker port web <span class="comment"># 查看端口映射情况</span></span><br><span class="line">docker top web <span class="comment"># 查看进程运行情况</span></span><br><span class="line">docker inspect web <span class="comment">#查看ip</span></span><br><span class="line">curl http://172.17.0.2</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;容器的基本操作&quot;&gt;&lt;a href=&quot;#容器的基本操作&quot; class=&quot;headerlink&quot; title=&quot;容器的基本操作&quot;&gt;&lt;/a&gt;容器的基本操作&lt;/h3&gt;&lt;h4 id=&quot;启动容器&quot;&gt;&lt;a href=&quot;#启动容器&quot; class=&quot;headerlink&quot; titl
      
    
    </summary>
    
      <category term="笔记" scheme="https://zinw623.github.io/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="docker" scheme="https://zinw623.github.io/categories/%E7%AC%94%E8%AE%B0/docker/"/>
    
    
      <category term="docker" scheme="https://zinw623.github.io/tags/docker/"/>
    
      <category term="笔记" scheme="https://zinw623.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>【notes】docker学习笔记1-docker基本组成</title>
    <link href="https://zinw623.github.io/2019/08/06/docker-1/"/>
    <id>https://zinw623.github.io/2019/08/06/docker-1/</id>
    <published>2019-08-06T02:30:45.000Z</published>
    <updated>2019-08-08T12:29:30.031Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Docker的基本组成"><a href="#Docker的基本组成" class="headerlink" title="Docker的基本组成"></a>Docker的基本组成</h3><ul><li>Docker Client 客户端</li><li>Docker Daemon 守护进程</li><li>Docker Image 镜像</li><li>Docker Container 容器</li><li>Docker Registry 仓库</li></ul><h4 id="Docker客户端-守护进程"><a href="#Docker客户端-守护进程" class="headerlink" title="Docker客户端/守护进程"></a>Docker客户端/守护进程</h4><ul><li>C/S架构</li><li>docker客户端对服务器的访问： 本地/远程</li><li>docker客户端向发送给守护进程请求，守护进程的执行结果还会传回给客户端。</li></ul><h4 id="Docker-Image镜像"><a href="#Docker-Image镜像" class="headerlink" title="Docker Image镜像"></a>Docker Image镜像</h4><ul><li>构建和打包阶段。</li><li>容器的基石，相当于保存了容器运行需要的源代码。</li><li>层叠的层叠文件系统。 bootfs（引导文件系统）-&gt; rootfs(Ubuntu) -&gt; add emacs -&gt; add Apache</li><li>联合加载（union mount）:一次加载多个文件系统（add Apache，add emacs），将所有文件系统叠加在一切。镜像可以叠加在一起，位于底部的成为基础镜像（rootfs），add emacs（副镜像）。</li></ul><h4 id="Docker-Container容器"><a href="#Docker-Container容器" class="headerlink" title="Docker Container容器"></a>Docker Container容器</h4><ul><li>通过镜像启动。</li><li>启动执行阶段。</li><li>配置数据和镜像层（bootfs -&gt; ······ -&gt; add emacs) -&gt; 可写层。</li><li>写时复制：docker出现变化时都会应用到可写层，先从只读镜像层复制到可写层然后只读层的文件被隐藏。</li></ul><h4 id="Docker-Registry仓库"><a href="#Docker-Registry仓库" class="headerlink" title="Docker Registry仓库"></a>Docker Registry仓库</h4><ul><li>保存docker镜像。</li><li>分为公有和私有。公有：Docker Hub</li></ul><h4 id="图示结构"><a href="#图示结构" class="headerlink" title="图示结构"></a>图示结构</h4><p>Docker:</p><p><img alt data-src="/uploads/docker/0.PNG"></p><p>Docker Image:</p><p><img alt data-src="/uploads/docker/1.PNG"></p><p>Docker Container:</p><p><img alt data-src="/uploads/docker/2.PNG"></p><h4 id="docker基本指令"><a href="#docker基本指令" class="headerlink" title="docker基本指令"></a>docker基本指令</h4><ul><li><p>查找镜像</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker search tutorial</span><br></pre></td></tr></table></figure></li><li><p>下载镜像</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull learn/tutorial</span><br></pre></td></tr></table></figure></li><li><p>启动一个容器，使用echo命令输出hello world</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run learn/tutorial <span class="built_in">echo</span> <span class="string">'hello world'</span></span><br></pre></td></tr></table></figure></li><li><p>启动一个容器下载ping</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run learn/tutorial apt-get install -y ping</span><br></pre></td></tr></table></figure></li><li><p>查看有哪些容器</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker ps -l</span><br></pre></td></tr></table></figure></li><li><p>提交容器，即创建一个新的镜像</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker commit [docker ID] learn/ping</span><br></pre></td></tr></table></figure></li><li><p>用新镜像建立一个容器</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run learn/ping ping www.baidu.com</span><br></pre></td></tr></table></figure></li><li><p>查看容器信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker inspect [docker ID]</span><br></pre></td></tr></table></figure></li><li><p>查看有哪些镜像</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker image</span><br></pre></td></tr></table></figure></li><li><p>将镜像保存到docker hub上</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker push /learn/ping</span><br></pre></td></tr></table></figure></li></ul><h3 id="Docker容器相关技术简介"><a href="#Docker容器相关技术简介" class="headerlink" title="Docker容器相关技术简介"></a>Docker容器相关技术简介</h3><h4 id="Docker依赖的Linux内核特性"><a href="#Docker依赖的Linux内核特性" class="headerlink" title="Docker依赖的Linux内核特性"></a>Docker依赖的Linux内核特性</h4><ul><li><p>Namespaces 命名空间<br>  提供了系统资源的隔离，for轻量级虚拟化服务<br>  五种命名空间：</p><ul><li>PID 进程隔离</li><li>NET 管理网络接口</li><li>IPC 管理跨进程通信的访问</li><li>MNT 管理挂载点</li><li>UTS 隔离内核和版本标识</li></ul></li><li><p>Control groups 控制组</p><ul><li>资源限制（内存上限等）</li><li>优先级设定（设定哪些进程组使用哪些资源）</li><li>资源计量</li><li>资源控制（挂起恢复）</li></ul></li></ul><h4 id="Docker容器的能力"><a href="#Docker容器的能力" class="headerlink" title="Docker容器的能力"></a>Docker容器的能力</h4><ul><li>文件系统隔离：每个容器都有自己的root文件系统</li><li>进程隔离： 每个容器都运行在自己的进程环境中</li><li>网络隔离： 容器间的虚拟网络接口和IP地址都是分开的</li><li>资源隔离和分组：使用cgroups将CPU和内存之类的资源独立分配给每个Docker容器</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Docker的基本组成&quot;&gt;&lt;a href=&quot;#Docker的基本组成&quot; class=&quot;headerlink&quot; title=&quot;Docker的基本组成&quot;&gt;&lt;/a&gt;Docker的基本组成&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Docker Client 客户端&lt;/li&gt;
&lt;li&gt;D
      
    
    </summary>
    
      <category term="笔记" scheme="https://zinw623.github.io/categories/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="docker" scheme="https://zinw623.github.io/categories/%E7%AC%94%E8%AE%B0/docker/"/>
    
    
      <category term="docker" scheme="https://zinw623.github.io/tags/docker/"/>
    
      <category term="笔记" scheme="https://zinw623.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>用Xshell管理虚拟机Ubuntu</title>
    <link href="https://zinw623.github.io/2019/08/03/xshell-vmware/"/>
    <id>https://zinw623.github.io/2019/08/03/xshell-vmware/</id>
    <published>2019-08-03T14:14:16.000Z</published>
    <updated>2019-08-13T08:28:01.528Z</updated>
    
    <content type="html"><![CDATA[<p>因为使用VM虚拟机太过占用资源，所以我们可以用Xshell连接到虚拟机，来达到节省本机资源的目的。</p><a id="more"></a><ol><li>安装SSH： </li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install openssh-server</span><br><span class="line">service iptables stop <span class="comment">#关闭防火墙</span></span><br><span class="line">service ssh start <span class="comment">#开启ssh服务</span></span><br></pre></td></tr></table></figure><ol start="2"><li>获得登录需要的ip ,在虚拟机输入：</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ifconfig</span><br></pre></td></tr></table></figure><img width="50%" data-src="/uploads/xshell-vmware/1.PNG"><p>ens*后面的inet后面的值就是ip。</p><ol start="3"><li><p>按照我之前写过的xshell连接的教程 <a href="/2019/08/01/XshellDownload/">windows系统：Xshell下载安装+连接服务器</a> 建立会话就ok，主机就是刚才你获得的ip，登录用的用户名和密码就是你安装时填的用户名(非root账户)和密码。</p></li><li><p>之后只需要打开虚拟机后最小化界面，从xshell登入后reboot一下虚拟机，这样从内存角度就能节省将近90多MB。</p></li></ol><p>注意： reboot后就不要在打开VMware了，一直让它最小化直到关闭。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;因为使用VM虚拟机太过占用资源，所以我们可以用Xshell连接到虚拟机，来达到节省本机资源的目的。&lt;/p&gt;
    
    </summary>
    
      <category term="Xshell" scheme="https://zinw623.github.io/categories/Xshell/"/>
    
    
      <category term="Xshell" scheme="https://zinw623.github.io/tags/Xshell/"/>
    
      <category term="虚拟机" scheme="https://zinw623.github.io/tags/%E8%99%9A%E6%8B%9F%E6%9C%BA/"/>
    
      <category term="Ubuntu" scheme="https://zinw623.github.io/tags/Ubuntu/"/>
    
  </entry>
  
  <entry>
    <title>photoshop cc 2019安装破解</title>
    <link href="https://zinw623.github.io/2019/08/02/photoshop-cc-2019-download/"/>
    <id>https://zinw623.github.io/2019/08/02/photoshop-cc-2019-download/</id>
    <published>2019-08-02T03:08:27.000Z</published>
    <updated>2019-08-08T03:22:57.465Z</updated>
    
    <content type="html"><![CDATA[<p>Photoshop如今已经非常常用的处理图片的软件，本文就是介绍一下photoshop cc 2019安装破解的完整过程。</p><p>注：本文参考了<a href="http://www.3322.cc/soft/48343.html" target="_blank" rel="noopener">http://www.3322.cc/soft/48343.html</a></p><a id="more"></a><h3 id="下载creative-cloud"><a href="#下载creative-cloud" class="headerlink" title="下载creative cloud"></a>下载creative cloud</h3><p>什么是creative cloud？creative cloud相当于adobe系列的一个应用商城，我们可以在里面安装各种adobe系列的软件。<br>下载链接： <a href="https://creativecloud.adobe.com/apps/download/creative-cloud?promoid=KSPDX" target="_blank" rel="noopener">官网链接</a>       <a href="https://pan.baidu.com/s/1qr6U3l48Nr-pakBNumR7Tw" target="_blank" rel="noopener">网盘链接</a></p><p>下载完成直接按提示安装，然后注册adobe账号并登陆。</p><h3 id="下载安装photoshop-cc-2019"><a href="#下载安装photoshop-cc-2019" class="headerlink" title="下载安装photoshop-cc-2019"></a>下载安装photoshop-cc-2019</h3><p>默认的下载位置在c盘，如果想改到其他盘可以点击右上角的三个点，出来菜单再点首选项。</p><img width="50%" data-src="/uploads/downloadPS/3.PNG"><p>然后点击creative Cloud界面，在安装位置条目处更改到你想安装到的位置。</p><img width="50%" data-src="/uploads/downloadPS/4.PNG"><p>打开creative cloud，找到photoshop的条目点击试用，photoshop自动下载安装成功。</p><img width="50%" data-src="/uploads/downloadPS/5.PNG"><img width="50%" data-src="/uploads/downloadPS/1.PNG"><h3 id="利用补丁破解"><a href="#利用补丁破解" class="headerlink" title="利用补丁破解"></a>利用补丁破解</h3><p>安装完成后安全起见先不要打开ps，我们先下载补丁工具。<br>下载链接：<a href="https://pan.baidu.com/s/1RWVV7HKcgqNU_a6GsLjyDw" target="_blank" rel="noopener">网盘链接</a> <a href="/uploads/downloadPS/PhotoshopCC2019pjbd_48343.zip">其他链接</a></p><p>将压缩包里的adobe.photoshop.cc.2019.20.0.0-patch.exe文件解压到ps安装目录下，就是你刚才修改的安装位置，保证那个位置下有photoshop.exe文件。</p><p>然后点击运行补丁（你可能会听到一段诡异的音乐。。。）。</p><img width="50%" data-src="/uploads/downloadPS/6.PNG"><p>点击应用，等待出现文件补丁已成功完成的提示。</p><img width="50%" data-src="/uploads/downloadPS/7.PNG"><p>这样就破解完成了，这时再打开ps发现没有试用还有多少天的提醒了。</p><img width="50%" data-src="/uploads/downloadPS/8.PNG"><p>按照补丁制作者的建议，在 编辑 ==&gt; 首选项 ==&gt; 常规 ==&gt; 停用”主页”屏幕 打钩。</p><img width="50%" data-src="/uploads/downloadPS/9.PNG"><p>最后做好重启一下ps再试用。</p><p>注：这篇文章是我安装后就写了，我在安装完的七天后再次检验是否失效，如果失效我会更新补丁，如果补丁失效可以回来看我是否有更新方法。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Photoshop如今已经非常常用的处理图片的软件，本文就是介绍一下photoshop cc 2019安装破解的完整过程。&lt;/p&gt;
&lt;p&gt;注：本文参考了&lt;a href=&quot;http://www.3322.cc/soft/48343.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://www.3322.cc/soft/48343.html&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="photoshop" scheme="https://zinw623.github.io/categories/photoshop/"/>
    
    
      <category term="破解" scheme="https://zinw623.github.io/tags/%E7%A0%B4%E8%A7%A3/"/>
    
      <category term="photoshop" scheme="https://zinw623.github.io/tags/photoshop/"/>
    
  </entry>
  
  <entry>
    <title>Xshell：在本地浏览器使用服务器的jupyter notebook</title>
    <link href="https://zinw623.github.io/2019/08/02/hexo-jupyter/"/>
    <id>https://zinw623.github.io/2019/08/02/hexo-jupyter/</id>
    <published>2019-08-02T01:47:42.000Z</published>
    <updated>2019-08-04T03:19:26.760Z</updated>
    
    <content type="html"><![CDATA[<p>有的服务器里只是命令行，无法可视化，可能就无法使用jupyter notebook。其实需要稍微修改一下连接的属性就能在本地浏览器里打开在服务器里启动的jupyter notebook<a id="more"></a>，具体操作如下：</p><ol><li>首先右击会话管理器里的服务器标签，在菜单点击属性。</li></ol><img width="50%" data-src="/uploads/xshellJupyter/1.PNG"><ol start="2"><li>然后点击左侧的隧道，然后再点击添加。</li></ol><img width="50%" data-src="/uploads/xshellJupyter/2.PNG"><ol start="3"><li>输入两个端口号，我这输入的是jupyter notebook默认的8888端口，然后点确定</li></ol><img width="50%" data-src="/uploads/xshellJupyter/3.PNG"><ol start="4"><li>然后再取消右下方转发X11连接到的选项，然后点确定。</li></ol><img width="50%" data-src="/uploads/xshellJupyter/4.PNG"><ol start="5"><li>之后双击会话管理器里的服务器进行连接，在命令行里输入jupyter notebook，启动后在浏览器里访问<a href="http://localhost:8888/" target="_blank" rel="noopener"></a>就会看到jupyter notebook的界面了。</li></ol><img width="50%" data-src="/uploads/xshellJupyter/5.PNG">]]></content>
    
    <summary type="html">
    
      &lt;p&gt;有的服务器里只是命令行，无法可视化，可能就无法使用jupyter notebook。其实需要稍微修改一下连接的属性就能在本地浏览器里打开在服务器里启动的jupyter notebook
    
    </summary>
    
      <category term="Xshell" scheme="https://zinw623.github.io/categories/Xshell/"/>
    
    
      <category term="Xshell" scheme="https://zinw623.github.io/tags/Xshell/"/>
    
      <category term="jupyter notebook" scheme="https://zinw623.github.io/tags/jupyter-notebook/"/>
    
  </entry>
  
</feed>
